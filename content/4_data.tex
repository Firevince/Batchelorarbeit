\chapter{Datenbeschaffung und Datenspeicherung}\label{ch:data}

\section{Ausgangslage}

Um eine Zusammenstellung verschiedener Audiosegmente möglichst genau und inhaltlich abgestimmt auf ein bestimmtes Thema zu fokussieren, müssen auch zu diesem Thema passende Audioabschnitte in den Daten verfügbar sein.
Als Audiodaten könnten fast alle Audioressourcen genutzt werden, wie zum Beispiel Beiträge aus Radioprogrammen, die Audiospuren von Videos oder ganze Podcast-Episoden.

Die Qualität dieses Systems würde mit steigender Anzahl an Audiomaterial bessere Ergebnisse erzielen, da dann zu vielen Themen mehr Inhalte zur Verfügung stehen.
Allerdings ist es im Umfang dieser Arbeit nicht möglich, alle verfügbaren Audiomaterialien für die Erstellung der Podcasts zu verwenden.
Die Audiodaten müssten dazu erst transkribiert werden und anschließend mit diesen Transkripten Embeddings generiert werden, was zeit- und ressourcenintensiv ist.
Für diese Arbeit wurde versucht, möglichst qualitativ hochwertige Daten zu benutzen, um mit den verfügbaren Ressourcen und in der gegebenen Zeit einen qualitativ hochwertigen Prototypen zu erstellen.

Als Audiomaterial würden sich am besten Podcasts eignen, in denen über verschiedene Themen sachlich gesprochen wird, die Informationen aber gleichzeitig faktisch korrekt sind.
Außerdem sollten die Audioquellen frei verfügbar sein, damit keine Urheberrechtsverletzung stattfindet.

\section{Audiodaten beschaffen}

\subsection{Die ARD-Audiothek}

Die ARD-Audiothek ist in Deutschland eine der größten Audio- und Podcastanbieter mit mittlerweile über 100.000 verschiedenen verfügbaren Audioinhalten und über 41 Millionen Audioabrufen.
Alle Inhalte unterliegen den journalistischen Grundsätzen der ARD und bieten somit einen sorgfältigen Qualitätsstandard.
Die verschiedenen Audioinhalte stammen von den einzelnen Landesrundfunkanstalten, der ARD und dem Deutschlandradio und liefern eine Vielzahl verschiedener Inhalte.
Sie enthält über 2000 verschiedene Podcasts in vielen unterschiedlichen Kategorien, wie Comedy, Sport, Wissenschaft, Wirtschaft, Gesellschaft, Kunst, Musik oder Philosophie.
In dieser Audiothek gibt es zudem Hörbücher, Hörspiele oder Podcasts nur für Kinder.
Die einzelnen Rundfunkanstalten tragen außerdem eigene Podcasts bei, die meist einen regionalen Kontext haben, wie zum Beispiel der Podcast „Giga Grünheide“ über das Tesla-Werk in Brandenburg vom rbb.
Alle diese Inhalte sind kostenlos und frei verfügbar und stellen eine gute Quelle für das Audiomaterial dar, das in dieser Arbeit verwendet wird.

\subsection{Podcastreihe Radiowissen}

Zur automatischen Generierung von Podcast-Episoden bietet es sich an, dass in den Ausgangsaudios die Sprache klar und verständlich ist und verschiedene Sprecher sich nicht ins Wort fallen bzw. gleichzeitig reden.
Außerdem ist es wünschenswert, die ausgeschnittenen Audiosegmente an klaren Satzgrenzen zu teilen, sodass der Ausschnitt nicht mitten im Satz beginnt und den Zuhörenden der Kontext vorenthalten wird.

Aufgrund dieser Kriterien wurde als Datengrundlage die Podcastreihe Radiowissen von Bayern 2 benutzt.
Diese ist nicht wie ein klassischer Podcast im Dialogstil aufgebaut, sondern ähnelt einem Hörspiel, bei dem der Text von einem Manuskript abgelesen wird.
Dazu kommen verschiedene Geräusche und Stimmen, um den Hörenden mehr Abwechslung zu bieten.

Der Fokus der einzelnen Episoden liegt auf interessanten Beiträgen zu verschiedenen Themen, die häufig Gebiete der Geschichte, Naturwissenschaft, Gesellschaft oder Philosophie umfassen.
Beispielepisoden sind: „Fasten - Verzicht und innerer Gewinn?“, „Die Maus - Anpassungskünstler und gefürchteter Schädling“, oder „Maria Sibylla Merian - Naturforscherin und Künstlerin“.

Die mehr als 2000 Episoden wurden von mehr als 150 verschiedenen Autoren geschrieben.
Dadurch sind die einzelnen Episoden unterschiedlich in ihrer Erzählweise.
In einigen Episoden kommen originale Audiospuren von historischen Aufnahmen vor oder auch Gastbeiträge von Experten.
Zudem wird beinahe jeder Podcast abwechselnd von mehreren Stimmen vorgetragen, was nachweislich die Aufmerksamkeit von Zuhörenden verbessert.

\subsection{Datenbeschaffung über die ARD-Audiothek-API}

Die Inhalte der ARD-Audiothek können entweder direkt über die Webseite erreicht oder mithilfe einer frei benutzbaren Web-GraphQL-API abgefragt werden.
(\url{https://api.ardaudiothek.de/graphql}) 
Über diese Schnittstelle sind alle Informationen, wie Titel, Beschreibungen, Autoren oder auch der Link zum MP3-File jeder Episode abrufbar.

Zunächst müssen alle Downloadlinks zu den einzelnen Episoden ermittelt werden.
Mit der GraphQL-Abfrage \autoref{ch:graphql-1} werden alle Download-Links zu den Podcast-Episoden des Podcasts „Radio Wissen“ von Bayern 2 ermittelt.
Das sind (Stand 3. Januar 2024) 2257 Podcast-Episoden.

Alle diese Audiodateien wurden anschließend heruntergeladen und auf einer lokalen Festplatte gespeichert.

Für weitere Analysen und die Kategorisierung der Audiodateien ist es außerdem sinnvoll, die Beschreibungen der einzelnen Episoden und die dazugehörigen Schlagwörter abzufragen, da diese eine kurze Zusammenfassung oder Einordnung der Episoden enthalten.
Außerdem bietet das Entstehungsdatum der Episoden die Möglichkeit, Informationen später nach Aktualität zu filtern.
Über die Anfrage \autoref{ch:graphql-2} können all diese Informationen abgefragt werden.

Über die API kann auch in einigen Fällen direkt ein Transkript des Audiofiles angefordert werden.
Allerdings ist die Transkription meist nicht sehr akkurat.
Näheres dazu in Kapitel \autoref{ch:method}.

\subsection{Audiodatenanalyse}

Die 2232 einzigartigen Episoden haben eine durchschnittliche Länge von 22 Minuten und eine durchschnittliche Größe von 21 MB.
Insgesamt weisen diese Audiodaten eine Größe von ungefähr 47 GB auf.

Dabei kam es insgesamt 15-mal vor, dass zwei Episoden denselben Titel tragen, aber eine unterschiedliche Download-URL aufwiesen.
Die Download-URLs unterscheiden sich nur, indem am Ende die Zeichen „-1“ oder „-2“ angefügt wurden.
Zum Beispiel hat die Episode „Quantenphysik - Wahr, aber verrückt“ den Downloadlink \url{https://media.neuland.br.de/file/1804047/c/feed/quantenphysik-wahr-aber-verrueckt.mp3} aber auch \url{https://media.neuland.br.de/file/2069613/c/feed/quantenphysik-wahr-aber-verrueckt-1.mp3}.
In diesem Fall liefert nur die zweite URL einen Download, die erste zeigt eine Fehlermeldung an.
Es gibt auch Fälle, in denen beide Links funktionieren, wie zum Beispiel \url{https://media.neuland.br.de/file/32891/c/feed/die-bamberger-hexenprozesse-unschuldig-muss-ich-sterben.mp3} und \url{https://media.neuland.br.de/file/1858845/c/feed/die-bamberger-hexenprozesse-unschuldig-muss-ich-sterben-1.mp3}.
Die Daten wurden dementsprechend bereinigt und doppelte Audioinhalte nur einmal abgespeichert.

\myfigure{mp3_length.png}{Länge der MP3 Datein}{mp3_length}
\section{Transkription der Podcasts (ASR)}

\section{Vorgehensweise}

Zunächst müssen die Daten gesammelt und aufbereitet werden.
Für dieses Projekt bildet die Datengrundlage die Transkripte bzw. Manuskripte der Podcasts der ARD-Audiothek.
In der Audiothek selbst gibt es keine Transkripte zu den Podcasts.
Für den Podcast „Radiowissen“ von Bayern 2 gibt es auf deren Seite die Manuskripte im PDF-Format.
Diese sind zwar inhaltlich hochqualitativ, da sie die exakte Wortwahl der Podcasts enthalten, als PDF-Format sind sie allerdings schwierig maschinell auszulesen und weiterhin besitzen sie keine Zeitinformationen zu den einzelnen Wörtern.
Die Zeitinformationen in Form von Zeitstempeln für jedes Wort sind wichtig, um die Audiofiles der Podcasts später an den richtigen Stellen zuzuschneiden.

Ein anderer Ansatz ergibt sich, wenn die Podcasts transkribiert werden.
Die Vorteile sind, dass die Transkription auch bei Podcasts funktioniert, für die vorab kein Transkript erstellt wurde, was die Mehrzahl aller Podcasts ausmacht.
Außerdem kann bei einer Transkription auch gleichzeitig die Zeitstempel für jedes Wort extrahiert werden.

\subsection{Transkriptionsmethoden im Überblick}

Um automatisch Audioinhalte mittels eines Automatic Speech Recognition Systems (ASR) zu transkribieren, können mittlerweile zahlreiche verschiedene Dienste benutzt werden.
Dabei muss immer zwischen Geschwindigkeit, Genauigkeit und Kosten abgewogen werden.
Es gibt viele Cloudanbieter, die die Transkription via API gegen Gebühren vornehmen.
Außerdem gibt es verschiedene Transkriptionsprogramme, die auf lokaler Hardware eingesetzt werden können.
Im Folgenden sind mehrere Ansätze beschrieben, die für die Transkription der Daten in Erwägung gezogen wurden.

\subsection{Fraunhofer IAIS}

Wie im \autoref{ch:theoretical} beschrieben, führt das Fraunhofer-Institut für Intelligente Analyse- und Informationssysteme (IAIS) schon automatische Transkriptionen der Audioinhalte der ARD-Audiothek durch.
Die Verwendung dieser Transkripte führt allerdings gleich zu mehreren Problemen.
Ein Grund ist die inkonsistente Verfügbarkeit der Daten.
Von den angefragten Transkripten verfügten 84 Episoden und damit ca. 3,8\% aller Episoden über keine Daten für Transkripte.
Ein weiterer Grund ist die vorgefertigte Segmentierung des Transkripts.
Die Transkripte sind in Segmente aufgeteilt, die zwischen 3-30 Sekunden dauern.
Zeitstempel sind jeweils nur für den Anfang und das Ende jedes Segments verfügbar.
Dadurch ist eine spätere Segmentierung nicht mehr möglich und die vorhandenen Segmente sind zu inkonsistent in Länge und beginnen und enden nicht an Satzgrenzen.

\subsection{Whisper}

Eines der besten kostenlosen Open-Source-ASR-Systeme bietet Whisper von OpenAI.
Dieses System wurde mit einem maschinellen Lernverfahren auf 680.000 Stunden Audiomaterial in verschiedenen Sprachen trainiert und erreicht damit State-of-the-Art-Performance in Transkriptionen.
Es ist sehr leistungsstark und kann lokal auf eigener Hardware laufen.

Konkret wurde in dieser Arbeit das Projekt faster-whisper verwendet, welches die ursprünglichen Whisper-Modelle mithilfe einer schnelleren Inferenz-Engine neu implementiert.
faster-whisper bietet außerdem Word-Level-Timestamps im Gegensatz zum ursprünglichen Whisper-Projekt.

Whisper bietet mehrere verschiedene Modelle zur Transkription an.
Es gibt die Modelle tiny, base, small, medium und large.
Das Basismodell hat ca. 74 Millionen Parameter und benötigt ca. 1 GB VRAM und ist ca. 16-mal schneller als das Large-Modell.
Bei einem Test für die Episode 1968-das-ausnahmejahr aus dem Podcast Radiowissen von Bayern 2 schneidet es aber nicht sehr gut ab.
Aus dem Wort „Vietnam“ wird „Wirdnam“, aus „Panzer in Prag“ wird „Panzer-Inprac“ und aus „Ohrfeige“ wird „Urfeige“.
Der vorgetragene Text wurde dabei ohne Störgeräusche und von einer Person flüssig vorgetragen.
Eine Evaluation der Transkriptionsfähigkeiten mittels einer Word Error Rate (WER) ist nicht durchgeführt worden, da notwendige Vergleichstranskripte nicht vorhanden sind.

Dagegen bietet das Medium-Modell von Whisper deutlich bessere Ergebnisse für dieselbe Episode.
Bei der Transkription konnte kein Fehler festgestellt werden.
Allerdings ist der Zeitaufwand durch höhere Rechenleistung immens.
Auf einem MacBook Pro 2016 mit einem Intel Core i7 benötigt die Transkription ca. 45 Minuten pro Episode.
Auf einer T4-GPU, wie sie Google kostenlos auf Google Colab zur Verfügung stellt, dauert eine Transkription immer noch 3,5 Minuten.
Für ca. 2000 Episoden benötigt diese Methode demnach ca. 7000 Minuten (ca. 116 Stunden).

Die Transkription für diese Arbeit wurde auf einem High-Performance-Cluster der Technischen Hochschule Nürnberg durchgeführt.
Dabei wurde eine NVIDIA A100-GPU verwendet.
Über vier separate Slurm-Jobs wurden die 2237 Episoden innerhalb von ca. 50 Stunden transkribiert.

Die Transkripte der Episoden sind im Durchschnitt ca. 2792 Wörter lang und benötigen ungefähr 20 KB Speicherplatz pro Transkript.

\myfigure{transcript_length.png}{Anzahl der Wörter pro transkript}{transcript_length}


\subsection{Eurovox}

Für die zukünftige Verwendung dieses Systems im öffentlich-rechtlichen Kontext würde es sich anbieten, weitere Podcast-Episoden mittels Cloud-Computing zu transkribieren.
Das heißt, das Whisper-Modell nicht auf der lokalen Hardware laufen zu lassen, sondern zum Beispiel auf den Servern von Eurovox.
Eurovox ist ein Softwaretool von der EBU, der European Broadcasting Union.
Sie ist ein Zusammenschluss von derzeit 68 Rundfunkanstalten in 56 Staaten Europas, Nordafrikas und Vorderasiens mit Sitz in Genf.
Das Tool Eurovox steht dabei allen Mitgliedern zur Verfügung.
Mithilfe dieses Tools können Text-to-Speech, Übersetzungen und Speech-to-Text Services über eine UI, oder eine API benutzt werden.
Für dieses Projekt könnte zunächst die Text-to-Speech-Funktion verwendet werden.
Leider wurde der Zugang zu der API für dieses Projekt noch nicht freigestellt.~\cite{eurovox2024}

\section{Datenaufbereitung}

\subsection{Segmentbildung}

Die von Whisper transkribierten Daten enthalten die erkannten Wörter sowie die einzelnen Zeitstempel für den Start und das Ende jedes Wortes mit einer Genauigkeit von einer Zehntelsekunde.
Für die spätere Analyse der Daten ist es sinnvoll, die einzelnen Wörter in größere Segmente zusammenzufassen, damit bei einer Suchfunktion der Kontext von umliegenden Wörtern miteinbezogen werden kann.

Für die Wahl der richtigen Segmentgröße gibt es keine einheitliche Lösung.
Die Wahl hängt unter anderem von der Strukturierung des zu embeddenden Inhaltes, den Fähigkeiten des Embedding-Modells und der Länge und Komplexität der einzelnen User-Queries ab.

Der primitivste Ansatz wäre, die basis Transkripte einfach in Blöcke einer bestimmten Größe aufzuteilen.
Dann wäre jedes Segment zum Beispiel 50 Wörter lang.
Der offensichtliche Nachteil eines solchen Ansatzes ist, dass die Segmente keinen Bezug zu dem Inhalt der Transkripte besitzen.
Die resultierenden Segmente würden mitten im Satz anfangen und aufhören und dadurch auch Informationen schlecht repräsentieren.
Zusätzlich würden diese Segmente bei der Rückübersetzung in Audio eine schlechte Nutzererfahrung bieten, da auch die Audiosegmente dann mitten im Satz anfangen und enden würden.

Ein etwas organisierterer Ansatz besteht darin, die einzelnen Wörter zunächst in Sätze zu gruppieren.
Für diese Gruppierung werden zwei Ansätze betrachtet.
Der erste Ansatz besteht darin, die von Whisper erzeugten Satzpunkte als Trennzeichen für einen Satz zu erlauben.
Als zweiter Ansatz wird eine tiefergehende Analyse der Transkripte mithilfe der Sprachbibliothek spaCy durchgeführt.

\subsection{Satzbildung mit Whisper}

Whisper erkennt von sich aus, ob ein Wort das Ende eines Satzes markiert.
Das geschieht wahrscheinlich vor allem aufgrund der Tonlage und der Pause zwischen Wörtern und der grammatikalischen Struktur der vorangegangenen Wörter~\cite{biron2021}~\cite{radford}.
Dann gibt Whisper das Wort mit einem Punkt, einem Fragezeichen oder einem Ausrufezeichen am Ende als erkanntes Wort aus.
Diese Interpunktion kann benutzt werden, um die einzelnen Wörter in Sätze zu gruppieren.
Leider ergibt sich dabei das Problem, dass bestimmte Wörter oder Abkürzungen zusätzliche Punkte enthalten.
Beispiele sind „Mr. Smith“, „seinem 26. Studioalbum“, „am 10. Januar 2016“.
Mit diesem naiven Ansatz der Satztrennung würden einige Sätze an ungewollten Stellen in mehrere Sätze aufgetrennt werden.
Die resultierenden Sätze bieten weniger inhaltlichen Zusammenhang und sind meist grammatikalisch nicht vollständig, was bei einer Rücküberführung in Audio eine schlechtere Nutzererfahrung ermöglicht.

\subsection{Satzbildung mit spaCy}

Um dieses Problem zu lösen, muss ein Algorithmus verstehen, welche Satzzeichen die wirklichen Satzenden anzeigen.
Dies erfordert ein Verständnis der Satzstruktur und ist deswegen nicht trivial möglich.
Um die Satzgrenzen zu finden, wird deshalb die Open-Source-Sprachbibliothek spaCy verwendet.~\cite{honnibal2017}

Mithilfe der NLP-Bibliothek spaCy kann ermittelt werden, welche Satzzeichen wirklich die Grenze eines Satzes markieren.
spaCy verwendet dafür Machine-Learning-Modelle, die aufgrund von vielen Daten gelernt haben, wo das Ende eines Satzes ist.
Auf der offiziellen Webseite von spaCy werden vier verschiedene Modelle für die deutsche Sprache zur Verfügung gestellt.
Das Modell \verb|„de_core_news_sm“| ist das kleinste Modell mit 13 MB Größe, dann folgt \verb|„de_core_news_md“| mit einer Größe von 42 MB und \verb|„de_core_news_lg“| ist das größte Modell mit 541 MB.
Das größte Modell bietet dabei vor allem viele vortrainierte Embedding-Vektoren für einzelne Wörter.
Das vierte Modell \verb|„de_dep_news_trf“| verbraucht 391 MB Speicherplatz und benutzt eine Transformer-Architektur.

spaCy führt eine selbst angegebene Accuracy Evaluation für Sentence Segmentation, also das Erkennen von Satzenden in einem Text, durch.
Darin ist der F-Score für das kleine Modell gleich 0,94; für das mittlere Modell 0,95 und für das große Modell ebenfalls 0,95.
Das Modell mit der Transformer-Pipeline bietet einen F-Score von 0,98 und wird deswegen in dieser Arbeit verwendet.~\cite{spacy2024}

Mithilfe dieses Ansatzes wird der folgende Satz beispielsweise richtig erkannt:
„Zwei Tage nach seinem 69. Geburtstag und der Veröffentlichung von Blackstar, seinem 26. Studioalbum.“


\subsection{Segmente anpassen}

Da wichtige Informationen über mehrere Sätze verteilt liegen können, kann es bei einfacher Segmentierung in Sätzen zu dem Problem kommen, dass Informationen über mehrere Sätze verteilt sind.
Oft ist das zum Beispiel bei Pronomen der Fall, wenn diese sich auf ein Nomen in einem vorherigen Satz beziehen.
Es gibt mehrere Möglichkeiten, dieses Problem anzugehen, zum Beispiel, indem Koreferenzauflösung durchgeführt wird und dadurch Pronomen durch ihre eigentlichen Substantive ersetzt werden.
In einem weiteren Ansatz könnten Segmente aus mehreren Sätzen gewählt werden, die sich gegenseitig überlappen.
Dann würde jedes Segment auch den Kontext von mehreren Sätzen darstellen und die Informationen über Satzgrenzen hinaus erfassen.
Beide Ansätze wurden in dieser Arbeit nicht verfolgt, bleiben aber für zukünftige Forschung offen.

\subsection{Segmente für TF-IDF vorbereiten}

\subsubsection{Lemmatisierung}

Für die Suche mithilfe des TF-IDF Algorithmus, ist es nützlich die Segmente vorher zu bearbeiten.
Um diese Abbildung von mehreren Worten auf ein Stammwort zu erreichen, gibt es zwei unterschiedliche Methoden:
Die Methode Stemming versucht regelbasiert Suffixe von Worten zu ersetzen, um aus Wörtern mit verschiedenen Endungen, wie zum Beispiel \qt{Wanderer} und \qt{Wanderung} zu einem Wort \qt{Wander} zu reduzieren.
Der Vorteil ist, dass der Algorithmus sehr schnell agieren kann, um die Worte auf ihre Stammform zu reduzieren.
Die Nachteile sind allerdings, dass die Deutsche Sprache sehr viele verschiedene Wortkonstrukte erlaubt, die nicht alle mithilfe von einzelnen Regeln auf ein gemeinsammes Stammwort gebracht werden können. 
Zum Beispiel bei dem Plural von \qt{Baum}; \qt{Bäume}.


\subsubsection{Kompositatrennung Pyphen vs german compound splitter}

Als weiteren Vorverarbeitungsschritt für die effiziente Keywordsuche kann man zusammengesetzte Wörter in ihre bestandteile auftrennen.
Das Vokabular des Datensazes besteht zu 61\% aus Nomen. 
Die meisten der Nomen sind zusammengesetzte Nomen, manche davon sind sehr lang, wie zum Beispiel \qt{Reichsdeputationshauptschlussakte}, oder \qt{Hochgeschwindigkeitstransportmittel}.
In dem ganzen Vokabular, das aus den Transkriptionen von Whisper stammt, besteht fast die Hälfte (47,8\%) aus zehn oder mehr Buchstaben.
Da für exakte Keywortsuche solche Begriffe fast nie auftreten, lohnt es sich, 


Dabei gibt es allerdings keine Möglichkeit, die verschiedenen Treffer dieser suche nach Relevanz zu hierarchisieren. Sucht man zum Beispiel nach dem Stichwort \qt{Klimakrise} würden dabei mehrere Stunden Material zusammenkommen [QUELLE]. 
Man könnte nun einfach die Ersten Segmente nehmen, die zusammen die vorgegebene Zeit überbrücken. 
Allerdings ist dieser Ansatz wenig Vielversprechend. 




\section{Datenspeicherung}

\subsection{Datenspeicherung SQLite}

Für die Analyse werden die MP3-Dateien der einzelnen Podcast-Episoden in einem separaten Ordner gespeichert und die Benennung der originalen Dateien stammt aus der Download-URL.
Zur Speicherung der Transkripte wird das relationale Datenbankmanagementsystem (RDBMS) SQLite eingesetzt. 
SQLite ist für Datenanalysezwecke sehr gut geeignet, weil es simpel und performant ist und als Open-Source-Projekt kostenlos verwendet werden kann.
Im Gegensatz zu anderen RDBMS, wie MySQL oder PostgreSQL, arbeitet SQLite serverunabhängig und speichert alle Tabellen in einer einzigen Datei, was sehr nützlich für den Datenaustausch zwischen verschiedenen Geräten ist.
Für einige Operationen, wie die Transkription der Audiodateien oder die Generierung von Embeddings, wird die Hardware eines High-Performance-Clusters der Technischen Hochschule Nürnberg genutzt.
Um die Daten zu analysieren und zu nutzen, bietet es sich an, auf Consumer-Hardware zu wechseln, und dafür ist eine gute Portabilität der Datenbank von Vorteil.

SQLite unterstützt Datenmengen bis zu 140 TB.
Allerdings wird auf der offiziellen Webseite angegeben, dass ab einer Größe von 1 TB auf serverbasierte RDBMS umgestiegen werden sollte, da die gesamte Datenbank in einem File gespeichert wird und viele Client-Betriebssysteme eine maximale Größe der Dateien vorgeben.
Falls das Projekt in der Zukunft auf mehrere Podcastreihen ausgeweitet wird, sollte vor diesem Hintergrund ein Wechsel des DBMS in Betracht gezogen werden.

\subsection{Datenspeicherung Vektoren SQLite}

Da SQLite nativ keine Listen oder Tabellen als Einträge in einer Datenbank speichern kann, ist es nicht trivial, die Embeddings abzuspeichern.
Zunächst wurde die Möglichkeit in Betracht gezogen, jeden einzelnen Eintrag aus einem Embedding-Vektor als separaten Eintrag in einer Tabelle zu speichern.

Das führt allerdings zu sehr ineffizienten Abfragen der Daten und beim Einfügen von Daten limitiert SQLite die Anzahl der Parameter, sodass längere Embeddings umständlich gestückelt abgespeichert werden müssten.

Als Nächstes wurde überprüft, ob die Embeddings als serialisiertes Array abgespeichert werden können.
Dafür wurde jedes Embedding in einen JSON-String umgewandelt, der dann gespeichert werden soll.
Dabei tritt leider das Problem auf, dass die Daten bei der Verwendung wieder deserialisiert werden müssen.
Dieser Schritt müsste jedes Mal wiederholt werden, wenn eine Suche stattfindet.
Dies dauert sehr lange und ist sehr ineffizient.
Für die Deserialisierung von 400.000 Sätzen mit einem 384-dimensionalen Embedding eines Sentence Transformer-Modells auf einem Intel i7 Quad-Core beträgt die Rechenzeit ca. 45 Minuten.

Zur Datenspeicherung der Embedding-Vektoren muss zwischen Dense- und Sparse-Vektoren unterschieden werden, da für beide Embedding-Methoden unterschiedliche Optimierungen in der Speicher- und Retrieval-Funktionalität vorliegen.

\subsection{Datenspeicherung Dense Vektoren}

Um dichte Vektoren abzuspeichern, bietet es sich an, eine separate Vektordatenbank zu nutzen.
Eine Vektordatenbank ist eine nichtrelationale Datenbank, die darauf spezialisiert ist, eine effiziente Suche in ungeordneten Informationen zu ermöglichen.
Dazu speichert sie die Embedding-Vektoren verschiedener Datenquellen effizient ab und erlaubt Zugriff auf schnelle Suchalgorithmen, wie Approximate Nearest Neighbour-Algorithmen.

Es gibt verschiedene Approximate Nearest Neighbour-Algorithmen, die alle darauf abzielen, den gesamten Suchraum für eine Ähnlichkeitssuche in kleinere Unterräume aufzuteilen.
Dabei werden oft Baum-Strukturen verwendet, um die Suche effizienter zu gestalten.
Speziell Algorithmen wie Hierarchical Navigable Small Worlds werden von vielen Vektordatenbanken benutzt \cite{malkov2020}.

In dieser Arbeit wird die Vektordatenbank Chroma verwendet.
Chroma ist eine einfache Vektordatenbank, die die Daten sowohl lokal als auch über eine Client-Server-Schnittstelle speichern kann.
Das Projekt ist erst im Mai 2022 als Start-up entstanden, verfügt aber mittlerweile über viele Features, die die Datenverwaltung erheblich vereinfachen.
Die Datenbank ist dabei mit einer NoSQL-Datenbank zu vergleichen, indem die Daten nicht relational in Tabellen, sondern in einer Collection als Objekte mit verschiedenen Metadaten gespeichert werden.
Jedes dieser Objekte hat eine Document-Eigenschaft, welche den Inhalt des Dokuments repräsentiert.
Dieser Inhalt ist in diesem Fall ein Ausschnitt aus einem Transkript, könnte aber auch ein Bild oder eine Audiodatei darstellen.

Außerdem besitzt jedes Objekt einen Embedding-Vektor, der mit dem Dokument assoziiert wird.
Innerhalb einer Collection müssen alle Objekte mit derselben Embedding-Methode encodiert werden.
Das sichert die Vergleichbarkeit der verschiedenen Vektoren untereinander.

ChromaDB bietet nativen Support für verschiedene Embedding-Modelle von OpenAI, Hugging Face, Cohere, Instructorembedding und JinaAI~\cite{chroma}.
Dafür müssen nur das Modell und der plattformspezifische API-Schlüssel angegeben werden und Chroma erstellt automatisch für jedes Dokument das Embedding.
Standardmäßig ist das Embedding des Sentence Transformer-Modells all-MiniLM-L6-v2 eingestellt.
Alternativ können auch schon vorgefertigte Embeddings eingefügt und die dazugehörige Embedding-Funktion eingetragen werden.
In dieser Arbeit wurden die Embeddings von OpenAI und all-MiniLM-L6-v2 schon vorher erstellt und dann erst eingetragen.

Chroma bietet außerdem sehr guten Support für effizientes Retrieval von Dokumenten.
Dazu sind schon verschiedene Verfahren der Nearest Neighbour-Suche implementiert.

In diesem Projekt wurde erst in einer späten Phase der Umstieg auf die Vektordatenbank vollzogen, weshalb einige Funktionen, die in dieser Datenbank automatisch integriert sind, noch einmal sehr ausführlich beschrieben werden.

\subsection{Datenspeicherung Sparse Vektoren}

Chroma bietet leider noch keine Unterstützung zur Speicherung von Sparse-Vektoren.
Stattdessen wurde das Python-Modul pickle verwendet, welches darauf spezialisiert ist, Python-Objekte effizient als Bytecode zu serialisieren.
Es bietet auch die Möglichkeit, Datenstrukturen effizient zu serialisieren bzw. zu deserialisieren.
Seit Python 3.8 gibt es auch die Möglichkeit, große NumPy-Arrays effizient zu speichern, was zuvor nur mit der Joblib-Bibliothek möglich war.
Pickle speichert die Daten in einem Python-spezifischen Format ab, was die Portabilität auf andere Systeme stark einschränkt.
Dieses Format erlaubt es aber, verschiedene Eigenschaften besser zu speichern als JSON (z.B. Pointer-Sharing)~\cite{pickle}.

Für den TF-IDF-Algorithmus, bei dem ein Vokabular von mehr als 200.000 Wörtern eine ebensogroße Dimensionalität der Embedding-Vektoren benötigt, würde ein normaler Serialisierungsalgorithmus für die 300.000 Sätze ca. 60 Milliarden Werte abspeichern.
Wenn für jeden dieser Werte eine 32-Bit-Gleitkommazahl als Datentyp abgespeichert würde, wären das ungefähr 240 GB Daten.
Ein Großteil dieser Werte (~99,9\%) sind dabei 0.
Ein NumPy-Array kann diese Werte sehr effizient zusammenfassen und mithilfe von pickle kann dieses kompakte Array effizient abgespeichert werden, wodurch eine tatsächliche Speichergröße von ca. 50 MB entsteht.
Dies ermöglicht auch ein effizientes Laden und Vergleichen der Embeddings, was für 400.000 Sätze auf einer Intel Quad-Core-i7-CPU nur circa eine Sekunde benötigt.