\chapter{Datenbeschaffung und Datenspeicherung}\label{ch:data}

\section{Ausgangslage}

Um eine Zusammenstellung verschiedener Audiosegmente möglichst genau und inhaltlich abgestimmt auf ein bestimmtes Thema zu fokussieren, müssen auch zu diesem Thema passende Audioabschnitte in den Daten verfügbar sein.
Als Audiodaten könnten fast alle Audioressourcen genutzt werden, wie zum Beispiel Beiträge aus Radioprogrammen, die Audiospuren von Videos oder ganze Podcast Episoden.

Die Qualität dieses Systems würde mit steigender Anzahl an Audiomaterial bessere Ergebnisse erzielen, da dann zu vielen Themen mehr Inhalte zur Verfügung stehen.
Allerdings ist es im Umfang dieser Arbeit nicht möglich, alle verfügbaren Audiomaterial für die Erstellung der Podcasts zu verwenden.
Die Audiodaten müssten dazu erst transkribiert werden und anschließend mit diesen Transkripten Embeddings generiert werden, was Zeit- und Ressourcenintensiv ist.
Für diese Arbeit wurde versucht möglichst qualitativ hochwertige Daten zu benutzen, um mit den möglichen Ressourcen und in gegebener Zeit einen qualitativ hochwertigen Prototypen zu erstellen. 

Als Audiomaterial würde sich am besten Podcasts eignen, in denen über verschiedene Themen sachlich gesprochen wird, die Informationen aber gleichzeitig faktisch korrekt wären.
Außerdem sollten die Audioquellen frei verfügbar sein, damit keine Urheberrechtsverletzung stattfindet.


\section{Audiodaten beschaffen}

\subsection{Die ARD-Audiothek}

Die ARD-Audiothek ist in Deutschland eine der gößten Audio- und Podcastanbieter mit mitlerweile über 100.000 verschiedenen verfügbaren Audioinhalten über 41 Millionen Audioabrufen~\cite{statista-a}.
ALle Inhalte unterliegen den journalistischen Grundsätzen der ARD und bieten somit einen sorgfältigen Qualitätsstandard. 
Die verschiedenen Audioinhalte stammen von den einzelnen Landesrundfunkanstalten, der ARD und dem Deutschlandradio und liefern eine Vielzahl verschiedener Inhalte.
Sie enthält über 2000 verschiedene Podcasts, in vielen unterschiedlichen Kategorien, wie Comedy, Sport, Wissenschaft, Wirtschaft, Gesellschaft, Kunst, Musik oder Philosophie. 
In dieser Audiothek gibt es zudem Hörbücher, Hörspiele, oder Podcasts nur für Kinder.
Die einzelnen Rundfunkanstalten tragen außerdem eigene Podcasts bei, die meist einen regionalen Kontext haben, wie zum Beispiel der Podcast \qt{Giga Grünheide} über das Teslawerk in Brandenburg vom rbb.
Alle diese Inhalte sind kostenlos und frei verfügbar und stellen eine gute Quelle für das Audiomaterial dar, das in dieser Arbeit verwendet wird.


% Dabei liegt die Nutzung der ARD-Audiothek mit 17,9\% auf Platz vier hinter Spotify, Youtube und Amazon Music \cite{mindline-media2023}.

% Im Appstore und im Google Playstore hat die App der ARD-Audiothek jeweils über eine Million Downloads.
% \cite{gotting2023}

\subsection{Podcastreihe Radiowissen}

Zur automatischen Generierung von Podcast Episoden bietet es sich an, dass in den Ausgangsaudios die Sprache klar und verständlich ist und verschiedene Sprecher sich nicht ins Wort fallen bzw. gleichzeitig reden.
Außerdem ist es wünschenswert, die ausgeschnittenen Audiosegmente an klaren Satzgrenzen zu teilen, sodass der Ausschnitt nicht mitten im Satz beginnt und dem Zuhörenden der Kontext vorenthalten wird. 

Aufgrund dieser Kriterien wurde als Datengrundlage die Podcastreihe Radiowissen von bayern2 benutzt. 
Diese ist nicht wie ein klassischer Podcast im Dialogstil aufgebaut, sondern ähnlich wie bei einem Hörspiel wird der Text von einem Manuskript abgelesen. 
Dazu kommen verschiedene Geräusche und verschiedene Stimmen, um den Hörenden mehr Abwechselung zu bieten.

Der Fokus der einzelnen Episoden liegt auf interessanten Beiträgen zu verschiedenen Themen, die häufig Gebiete der Geschichte, Naturwissenschaft, Gesellschaft oder Philosophie umfasst.
Beispielepisoden sind: \qt{Fasten - Verzicht und innerer Gewinn?}, \qt{Die Maus - Anpassungskünstler und gefürchteter Schädling}, oder \qt{Maria Sibylla Merian - Naturforscherin und Künstlerin}.

Die mehr als 2000 Episoden wurden von mehr als 150 verschiedenen Autoren geskriptet.
Dadurch sind die einzelnen Episoden unterschiedlich in der Erzählweise.
In einigen Episoden kommen originale Audiospuren von historischen Aufnahmen vor oder auch Gastbeiträge von Experten. 
Zudem wird beinahe jeder Podcast abwechselnd von mehreren Stimmen vorgetragen, was nachweislich die Aufmerksamkeit von Zuhörenden verbessert~\cite{kang2012}.

\subsection{Datenbeschaffung über die ARD-Audiotheks API}

Die Inhalte der ARD-Audiothek kann man entweder direkt über die Webseite erreichen oder mithilfe einer frei benutzbare Web-GraphQL API abfragen.
(https://api.ardaudiothek.de/graphql) 
Über diese Schnittstelle sind alle Informationen, wie Titel, Beschreibungen, Autoren oder auch der Link zu dem MP3-File jeder Episode abrufbar.

Zunächst müssen alle Downloadlinks zu den einzelnen Episoden ermittelt werden.
Mit der GraphQL-Abfrage \autoref{ch:graphql-1} erhält man alle Download-Links zu den Podcast-Episoden des Podcasts \qt{Radio Wissen} von bayern2.
Das sind (Stand 3. Januar 24) 2257 Podcast Episoden.

Alle diese Audiodatein wurden anschließend heruntergeladen und auf einer lokalen Festplatte gespeichert.

Für weitere Analysen und die Kategorisierung der Audiodatein ist es außerdem Sinnvoll, die Beschreibungen der einzelnen Episoden und die dazugehörenden Schlagwörter abzufragen, da diese eine kurze Zusammenfassung oder Einordnung der Episoden enthalten.
Außerdem bietet das Entstehungsdatum der Episoden die Möglichkeit, Informationen später nach Aktualität zu filtern.
Über die Anfrage \autoref{ch:graphql-2} können all diese Informationen abgefragt werden.

Über die API kann auch in einigen Fällen direkt ein Transkript des Audiofiles angefordert werden. 
Allerdings ist die Transkription meist nicht sehr akkurat.
Nähreres dazu in Kapitel \autoref{ch:method}


\subsection{Audiodatenanalyse}


Die 2232 einzigartigen Episoden haben eine durchschnittliche Länge von 22 Minuten und eine durchschnittliche Größe von 21 MB.
Insgesamt weisen diese Audiodaten eine Größe von ungefähr 47 GB auf. 

Dabei kam es insgesamt 15-mal vor, dass zwei Episoden denselben Titel tragen, aber eine unterschiedliche Download-URL aufwiesen.
Die Download-URLs unterscheiden sich nur, indem am Ende die Zeichen \qt{-1} oder \qt{-2} angefügt wurden.
Zum Beispiel hat die Episode \qt{Quantenphysik - Wahr, aber verrückt} den Downloadlink https://media.neuland.br.de/file/1804047/c/feed/quantenphysik-wahr-aber-verrueckt.mp3 aber auch https://media.neuland.br.de/file/2069613/c/feed/quantenphysik-wahr-aber-verrueckt-1.mp3.
In diesem Fall liefert nur die zweite URL einen Download, die erste zeigt eine Fehlermeldung an.
Es gibt auch Fälle in denen beide Links funktionieren, wie zum Beispiel 
https://media.neuland.br.de/file/32891/c/feed/die-bamberger-hexenprozesse-unschuldig-muss-ich-sterben.mp3 und
https://media.neuland.br.de/file/1858845/c/feed/die-bamberger-hexenprozesse-unschuldig-muss-ich-sterben-1.mp3 
Die Daten wurden dementsprechend bereinigt und doppelte Audioinhalte nur einmal abgespeichert.



\includegraphics[width=\linewidth]{figures/mp3_length.png}

Der Graph zeigt die Verteilung der Längen der Audiodatein.
Der kürzeste Podcast ist \qt{ab-jetzt-in-der-ard-audiothek-kinder-der-flucht-frauen-erzaehlen.mp3}, der nur 2 Minuten 30 Sekunden lang ist und ein Teaser für einen anderen Podcast darstellt.
Die längste MP3 Datei ist mit Abstand \qt{deportation-und-exil-eine-polnische-odyssee-im-zweiten-weltkrieg.mp3} mit einer Länge von mehr als 53 Minuten.
Diese Episode ist mehr als doppelt so lang wie eine durchschnittliche Episode und bildet einen Ausnahmefall.
Für das System werden trotz der verschiedenen Längen alle Audiodatein benutzt, da sie später in kleinere Segmente zerteilt werden.


\section{Transkription der Podcasts (ASR)}

\section{Vorgehensweise}

Zunächst müssen die Daten gesammelt und aufbereitet werden. 
Für dieses Projekt bildet die Datengrundlage die Transkripte, bzw. Manuskripte der Podcasts der ARD-Audiothek. 
In der Audiothek selber gibt es keine Transkripte zu den Podcasts. 
Für den Podcast \qt{radiowissen} von bayern2 gibt es auf deren Seite die Manuskripte in PDF Format. 
Diese sind zwar inhaltlich hochqualitativ, da Sie exakte Wortwahl der Podcasts enthalten, als PDF Format sind sie allerdings schwierig maschinell auszulesen und weiterhin besitzen sie keine Zeitinformationen zu den einzelnen Wörtern. 
Die Zeitinformationen in Form von Zeitstempeln für jedes Wort sind wichtig, um die Audiofiles der Podcasts später an den richtigen Stellen zuzuschneiden. 

Ein anderer Ansatz ergibt sich, wenn man die Podcasts transkribiert. 
Die Vorteile sind, dass die Transkription auch bei Podcasts funktioniert, für die vorab kein Transkript erstellt wurde, was die Mehrzahl aller Podcasts ausmacht. 
Außerdem kann man bei einer Transkription auch gleichzeitig die Zeitstempel für jedes Wort extrahieren.

\subsection{Transkriptionsmethoden im Überblick}

Um automatisch Audioinhalte mittels eines Automatic Speech Recognition Systems (ASR) zu transkribieren, können mittlerweile zahlreiche verschiedene Dienste benutzt werden.
Dabei muss immer zwischen Geschwindigkeit, Genauigkeit und Kosten abgewogen werden.
Es gibt viele Cloudanbieter, die die Transkription via API gegen Gebühren vornehmen.
Außerdem gibt es verschiedene Transkriptionsprogramme, die auf lokaler Hardware eingesetzt werden können.
Im Folgenden sind mehrere Ansätze beschrieben, die für die Transkription der Daten in Erwägung gezogen wurden.


\subsection{Fraunhofer IAIS}

Wie im Kapitel \autoref*{ch:theoretical} beschrieben führt das Fraunhofer-Institut für Intelligente Anlyse- und Informationssysteme (IAIS) schon automatische Transkriptionen der Audioinhalte der ARD-Audiothek durch.
Die Verwendeung dieser Transkripte führt allerdings gleich zu mehreren Problemen.
Ein Grund ist die inkonsistente Verfügbarkeit der Daten.
Von den angefragten Transkripten verfügten 84 Episoden und damit ca. 3,8\% aller Episoden über keine Daten für Transkripte.
Ein weiterer Grund ist die vorgefertigte Segmentierung des Transkripts.
Die Transkripte sind in Segmente aufgeteilt, die zwischen 3-30 Sekunden dauern.
Zeitstempel sind jeweils nur für den Anfang und das Ende jedes Segmentes verfügbar.
Dadurch ist eine spätere Segmentierung nicht mehr möglich und die vorhanden Segmente sind zu inkonsistent in Länge und beginnen und enden nicht an Satzgrenzen.




\subsection{Whisper}

Faster-whisper - word-level timestamps

Eines der besten kostenlosen Open-Source ASR Systeme bietet Whisper von OpenAI. 
Dieses System wurde mit einem maschinellen lernverfahren auf 680 000 Stunden Audiomaterial in verschiedenen Sprachen trainiert und erreicht damit State-of-the-Art Performance in Transkriptionen.
Es ist sehr leistungsstark und kann lokal auf eigener Hardware laufen.~\cite{radford}

Konkret wurde in dieser Arbeit das Projekt faster-whisper~\cite{faster-whisper2024} verwendet, welches die ursprünglichen Whispermodelle mithilfe einer schnelleren Inferenz Engine neu implementiert.
faster-whisper bietet außerdem word-level timestamps im Gengensatz zu dem ursprünglichen Whisper Projekt.

Whisper bietet mehrere verschiedene Modelle zur Transkription an. 
Es gibt die Modelle tiny, base, small, medium und large. 
Das Basismodell hat ca. 74 Millionen Parameter und benötigt ca. 1 GB VRAM und ist ca. 16-mal schneller als das large Model. 
Bei einem Test für die Episode 1968-das-ausnahmejahr aus dem Podcast Radiowissen von br2 schneidet es aber nicht sehr gut ab. 
Aus dem Wort „Vietnam“ wird „Wirdnam“, aus „Panzer in Prag“ wird „Panzer-Inprac“ und aus  „Ohrfeige“ wird „Urfeige“. 
Der vorgetragene Text wurde dabei ohne Störgeräusche und von einer Person flüssig vorgetragen. 
Eine Evaluation der Transkriptionsfähigkeiten mittels einer Word error rate (WER) ist nicht durchgeführt worden, da notwendige Vergleichstranskripte nicht vorhanden sind.

Dagegen bietet das Medium Model von Whisper deutlich bessere Ergebnisse für dieselbe Episode. 
Bei der Transkription konnte kein Fehler festgestellt werden. 
Allerdings ist der Zeitaufwand durch höhere Rechenleistung immens. 
Auf einem Macbook Pro 2016 mit einem Intel Core i7 benötigt die Transkription ca. 45 Minuten pro Episode. 
Auf einer T4 GPU, wie sie Google kostenlos auf Google Colab zur verfügung stellt, dauert eine Transkription immer noch 3,5 Minuten. 
Für ca. 1000 Episoden bräuchte man demnach ca. 3500 Minuten (ca. 58 Stunden). 

Die Transkription für diese Arbeit wurde auf einem High Performance Cluster der Technischen Hochschule Nürnberg durchgeführt.


Drawbacks von Whisper:
Medium Modell immer noch einige Fehler: 
- gubt statt Grub
- wenn Sprecherwechsel manchmal ganze Sätze weg (im-bann-des-mondes-archaische-mythen-und-religionen.mp3)

TODO large ausprobieren, auf den Servern der Uni und mit der Ebu API
Eurovox


Die Transkripte der Episoden sind meißt ca. 3000 Wörter lang und benötigen ungefähr 20 KB Speicherplatz pro Transkript.

\includegraphics[width=\linewidth]{figures/transcript_length.png}


\subsection{Eurovox}

Das Problem der langen Wartezeiten lässt sich umgehen, wenn Cloud computing benutzt wird.
Das heißt, das Whisper model nicht auf der lokalen Hardware laufen zu lassen, sondern zum Beispiel auf den Servern von Eurovox. 
Eurovox ist ein Software tool von der EBU, der Europian Broadcast Union. 
Sie ist ein zusammenschluss von derzeit 68 Rundfunkanstalten in 56 Staaten Europas, Nordafrikas und Vorderasien mit Sitz in Genf. [4] 
Das tool Eurovox steht dabei allen Mitgliedern zur Verfügung.  
Mithilfe dieses Tools kann man Text-to-Speech, Übersetzungen und Speech-to-Text Services über eine UI benutzen. 
Es gibt sogar die Möglichkeit wärend eines Streams live audio captions zu erzeugen. 
Für dieses Projekt benutzen wir aber zunächst nur die Translation Funktion. 
Außerdem verwenden wir die API von Eurovox um später das ganze Projekt auf mehr Episode Transkripte ausweiten zu können. 
Laut den Entwicklern soll die API auch demnächst open-sourced werden [QUELLE].

Die API stellt, ebenso wie das Tool, eine Reihe verschiedener Anbieter zur verfügung, über die die Audios transkribieren lassen können. 

\section{Datenaufbereitung}

\subsection{Segmentbildung}

Die von Whisper transkribierten Daten enthalten die erkannten Wörter, sowie die einzelnen Zeitstempel für den Start und das Ende jedes Wortes mit einer Genauigkeit von einer Zehntelsekunde.
Für die spätere Analyse der Daten, ist es sinnvoll, die einzelnen Wörter in größere Segmente zusammenzufassen, damit bei einer Suchfunktion der Kontext von umliegenden Wörtern miteinbezogen werden kann.

Für die Wahl der richtigen Segmentgröße gibt es keine einheitliche Lösung.
Die Wahl hängt unter anderem von der Strukturiereung des zu embeddenden Inhaltes, den Fähigkeiten des Embeddingmodels und der Länge und Komplexität der einzelnen User querys ab.

Der primitivste Ansatz wäre, die basis Transkripte einfach in Blöcke einer bestimmten Größe aufzuteilen.
Dann wäre jedes Segment zum Beispiel 50 Worte lang.
Der offensichtliche Nachteil eines solchen Ansatzes ist, dass die Segmente keinen Bezug zu dem Inhalt der Traskripte besitzen.
Die resultierenden Segmente würden mitten im Satz anfangen und aufhören und dadurch auch Informationen schlecht repräsentieren.
Zusätzlich würden diese Segmente bei der Rückübersetzung in Audio eine schlechte Nutzererfahrung bieten, da auch die Audiosegmente dann mitten im Satz anfangen und enden würden.  

Ein etwas organisierterer Ansatz besteht darin, die einzelnen Wörter zunächst in Sätze zu gruppieren.
Für diese Gruppierung werden zwei Ansätze betrachtet.
Der erste Ansatz besteht darin, die von Whisper erzeugten Satzpunkte als  Trennzeichen für einen Satz zu erlauben.
Als zweiter Ansatz wird eine tiefergehende Analyse der Transkripte mithilfe der Sprachbibliothek spaCy durchgeführt.

\subsection{Satzbildung mit Whisper}

Whisper erkennt von sich aus, ob ein Wort das Ende eines Satzes markiert.
Das geschieht wahrscheinlich vor allem aufgrund der Tonlage und der Pause zwischen Wörtern und der grammatikalischen Struktur der vorangegangenen Wörtern~\cite{biron2021}~\cite{radford}.
Dann gibt Whisper das Wort mit einem Punkt, einem Fragezeichen oder einem Ausrufezeichen am Ende als erkanntes Wort aus.
Diese Punktuation kann benutzt werden, um die einzeln Wörter in Sätze zu gruppieren.
Leider ergibt sich dabei das Problem, dass bestimmte Wörter, oder Abkürzungen zusätzliche Punkte enthalten.
Beispiele sind \qt{Mr. Smith}, \qt{seinem 26. Studioalbum}, \qt{am 10. Januar 2016}.
Mit diesem naiven Ansatz der Satztrennung würden einige Sätze an ungewollten Stellen in mehrere Sätze aufgetrennt werden.
Die resultierenden Sätze bieten weniger inhaltlichen Zusamenhang und sind meist grammatikalisch nicht vollständig, was bei einer rücküberführung in Audio eine schlechtere User-experience ermöglicht.


\subsection{Satzbildung mit spaCy}

Um dieses Problem zu lösen, muss ein Algorithmus verstehen, welche Satzzeichen die wirklichen Satzenden anzeigen.
Dies erfordert ein Verständnis der Satzstruktur und ist deswegen nicht trivial möglich.
Um die Sätzegrenzen zu finden, wird deshalb die Open-Source Sprachbibliothek spaCy verwendet.~\cite{honnibal2017}

Mithilfe der NLP Bibliothek spaCy kann ermittlet werden, welche Satzzeichen wirklich die Grenze eines Satzes Markieren.
spaCy verwendet dafür Machine Learning Modelle, die aufgrund von vielen Daten gelernt haben, wo das Ende eines Satzes ist.
Auf der offiziellen Webseite von spaCy werden vier verschiedene Modelle für die deutsche Sprache zur verfügung gestellt.
Das Modell 
\begin{verbatim}
    de_core_news_sm 
\end{verbatim} 
ist das kleinste Modell mit 13 MB Größe, dann folgt 
\begin{verbatim} 
    de_core_news_md 
\end{verbatim}
mit einer Größe von 42 MB und 
\begin{verbatim}
    de_core_news_lg 
\end{verbatim}
ist das größte Modell mit 541 MB.
Das größte Modell bietet dabei vor allem viele vortrainierte Embedding Vektoren für einzelne Wörter.
Das vierte Modell 
\begin{verbatim}
    de_dep_news_trf
\end{verbatim}
verbraucht 391 MB Speicherplatz und benutzt eine Transformerarchitektur.

spaCy führt eine selbst angegebenen Accuracy Evaluation für Sentence Segmentation, also das Erkennen von Satzenden in einem Text.
Darin ist der F-Score für das kleine Modell gleich 0,94; für das mittlere Modell 0,95 und für das große Modell ebenfalls 0,95.
Das Modell mit der Transformer pipeline bietet einen F-score von 0,98 und wird deswegen in dieser Arbeit verwendet.~\cite{spacy2024}

Mithilfe dieses Ansatzes wird der folgende Satz beispielsweise richtig erkannt: 
\qt{Zwei Tage nach seinem 69. Geburtstag und der Veröffentlichung von Blackstar, seinem 26. Studioalbum.}

\subsection{Segmente anpassen}

Da wichtige Informationen über mehrere Sätze verteilt liegen können, kann es bei einfacher Segmentierung in Sätzen zu dem Problem kommen, dass Informationen über mehrere Sätze verteilt sind.
Oft ist das zum Beispiel bei Pronomen der Fall, wenn diese sich auf ein Nomen in einem vorherigen Satz beziehen.
% Zum Beispiel die Satz "Die mächtige prächtige Stadt Amsterdam. In ihr blüht das Leben"
Es gibt mehrere Möglichkeiten dieses Problem anzugehen, zum Beispiel, indem Koreferenzauflösung durchführt wird und dadurch Pronomen durch ihre eigentlichen Substantive ersetzt werden.
In einem weiteren Ansatz könnten Segmmente aus mehreren Sätzen gewählt werden, die sich gegenseitig überlappen.
Dann würde jedes Segment auch den Kontext von mehreren Sätzen darstelln und die Informationen über Satzgrenzen hinaus erfassen.
Beide Ansätze wurden in dieser Arbeit nicht verfolgt, bleiben aber für zukünftige Foschung offen.


\section{Datenspeicherung}

\subsection{Datenspeicherung SQLite}

Für die Analyse werden die MP3-Dateien der einzelnen Podcast-Episoden in einem seperaten Ordner gespeichert und die Benennung der originalen Dateien stammt aus der Download-URL.
Zur Speicherung der Transkripte wird das relationale Datenbank Managment System (RDBMS) SQLite  eingesetzt. SQLite ist für Datenanalysezwecke sehr gut geeignet, weil es simpel und performant ist und als opensource-Projekt kostenlos verwendet werden kann.
Im Gegensatz zu anderen RDBMS, wie mySQL oder PostgreSQL arbeitet SQLite serverunabhängig und speichert alle Tabellen in einer einzigen Datei, was sehr nützlich für den Datenaustausch zwischen verschiedenen Geräten ist.
Für einige Operationen, wie die Transkription der Audiodateien oder die Generierung von Embeddings wird die Hardware eines High-Performance Clusters der Technischen Hochschule Nürnberg genutzt. 
Um die Daten zu analysieren und zu nutzen bietet es sich an, auf Consumer-Hardware zu wechseln und dafür ist eine gute Portabilität der Datenbank von Vorteil.

SQLite unterstützt Datenmengen bis zu 140 TB. 
Allerdings wird auf der offiziellen Webseite angegeben, dass ab einer Größe von 1 TB auf serverbasierte RDBMS umgestiegen werden sollte, da die gesamte Datenbank in einem File gespeichert wird und viele Client-Betriebssysteme eine maximale Größe der Dateien vorgeben. 
Falls das Projekt in der Zukunft auf mehrere Podcastreihen ausgeweitet wird, sollte vor diesem Hintergrund ein Wechsel des DBMS in Betracht gezogen werden.

\subsection{Datenspeicherung Vektoren SQLite}

Da SQlite nativ keine Listen oder Tabellen als Einträge in einer Datenbank speichern kann, ist es nicht trivial, die Embeddings abzuspeichern.
Zunächst wurde die Möglichkeit in Betracht gezogen, jeden einzelnen Eintrag aus einem Embedding-Vektor als seperaten Eintrag in einer Tabelle zu speichern. 

Das führt allerdings zu sehr ineffizienten Abfragen der Daten und beim Einfügen von Daten limitiert SQLite die Anzahl der Parameter, sodass längere Embeddings umständlich gestückelt abgespeichert werden müssten.

Als Nächstes wurde überprüft, ob die Embeddings als serialisiertes Array abgespeichert werden können.
Dafür wurde jedes Embedding in einen JSON-String umgewandelt, der dann gespeichert werden soll. 
Dabei tritt leider das Problem auf, dass die Daten bei der Verwendung wieder deserialisiert werden müssen.
Dieser Schritt müsste jedes Mal wiederholt werden, wenn eine Suche stattfindet. 
Dies dauert sehr lange und ist sehr ineffizient. 
Für die deserialisierung von 400.000 Sätzen mit einem 384 dimensionalen Embedding eines Sentence Transformer Modells auf einem Intel i7 Quad-Core beträgt die Rechenzeit ca. 45 Min.

Zur Datenspeicherung der Embedding-Vektoren muss zwischen Dense- und Sparce-Vektoren unterschieden werden, da für beide Embedding-Methoden unterschiedliche Optimierungen in der Speicher und Retrival Funktionalität vorliegen. 

\subsection{Datenspeicherung Dense Vektoren}

Um Dichte-Vektoren abzuspeichern, bietet es sich an, eine seperate Vektordatenbank zu nutzen.
Eine Vektordatenbank ist eine nichtrelationale Datenbank, die darauf spezialisiert ist, eine effiziente Suche in ungeordneten Informationen zu ermöglichen.
Dazu speichert sie die Embedding-Vektoren verschiedener Datenquellen effizient ab und erlaubt Zugriff auf schnelle Suchalgorithmen, wie Approximate Nearest Neighbour Algorithmen.

Es gibt verschiedene Approximate Nearest Neighbour Algorithmen, die alle darauf abzielen, den gesamten Suchraum für eine Ähnlichkeitssuche in kleinere Unterräume aufzuteilen.
Dabei werden oft Baum-Strukturen verwendet, um die Suche effizienter zu gestalten.
Speziell Algorithmen wie Hierarchical Navigable Small Worlds werden von vielen Vektordatenbanken benutzt.

In dieser Arbeit wird die Vektordatenbank Chroma verwendet. 
Chroma ist eine einfache Vektordatenbank, die die Daten sowohl lokal als auch über eine Client-Server-Schnittstelle speichern kann.
Das Projekt ist erst im Mai 2022 als Start-up entstanden, verfügt aber mittlerweile über viele Features, die die Datenverwaltung erheblich vereinfachen.
Die Datenbank ist dabei mit einer NoSQL-Datenbank zu vergleichen, indem die Daten nicht relational in Tabellen, sondern in einer Collection als Objekte mit verschiedenen Metadaten gespeichert werden.
Jedes dieser Objekte hat eine Document-Eigenschaft, welche den Inhalt des Dokuments repräsentiert.
Dieser Inhalt ist in diesem Fall ein Ausschnitt aus einem Transkript, könnte aber auch ein Bild oder eine Audiodatei darstellen.

Außerdem besitzt jedes Objekt einen Embedding-Vektor, der mit dem Dokument assoziiert wird. 
Innerhalb einer Collection müssen alle Objekte mit derselben Embedding-Methode encodiert werden.
Das sichert die Vergleichbarkeit der verschiedenen Vektoren untereinander.

ChromaDB bietet nativen Support für verschiedene Embeddingmodelle von OpenAI, Huggingface, Cohere, instructorembedding und JinaAI~\cite{chroma}.
Dafür müssen nur das Model und der plattformspezifische API-KEY angegeben werden und Chroma erstellt automatisch für jedes Dokument das Embedding.
Standardmäßig ist das Embedding des Sentence Transformer Modells all-MiniLM-L6-v2 eingestellt.
Alternativ können auch schon vorgefertigte Embeddings eingefügt und die dazugehörige Embedding Funktion eingetragen werden.
In dieser Arbeit wurden die Embeddings von OpenAI und all-MiniLM-L6-v2 schon vorher erstellt und dann erst eingetragen.

Chroma bietet außerdem sehr guten Support für effizientes Retrieval von Dokumenten.
Dazu sind schon verschiedene Verfahren der Nearest Neighbour Suche implementiert.

In diessem Projekt wurde erst in einer späten Phase der Umstieg auf die Vektordatenbank vollzogen, weshalb einige Funktionen, die in dieser Datenbank automatisch integriert sind, noch einmal sehr ausführlich beschrieben werden.


\subsection{Datenspeicherung Sparce Vektoren}

Chroma bietet leider noch keine Unterstützung zur Speicherung von Sparce Vektoren.
Stattdessen wurde das Python-Modul pickle verwendet, welches darauf spezialisiert ist, Python Objekte effizient als Bytecode zu serialisieren. 
Es bietet auch die Möglichkeit, Datenstrukturen effizient zu serialisieren beziehungsweise zu deserialisieren.
Seit Python 3.8 gibt es auch die Möglichkeit, große NumPy Arrays effizient zu speichern, was zuvor nur mit der joblib Bibliothek möglich war.
Pickle speichert die Daten in einem Python spezifischen Format ab, was die Portierbarkeit auf andere Systeme stark einschränkt. 
Dieses Format erlaubt es aber verschiedene Eigenschaften besser zu speichern als JSON (z.B. Pointer sharing)~\cite{pickle}.

Für den TF-IDF Algorithmus, bei dem ein Vokabular von mehr als 200.000 Wörtern eine ebensogroße Dimensionalität der Embeddingvektoren benötigt, würde ein normaler Serialisierungsalgorithmus für die 300.000 Sätze ca 60 Milliarden Werte abspeichern.
Wenn für jeden dieser Werte eine 32 Bit Gleitkommazahl als Datentyp abgespeichert würde, wären das ungefähr 240 GB Daten. 
Ein Großteil dieser Werte (~99,9\%) sind dabei 0.
Ein Numpy Array kann diese Werte sehr effizient zusammenfassen und mithilfe von pickle kann dieses kompakte Array effizient abgespeichert werden, wodurch eine tatsächliche Speichergröße von ca. 50 MB entsteht.
Dies ermöglicht auch ein effizientes laden und vergleichen der Embeddings, was für 400.000 Sätze auf einer Intel Quad-Core i7 CPU nur circa eine Sekunde benötigt.

In der Zukunft könnte dieser Schritt durch einen Umstieg auf eine Plattform wie Elasticsearch vereinfacht werden.




