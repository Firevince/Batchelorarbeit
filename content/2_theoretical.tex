\chapter{Ausgangslage und theoretische Grundlagen}\label{ch:theoretical}

\section{Erstellung einer Podcast Episode}

Erfolgreiche Podcasts zu erstellen benötigt viele einzelne Schritte.
Theam:
Am Anfang benötigt man immer eine Idee, die den Podcast bestimmt.
Das kann die Abhandlung eines Themas oder das halten an ein bestimmtes Format sein.
Vorbereitung:
Dann muss eventuell ein Skript vorgeschrieben werden, Gäste eingeladen werden oder das die Audio-Aufnahmebedingungen geklärt werden.
Zur vorbereitung von INterview Podcasts muss sich auf den jeweiligen gegenüber vorbereitet, Fragen formuliert und hintergrundwissen recherchiert werden.
Aufnahme:
Für die Aufnahme müssen die Sprecher vor Ort oder Remote sprechen.
Es gibt Audio und Video Podcasts
Nachbereitung/Editierung:
Zur Nachbereitung gehört das Anpassen der Lautstärke, herausschneiden von ungewünschten Abschnitten, Einfügen von Geräuschen oder zusätzlichen passagen.
Shownotes erstellen:
Der Podcast sollte transkribiert werden und eine Zusammenfassung erstellt werden.
Showbild erstellen:
Ein Bild für diese Episode erstellen.
Werbung schalten:
Kleine Teaser erstellen, die man z.B. auf Social Media teilen kann.


\section{KI in Podcasts}

Künstliche Intelligenz verändert die Branche des Podcastings in vielerlei Hinsicht. 

Transkriptionsmodelle wie Whisper sind in der Lage live Transkriptionen der Podcasts zu erstellen die eine Qualität bestitzen, die mit professionellen menschlichen Transkriptoren mithalten kann \cite{radford}.
Diese können außerdem verschiedene Stimmen unterscheiden und die Emotionen der Sprecher erkennen, was es in der Nachbereitung eines Podcasts erheblich erleichtert bestimmte Stellen zu finden \cite{wagner2023}
Es gibt Ansätze, KI gestützt Teaser von längeren Podcast Episoden zu erstellen, welche dann zu Werbezwecken in Sozial Media geteilt werden können. \cite{wang2023}

Ein großes Entwicklungsfeld in der Podcastbranche ist die komplett automatische Generierung von Podcasts. Die Technologie der automatischen Stimmengenerierung ist soweit fortgeschritten, dass sich künstlich generierte Stimmen fast so gut anhören wie echte Stimmen \cite{shi2023}.

Laut der Plattform Podcastle, die Software für Aufnahme und Editierung von Podcasts herstellt, 
erreichen heute schon AI-basierte Podcasts rund 45 Millionen US-Amerikaner \cite{podcastle2023}.
Der Podcast "Hacker News Recap" ist ein vollkommen von AI produzierter Podcast und erreicht in vielen Ländern wie Schweden, oder Italien die Top 100 bei Apple Podcasts in der Kategorie Daily News \cite{chartable}.
In Deutschland ist der Podcast immerhin auf Platz 169.

\section{Related Work}

Im Bereich der automatisierten Podcast Erstellung gibt es einige Versuche mittels künstlich generierter Texte und Stimmen einen eigenen Podcast zu erstellen.

In dem Artikel \glqq NewsPod: Automatic and Interactive News Podcasts\grqq{} \cite{laban2022} wird ein neuer Ansatz für die automatische Erstellung von Podcast Episoden vorgestellt. 
Dabei ermitteln mehrere Machine Learning Systeme den Inhalt von bestimmten Nachrichtenseiten und extrahieren daraus Fragestellungen, die den Inhalt des Textes wiederspiegeln sollen. Die Autoren benutzen ein GPT-2 Sprachmodell, das auf Fragengenerierung trainiert wurde, um aus jedem Absatz 7 Fragen zu ermitteln. 
Für jede Frage wird dann eine Seperate Antwort generiert. 
Der Wichtigste Aspekt an diesem Artikel ist, dass die Benutzer der Software Interaktiv agieren können und selbstgewählte Fragen mithilfe eines Mikrofons oder einer Tastatur stellen können, die dann von dem System beantwortet werden. 
Die Autoren dieses Papers führen außerdem zwei Studien zur Nutzung dieses Systems durch.
Der Gegenstand der ersten Studie ist das die Zufriedenheit einer Testgruppe mit der Erzählweise des Textes und der automatisch generierten Stimme des Sprechers. 
Die zweite Studie untersucht die Interaktion der Zuhörer während der Benutzung des Systems. 
\cite{laban2022}

In der ersten Studie konnten die Autoren feststellen, dass einer ihrer Ansätze, QA Best,  so erfolgreich war, dass \glqq  80\% of QA Best listeners said they would use the system to listen to the news in the future\grqq{}  \cite{laban2022}.
Die zweite Studie kommt zu dem Schluss, dass zwar die Bereitschaft eigene Fragen zu stellen mit 85\% der Zuhörer sehr hoch war, die Zufriedenheit der Tester mit der Qualität der Antworten aber sehr gering ausfiel, da \glqq  76\% of the answers were rated Irrelevant/Confusing\grqq{} \cite{laban2022}.
Als zweites würde ich die Arbeit von \cite{zhang2020} priorisieren, da sie als Vorarbeit von \cite{laban2022} dient und das wichtige Summarization-Modell PEGASUS vorstellt, das ich auch in meiner Arbeit verwenden kann.




Als weitere Quellen werde ich \cite{karpukhin2020}, \cite{reddy2019} und \cite{choi2018} verwenden, die sich alle mit der automatischen Fragen- und Antwortgenerierung aus Texten beschäftigen.
\cite{karpukhin2020} ist dabei der neueste Ansatz zur Ermittlung von Kontext, der sich außerdem von dem TF-IDF Algorithmus unterscheidet und sehr gute Ergebnisse verspricht.

Für das Design der Podcast Episoden werde ich die Artikel von Podcast Übersicht von \cite{jones2021} verwenden, in dem mithilfe von Deep Learning Ansätzen mehr als 100.000 Podcast Episoden analysiert wurden und daraus Aspekte für eine erfolgreiche Podcast Episode kristalisiert wurden. Diese Aspekte werden mir bei der Gestaltung und dem Aufbau einer Episode helfen. 

In dem Paper \cite{kang2012} untersuchten die Autoren den Effekt von mehrere Stimmen auf das Podcasterlebnis. Außerdem untersuchten Sie in Ihrer Studie die Effekte des Erzählstils und finden heraus, dass informelle Sprache und ein ungezwungener Kommunikationsstil das Erlebnis von Zuhörern verbessert. 



Im weiteren werde ich noch die Arbeiten von \cite{maroni2020}, \cite{clark2020} und \cite{du2017} verwenden, die sich mit Deep Learning Modellen zur Erschließung von Texten beschäftigen.

\section{Information Retrieval}

Das Thema Information Retrieval (Informationsrückgewinnung) bezeichnet den Vorgang aus einer großen Menge unsortierter Daten bestimmte Informationen wieder zu extrahieren.
Im Gegensatz zu Data-Mining werden dabei keine neuen Daten erschaffen, sondern lediglich bereits existierendes Wissen verfügbar gestellt.
Ein bekanntes Beispiele eines Information Retrieval Algorithmus ist der PageRank Algorithmus für die Suchmaschine Google.
Suchmaschinen sind das Musterbeispiel eines Information Retrieval Systems.
Ein User gibt Stichworte ein, über die er mehr Informationen erhalten will und erhält daraufhin Links zu Webseiten, die diese Informationen wahrscheinlich enthalten.

SPLADE

\section{Embeddings}

Das Ziel dieser Arbeit besteht darin, eine Schnittstelle für einen Hörer zu erstellen, die ihm ermöglicht, einen Zusammenschnitt aus vielen verschiedenen Podcast Episoden ist, und genau zu einem Thema passt. 
Die wichtigste Aufgabe besteht also darin, die wesentlichen Abschnitte aus den Episoden herauszufinden. 
Dazu muss das System verstehen, welche Abschnitte zu verschiedenen Anfragen passen. 

Die menschliche Sprache ist ein hochkomplexes Konstrukt mit einer Grammatik, die sehr viel flexibler, kreativer, vieldeutiger, und komplexer ist als maschinensprache. 
Es gibt viele kleine Bedeutungsnuancen, sie ist stark von dem allgemeinen Wissen der Welt geprägt und sie ändert sich im laufe der Zeit. 
Das alles macht es für Computer sehr schwierig die Menschliche Sprache zu Verstehen. 
Neuere Forschung im Bereich des Natural Language Processing bietet einige Ansätze um dieses Problem zu lösen. 
Es gibt dazu viele verschiedene Methoden, die alle darauf abzielen, Worte oder Texte in Vektoren umzuwandeln, die den Inhalt dieser Worte oder dieser Texte repräsentiert.
Diese Vektoren nennt man Embeddings.
Das Ziel dieser Embeddings besteht für unseren Zweck darin, eine Suchfunktion zu beschreiben, die auf eine Anfrage (Query) hin, passende Dokumente aus einem großen Korpus an Dokumenten liefern kann.

Ein relativ moderner Ansatz besteht in der Suche mithilfe von Embedding Vektoren. 
Dabei versuchen wir die Semantik (also die Bedeutung) der einzelnen Segmente Mathematisch mithilfe eines Vektors zu repräsentieren. 
Dieser Vektor soll dann zur Suche nach Ähnlichkeit dienen. Ein Vektor, der aus dem Satz „Ich sitze auf einer Bank im Grünen“ und „Im Park steht eine alte Bank“ sollen dabei sehr Ähnlich zueinander sein, der Satz „Ich gehe in die Bank und hebe Geld ab“ aber nicht sehr Ähnlich. 
Hierbei soll erkannt werden, dass sich die ersten beiden Sätze auf eine Bank zum Sitzen in der Natur beziehen und der dritte Satz das Geldinstitut meint. 
Ein solch filigranes Verständnis der Bedeutung ist nicht einfach zu erreichen. 
Ein Regelbasierender Algorithmus würde die Unterschiede bei solchen Homonymen in keinem Fall erkennen können. 
Regelbasierte Algorithmen wie z.B. [QUELLE] haben auch Probleme mit Negierung wie zum Beispiel in „Ich habe nichts gegessen.“ und „Nichts habe ich heute gemacht außer gegessen“.

Für unsere Aufgabe ist es Sinnvoll nicht nur Inhaltliche Vergleiche zu erstellen, die überprüfen, ob zwei Sätze ungefähr die Gleiche Bedeutung haben, wie "Die Sonne bringt mich zum schwitzen" oder "Ich schmelze in der Hitze".
Besser wäre eine asymmetrische Ähnlichkeit, wie sie bei Frage und Antwortpaaren vorkommt.
Zum Beispiel wäre auf die Frage: "Welche Farbe hat der Himmel bei Sonnenuntergang?" ein gute Antwort: "Der Himmel hat bei Sonnenuntergang oft orange und rosa Farbtöne."  und nicht der Satz "Wie sieht der Himmel am Abend aus?"
Diese Asymmetrische Ähnlichkeit ist schwerer zu ermitteln, da man trivialerweise in der Antwort Informationen findet, die in der Frage nicht vorkommen.

Auf der 

\section{Geschichte der Embeddingverfahren}

Die Geschichte der Embeddings startet mit stochastischen Verfahren, wie dem TF-IDF Algorithmus und dem BM25 Algorithmus. 
Term Frequency - inverse Dokument Frequency (TF-IDF) ist ein Algorithmus um Dokumente nach Keyworten zu durchsuchen.
Das Verfahren verwendet Worthäufigkeiten (Term-Frequency), um Dokumenten, in denen ein Wort häufiger vorkommt zu bevorzugen.
Außerdem verwendet es die generelle Seltenheit von Wörter (Inverse Document Frequency) um Wörtern, die seltener vorkommen mehr Gewicht bei der Suche einräumen.

Daraus bildet der TF-IDF Algorithmus einen dünnbesetzten Vektor, der die Anzahl und Seltenheit jedes im Dokument vorkommendes Wortes abbildet. 
Dünnbesetzt heißt in diesem Zusammenhang, dass der Vektor viele Einträge mit dem Wert 0 besitzt. 
Das folgt aus der Bedingung, dass diese Vektoren untereinander vergleichbar sein müssen und dadurch Einträge für jedes Wort besitzen, welches im gesamten Korpus an Dokumenten vorkommt.
Jedes einzelne Dokument hat dabei aber nur ein Bruchteil der Gesamtwörter und damit viele Nullwerte.

Best Matching 25 (BM25) ist eine Famile von Algorithmen die versuchen den TF-IDF Algorithmus zu verbessern indem sie versuchen, die Priorisierung Langer Dokumente und zu oft vorkommende Worte auszugleichen.

Außerdem kann man diese Algorithmen noch anpassen, sodass sie auch n-gramme als Einträge der Vektoren miteinbeziehen. 
Das heißt, dass nicht nur einzelne Wörter berücksichtigt werden, sondern auch häufig zusammen vorkommende Wörter als einzelnes Token in dem Vektor abgebildet werden. 
Das bewirkt zwar, dass die Ähnlichkeit der Dokumente in der Regel besser ermittelt werden kann, vergrößert aber die Vektoren überproportional.

Semantische Analysen begannen erst ab ca. 1990 mit der Latenten Semantischen Analyse (LSA).
Diese verwendet Eigenvektoren um aus einer Term-Frequency Matrix versteckte (latente) Eigenschaften zu ermitteln, welche die Dokumente besser repräsentieren als die TF Matrix an sich. 
Es werden von diesen Eigenvektoren nur die wichtigsten ausgewählt, sodass jedes Dokument sehr gut als eine linearkombination dieser Vektoren repräsentiert werden kann.
Dann werden nur noch die Koeffizienten dieser Vektoren gespeichert und man kann von einer Dimensionsreduktion profitieren.
Die resultierenden Koeffizienten-Vektoren sind nun nicht mehr spärlich besetzt sondern Dicht.
Die resultierenden Eigenvektoren bilden häufig latente Themen der Dokumente ab.

Wirkliche semantische Word Embeddings kamen erst mit der Veröffentlichung von Word2Vec (2013) in den breiteren Nutzen. 
Word2Vec verwendet ein neuronales Netz, um mithilfe der benachbarten Wörter Kontextinformationen über das eigentliche Wort zu erhalten. 
Dafür gibt es die beiden Ansätze continous Bag of Words und Skipgramm.
Bei dem Ansatz Continous Bag of words werden dem Neuronalen Netz die umgebenden Wörter als Input gegeben, und das Model hat die Aufgabe daraus das Wort zu ermitteln. 
Dieses Verfahren wird in einem Sliding Window für jedes Wort aus einem Text wiederholt. 
Für einen bestimmten Korpus aus verschiedenen Dokumenten kann dieses Modell dadurch die beziehungen verschiedener Worte zueinander und die Ähnlichkeit verschieddener Worte, die oft in gleichem Kontext vorkommen erlerenen. 
Das Verfahren des Skipgramms funktioniert ähnlich, allerdings wird dabei für jedes Wort versucht die umliegenden Wörter zu ermitteln. 
Dieser Ansatz dauert länger im Training, hat aber den Vorteil, dass er auch seltener vorkommende Wörter gut repräsentieren kann.

Ein Jahr später wurde GloVe (Global Vectors for Word Representations) als Forschungsprojekt von der Stanford Universität vorgestellt. 
GloVe verbindet Konzepte aus LSA und Word2Vec, indem es die auf einer globalen Matrix Faktorisierung beruht aber die Distanz von Wörtern zueinander mitberücksichtigt.
Dadurch schaffte GloVe eine weitere Verbesserung der Embeddings und wird heutzutage noch unter anderem in NLP Bibliotheken wie SpaCy verwendet.[englische Wikipedia referiert auf Buch aus 2018]

Im Jahre 2018 wurde der Ansatz ELMo (Embeddings from Language Model) für Word Embeddings vorgestellt.
Dieser benutzt Bidirectionale Long Short Term Memory Modelle (LSTM) um daraus Embeddings zu generieren.
LSTM Modelle basieren auf Recurenten Neuronalen Netzen (RNN) und gehören zu den Sequenz to Sequenz Modellen. 
Sie bestehen dabei aus einem Encoder und einem Decoder.
Der Encoder codiert eine Inputsequenz, zum Beispiel ein Dokument, zu einem Embeddingvektor. 
Der Decoder dekodiert diesen Embeddingvektor wieder zu einer Sequenz, z.B. einer Zusammenfassung des Dokuments. [Quelle]
Für ELMo ist dabei nur der Encoder Teil wichtig.
Dabei werden zwei Layer von Forward und Backward LSTMs eingesetzt, die für jedes Wort den Kontext vor und nach dem Wort berücksichtigen, um sematische Embeddings zu erstellen. [Quellen]

Ein Jahr später wurden die Erstellung von Word und Sentence Embeddings durch BERT revolutioniert.
BERT (Bidirectional Encoder Representations from Transformers) \cite{devlin2019} ist ein auf NLP Aufgaben spezialisierter Transformer.
Das Model wurde von Google 2019 entwickelt und war seinerzeit das Beste Model, indem es in elf verschiedenen Aufgaben im Bereich des NLP State-of-the-art Ergebnisse lieferte.
Es ist auf zwei Datensätzen, dem BooksCorpus mit 800 Millionen Wörtern und der englischen Wikipedia mit 2,5 Milliarden Wörtern trainiert worden. \cite{devlin2019}
Dadurch hat BERT von einer große Auswahl an verschiedenen Themen gelernt und kann für diese sinnvolle Semantische Embedding Vektoren erstellen.













Unter diesen Aufgaben befinden sich Next Sentence Prediction (NSP) und Masked Language Modeling, auf die es auch trainiert wurde.



Außerdem kann man das Modell mit wenig aufwand auf andere Aufgaben finetunen und damit sehr gute Ergebnisse im Bereich der Named Entity Recognition, der Sentiment Analyse oder  










Der Wunsch die menschliche Sprache für den Computer verständlich zu machen ist fast so alt, wie die Computer an sich. 
In den 1940er Jahren, nach Ende des 2. Weltkrieges 
Der Bereich des NLP riesig. 
Es gibt [QUELLE] paper dazu. 
Es hat viele verschiedene Unterthemen. 
Die Aufgaben im Bereich des NlP sind vielseitig . Einige Ansätze 






\section{Transformer Architektur}


Eine Transformerarchitektur ist eine der modersten und leistungsfähigsten Architekturen, um NLP Aufgaben zu lösen. 
Sie bildet dabei den Nachfolger bzw. Konkurenten zu den bis dato vorherschenden Rekurenten Neuronalen Netzen (RNN), Gated Recurenten Units (GRU) oder Long-Short Term Memory Systemen (LSTM). 
Ähnlich wie diese Architekturen, ist auch der Transformer ein Sequenze to Sequenz Model, das der Encoder teil nimmt als Eingabe ein Folge von Tokens (ein Satz, eine Audiodatei) und der Decoder Teil bildet daraus eine andere Sequenz (ein Satz, eine Audiodatei). 
In diesem Abschnitt gehen wir nur auf den Encoder ein, weil er die Embeddings erstellt.

Der Unterschied von Sequenze to Sequenz Modellen zu einem normalen Neuronalen Netz besteht darin, dass die Eingabe und Ausgabe Sequenzen mit variablen Längen sein können, was bei Neuronalen Netzen nicht funktioniert, da sie eine feste Anzahl an Input und Output Neuronen besitzen.

RNNs sind eine weiterentwicklung normaler Neuronaler Netze, indem Sie mehrere Neuronale Netze hintereinander schalten. 
Dadurch wird für jedes Inputtoken ein Embedding generiert. 
Die nächste Zelle des RNN nimmt dann das Embedding der vorhergehenden Zelle und das nächste Input token und generiert daraus wieder ein Embedding.

Ein Problem welches diese Architekturen besitzen ist Vanishing Gradient
Diese Architekturen versuchen immer im Encoder die gesamte Input Sequenz von rechts nach links (forward)oder von links nach rechts (backward) zu verarbeiten, um daraus ein einzelnes Embedding zu erstellen.
Informationen, die in der Mitte der Sequenz vorkommen werden wahrscheinlich von danach folgenden Informationen überschrieben, weil das Modell versuchen muss alle Informationen in die selbe Matrix zu speichern. 

Der Ansatz des LSTM versucht dieses Problem zu lösen, indem er Gated Langzeit Gedächtniszellen einführt, die zusätzlich zu den kurzzeit Gedächtniszellen agieren und nicht so leicht überschrieben werden.
Dadurch können sie wichtige Informationen länger speichern.

Der Unterschied von Transformern zu RNNs, GRUs und LSTMs ist der Attention Mechanismus.
RNNs, GRUs und LSTMs sind darauf angewiesen, den Input sequentiell Token für Token zu verarbeiten, da jede Zelle asl Input das Embedding der vorrausgehenden Zelle benötigt.
Das macht das Training eines Models sehr Zeitaufwendig.

Der Große Vorteil der Transformer ist, dass sie parallelisierbarer sind.
Alle Tokens einer Input Sequenz können synchron verarbeitet werden. 
Dabei wird für jedes Token zunächst durch ein vorher ermitteltes Word Embedding ersetzt.
Der Mechanismus des Positional Encodings stellt sicher, dass das Model die Reihenfolge der tokens mitberücksichtigen kann.
Dieser Mechnisus nimmt berechnet aufgrund der Position des Tokens in der Sequenz ein positional Embedding auf der Grundlage von Sinus und Cosinus funktionen und addiert es auf das vorherige Word Embedding.

Der positional encoded Input wird dann von mehreren self Attention Köpfen verarbeitet, der sogenannten Multihead self attention. 
Jeder dieser Köpfe spaltet den Input in eine Query, Key und Value Matrix auf, indem es das ursprüngliche Embedding jeweils mit einer vortrainierten Query, Key und Value Matrix multipliziert.
Die resultierenden Matrizen werden dann mithilfe folgender Formel zur Attention Matrix umgewandelt:

$Attention(Q,K,V)=softmax(\frac{QK^t}{\sqrt{d_k}})*V$

Der Parameter d stellt dabei die Dimension der Matrix k dar.


Um die Formel besser zu verstehen, können wir an einem Beispiel sehen, was die einzelnen Matrizen für Aufgaben besitzen.
Als Eingabe nehmen wir den Satz: "Alice fuhr gestern durch die Nürnberger Innenstadt".
Die Query Matrix beschreibt nun eine Frage an diesen Satz, zum Beispiel "Wer?".
Die Key Matrix hat nun die Antwort darauf, wo diese Informationen im Satz stehen könnten, also wahrscheinlich im ersten Wort. 
Die beiden Matrizen werden nun Multipliziert, um die Position der Antwort auf die Frage zu erhalten. 
Das Ergebnis wird durch Division mit der Dimension der Matrix skaliert und anschließender Softmax funkion normiert, um die Relevanz der Schlüssel anzupassen.
Anschließend wird die Angepasste Fragen-schlüssel Matrix mit der Value Matrix multipliziert, welche die Informationen der Sequenz beinhaltet. 
An der ersten Stelle steht Alice.

Diese daraus resultierende Matrix wird nocheinmal mit der ursprünglichen Matrix in einer sogenannten residual Connection addiert und normiert, bevor sie in ein einfaches Neuronales Netz gespeist werden.

\section{Large Language Model}

Large Language Model bezwichnet ein Language Model, das auf vielen Parametern trainiert wurde. 
Language Model bezeichnet den Bezug zu dem Verständnis Natürlicher Sprache.
Large Language Models werden vor allem für ChatBots, Text-Übersetzungen, und Sprach-Übersetzungen eingesetzt.
Bekannte LLMs sind zum Beispiel ChatGPT con OpenAI, welches auf ca. 175 Milliarden Parameter trainiert wurde. [https://www.ankursnewsletter.com/p/gpt-4-gpt-3-and-gpt-35-turbo-a-review]
Weitere bekannte LLMs sind Google Bart und Gemini, Megatron von NVIDIA oder Ayax von Apple (das erst noch kommt).
Zu den bekannten kommerziellen LLMs, die von großen Tech Firmen entwickelt werden, kommen immer mehr open-source Modelle in den Vordergrund, die zwar (noch) nicht so Leistungsstark sind, aber auf eigener Hardware laufen können.



\section{Huggingface}

Da es mittlerweile eine Vielzahl unterschiedlicher LLMs gibt, die alle verschiedene spezielle Aufgaben besitzen und in manchen Aspekten Vor- und Nachteile haben, ist es Sinnvoll, diese untereinander vergleichen zu können.
Eine solche Vergleichsmöglichkeit bietet die Plattform Huggingface, auf der über 400.000 Machine Learning Modelle, davon mehr als 200.000 Transformer gehostet werden.
In der Kategorie Question Answering, gibt es über 1300 Einträge und in der Kategorie "Sentence Similarity" ca. 3000.
In beiden Kategorien gibt es Embedding Modelle zu finden.
Die einzelnen Modelle kann man sich kostenlos herunterladen, sie sind ausführlich dokumentiert und man kann sie untereinander mithilfe des Abschneidens auf verschiedenen Kontroll-Datensätzen vergleichen.
Durch Leaderboards kann man die besten Modelle bestimmen.
Zum Beispiel das Massive Text Embedding Benchmark Leaderbord https://huggingface.co/spaces/mteb/leaderboard.\cite{muennighoff2023}

Auf diesem Leaderboard werden Embedding Modelle für unterschiedliche Kategorien evaluiert.
Semantic Textual Similarity 
Classification




\section{SBERT}

\section{LLAMA}
Ein sehr Leistungsfähiges LLM ist das von Meta entwickelte LLama und deren Nachfolger LLama2. \cite{touvron2023}

\section{OpenAI Modelle}

\href{https://twitter.com/Nils_Reimers/status/1487014195568775173}{twitter}

OpenAI stellt außer den Text (GPT-4) und Bild (DALLE) generierungs Modellen auch Modelle zur Embedding erstellung in Ihrer API vor.
Ada (1024 dimensions)
Babbage (2048 dimensions)
Curie (4096 dimensions)
Davinci (12288 dimensions)

\section{Dimensionalität}

\section{SpaCy und dictionaries}
