\chapter{Evaluation verschiedener semantischer Verfahren}\label{ch:experiments}


\section{BERT Embeddings}

BERT (Bidirectional Encoder Representations from Transformers) \cite{devlin2019} ist ein auf NLP Aufgaben spezialisierter Transformer.
Er wurde von Google 2019 entwickelt und war seinerzeit das Beste Modell um verschiedene Aufgaben im Bereich des NLP zu lösen.
Unter diesen Aufgaben befinden sich Next Sentence Prediction (NSP) und Masked Language Modeling, auf das es auch trainiert wurde.
Außerdem kann man das Modell mit wenig aufwand auf andere Aufgaben finetunen und damit sehr gute Ergebnisse im Bereich der Named Entity Recognition, der Sentiment Analyse oder  

\section{Sentence Transformer Embeddings}

Das Sentence Transformer Projekt baut auf der Architektur von BERT auf. 
Es wird auch SBERT für Sentence BERT genannt. 
Unter dem 

\section{LLama2 Embeddings}

\section{Auswertung}