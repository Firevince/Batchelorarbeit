\chapter{Evaluation verschiedener semantischer Verfahren}\label{ch:experiments}


\section{BERT Embeddings}

Um einen Embeddingvektor zu erstellen, benutzen wir das BERT Model. BERT, das Akronym für Bidirectional Encoder Representations from Transformer, ist ein Sprachmodel, das 2018 von Google entwickelt, und zur Benutzung freigegeben wurde. 
BERT ist ein Neuronales Netzwerk mit 12 Schichten, das für zwei verschiedene Aufgaben gleichzeitig trainiert wurde. Zum einen wurde es auf eine Masked Language Modeling Aufgabe und zum Anderen auf eine Next Sentence Prediction trainiert. 

Masked Language Modeling
Bei dieser Aufgabe soll das Model versuchen, aus dem Kontext eines Satzes ein maskiertes Wort in diesem Satz vorherzusagen. Dafür wird dem Model ein Satz gegeben, in dem zufällige Wörter einfach versteckt werden und das Model soll für diese Wörtereine Vorhersage treffen. 


\section{Sentence Transformer Embeddings}

Das Sentence Transformer Projekt baut auf der Architektur von BERT auf. 
Es wird auch SBERT für Sentence BERT genannt. 
Unter dem 

\section{LLama2 Embeddings}

\section{Auswertung}