\chapter{Evaluation verschiedener semantischer Verfahren}\label{ch:experiments}

\section{Ausgangslage}

In diesem Kapitel werden verschiedene Embeddingmodelle miteinander verglichen, um das Modell mit den besten Retrivaleigenschaften zu ermitteln.
Wie in Kapitel \autoref*{ch:theoretical} beschrieben, gibt es bereits Rankings von verschiedenen Embeddingmodellen für ihre Retrieval Funktionalität.
Leider gibt es noch nicht genügend deutsche Datensätze, um die Retrievaleigenschaften der Modelle für Texte der deutschen Sprache zu bewerten.
Spezielle Datensätzen sind zu Zeitpunkt dieser Arbeit nur für Englisch, Chinesisch, Französisch und Polnisch verfügbar~\cite{mteb}.

Die explizite Bewertung im Kontext der eigenen Daten hat außerdem den Vorteil, dass sie noch genauer die Eigenschaften der Modelle für diese Daten bewerten kann.


\section{Auswahl der Embeddings}

Für die Evaluation werden vier verschiedene Embeddings betrachtet.
Dabei wird sowohl der Sparse Vektoren des TF-IDF Algorithmus, wie auch die Dense Vektoren von dem all-mini-LM-L6-v2 Model und Embeddings von Voyage AI und OpenAI miteinander verglichen.

Konkret werden Embeddings des TF-IDF Algorithmus

TF-IDF - vergleich zu Keyword-suche
all-mini-LM-L6-v2 - klein, schnell, most downloaded~\cite{2024}


\section{Themenauswahl}

Die Bewertung des IR Systems erfolgt auf einer Auswahl verschiedener Themen.
Das System berechnet für jedes Thema ein Embedding, vergleicht es mit den Embeddings in der Datenbank und erstellt daraus eine Ranking Liste.
Für jedes Thema wird dann eine sortierte Liste der Segmente zurückgeben, welche benutzt wird, um das Model zu evaluieren.

Die Themen, nach welchen die Embeddings evaluiert werden sollen sind in drei Kategorien eingeteilt:
topical, refinding, known items~\cite{jones2021}
Die entsprechenden Aufgabenstellungen sind in drei Kategorien eingeteilt: topical, re-finding und known items.
Bei der Kategorie topical wird verlangt, passende Segmente zu einem bestimmten Thema zu finden.
Beim re-finding geht es darum, einen zuvor bekannten Audioinhalt wiederzufinden.
Dabei waren nur Teile des Audioinhaltes oder bestimmte Rahmenbedingungen vorgegeben (z.B. eine Podcast-Episode, welche die Person vor einer Woche hörte).
In der letzten Kategorie den known Item sind bereits der Titel bzw.\ weitere Metainformationen bekannt.
Die einzelnen Themen verfügten dabei außerdem noch über eine Beschreibung, die spezifiziert, was eine Testperson von der Suche mit diesem Prompt erwarten würde.


Bei



\section{Bewertungsmethoden}


Für die Bewertung eines Information Retrival Systems können verschiedene Methoden verwendet werden.


Die Güte von einem Information Retrieval System kann mithilfe des normalized discounted cumulative gain (NDCG) beschrieben werden. 
Dafür muss allerdings für jedes Thema bereits eine vorgefertige Vergleichsliste vorhanden sein, mit der man dann die Ergebnisse vergleichen kann.
Diese Vergleichsliste wird normalerweise Manuell erstellt.
Für 400.000 Segmente auf 9 verschiedenen Themen ein manuelles Ranking durchzuführen ist leider im Rahmen dieser Arbeit nicht durchführbar.

Ein weiterer Ansatz für das Ranking der einzelnen Segmente kann mithilfe von LLMs erfolgen.
Leistungsstarke LLMs, wie GPT-4 werden mittlerweile in der Forschung für automatische Bewertungsaufgaben benutzt~\cite{naismith2023}~\cite{nilsson2023}.



\section{Bewertungskriterien}


Die Aufgabe besteht darin, zu untersuchen, wie gut eine Suchfunktion mithilfe der Embeddings in der Lage ist, passende Informationen zu einem Thema zu extrahieren.

Für die Evaluation der verschiedenen Embeddings werden verschiedene Kriterien angelegt.
Einerseits müssen die extrahierten Segmente zu dem Thema passen und nicht irrelevante andere Themen behandeln.
Das ist der wichtigste Punkte bei der Evaluation.

Ein weiteres Kriterium ist der Inhaltliche Zusammenhang der einzelnen Segmente.
Die einzelnen Segmente sollten eine inhaltliche Verbindung aufweisen, sodass der/die Hörer*in nicht zwischen verschiedenen Themn hin und hergeworfen wird.
Zum Beispiel soll ein Podcast über \qt{Geschichte Amsterdam} nicht in einem Segment über die Gründung der Niederlande sprechen und im nächsten Segment davon handeln, wie sich Amsterdam in der Nazi Zeit verhalten hat.
Gleichzeitig sollen die Segmente auch nicht alle denselben Inhalt aufweisen und zum Beispiel in drei verschiedenen Segmenten wiederholen, dass die Niederlande im goldenen Zeitalter des 17. Jahrhunderts ein florierendes Handelsnetzwerk aufgebaut hatten.
 


\section{Prompt Engineering}

Reasoning
Json
GPT-4
beispielprompt


 


\section{Sentence Transformer Embeddings}

Das Sentence Transformer Projekt baut auf der Architektur von BERT auf. 
Es wird auch SBERT für Sentence BERT genannt. 
Uni Darmstadt
Haben verschiedene Modelle.

Unter dem 


\section{OpenAI Embeddings}

\href{https://twitter.com/Nils_Reimers/status/1487014195568775173}{twitter}

OpenAI stellt außer den Text (GPT-4) und Bild (DALLE) generierungs Modellen auch Modelle zur Embedding erstellung in Ihrer API vor.
Ada (1024 dimensions)
Babbage (2048 dimensions)
Curie (4096 dimensions)
Davinci (12288 dimensions)

\section{voyage}

über API
1024 Dimensionen


\section{Auswertung}

\myfigure{evaluation_all.png}{Vergleich der verschiedenen Embeddingmodelle (Themen 1-3 Wörter)}{evaluation_all}

\myfigure{evaluation_queries.png}{Vergleich der verschiedenen Embeddingmodelle (Fragen)}{evaluation_all}

könnte durch Augmentierung der Userfrage durch ChatGPT verbessert werden.\cite{shum2024}\cite{dai2023}