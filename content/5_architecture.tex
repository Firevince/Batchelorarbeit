\chapter{Architektur}\label{ch:method}

\section{Mikroservicearchitektur}

Der zweite Teil des Systems besteht darin, aus der Anfrage der Nutzenden eine passende Podcast-Episode zu erstellen.
Die dazu notwendigen Schritte lassen sich zunächst in mehrere kleine Projekte unterteilen.
Bei einem so großen Projekt lohnt es sich, eine Mikroservice-Architektur~\cite{fowler} anzustreben, bei der jeder Teil für sich gesehen eine Aufgabe erfüllt und die einzelnen Systeme nur über festgelegte Programmierschnittstellen (APIs) miteinander kommunizieren.
Diese Methode wird auch Divide-and-Conquer genannt.

\section{Anforderungen}

Zur Strukturierung des Projektes bietet es sich an, vorab Anforderungen an das System zu stellen.
Da das gesamte Projekt als eine Reihe von Mikroservices umgesetzt werden soll, werden folgend für jeden Mikroservice einzelne Anforderungen gestellt.

Zunächst muss die Datengrundlage geschaffen werden.
Dafür müssen die Podcast-Episoden heruntergeladen, transkribiert und anschließend alle Sätze in der Datenbank abgespeichert werden.
Dieses System muss in der Lage sein, anhand des Titels eines Podcasts bzw. der ID in der Audiothek sämtliche noch nicht in der Datenbank hinterlegten Episoden herunterzuladen, zu transkribieren und abzuspeichern.
Dieser Teil wurde in \autoref{ch:data} erläutert.

Der nächste Mikroservice muss in der Lage sein, diese Transkriptdaten in eine Form umzuwandeln, in der eine Suchfunktion relevante Abschnitte aus diesen Daten extrahieren kann.
Dafür soll es die einzelnen Wörter in sinnvolle Abschnitte gruppieren und ein Embedding für jeden Abschnitt berechnen.
Die Embeddings müssen dann jeweils in der Vektordatenbank abgespeichert werden.

Für den produktiven Einsatz könnte es vorteilhaft sein, diese beiden Services dynamisch zu gestalten, sodass automatisch neue Podcast-Episoden transkribiert und in der Datenbank gespeichert werden.
Für jedes dieser Transkripte könnten dann automatisch Embeddings berechnet werden.
In der Zukunft könnte man darüber nachdenken, diesen Teil komplett auf der Serverseite der ARD Audiothek zu hosten, um diese Daten auch für andere Anwendungen verfügbar zu machen.

Ein weiterer Service soll dann die Suchfunktion übernehmen, indem er Stichworte oder Sätze als Parameter erhält und zu diesen eine bestimmte Anzahl an relevanten Abschnitten zurückgibt.
Dafür muss ein Embedding für die Suchbegriffe errechnet werden und dieses Embedding mit allen Embeddings in der Datenbank verglichen werden.

Diese relevanten Abschnitte sollen dann noch weiter von LLMs analysiert werden.
Ein weiterer Service erhält die sortierte Liste und nutzt LLMs wie ChatGPT, um zum Beispiel eine Auswahl aus vielen Dokumenten zu treffen, die Reihenfolge der Abschnitte zu bestimmen oder ähnliche Themen herauszufinden.

Der nächste Service muss in der Lage sein, eine Liste an Segmenten entgegenzunehmen und daraus eine Audiodatei zu erzeugen.
Dafür muss er die Audiosegmente an den richtigen Stellen aus den originalen Audiodateien schneiden und die Audiosegmente zusammenfügen können.

Als Letztes muss die Audiodatei ausgeliefert werden.
Dafür soll zum einen eine API implementiert werden, die eine Anfrage verarbeiten kann und einen Link zu einer Audiodatei zurückgibt.
Außerdem soll ein Userinterface in Form einer Webseite aufgebaut werden, welches die Anfrage in einem Formular entgegennimmt und dann die Audioinhalte ausliefert.
Die Webseite soll im Browser verfügbar sein und mehrere User-Anfragen verarbeiten können.
\section{Programmiersprache Python}

In dieser Arbeit wird die Programmiersprache Python verwendet.
Python zählt zu den am meisten verwendeten Programmiersprachen weltweit und ist laut dem TIOBE-Index im Jahr 2023 sogar die meistverwendete Programmiersprache überhaupt~\cite{index2023}.
In dieser Arbeit wird Python verwendet, da es Unterstützung für sehr gute Bibliotheken für Machine Learning und NLP-Anwendungen gibt.
Vor allem die Unterstützung für Transformer-Modelle mit der Bibliothek transformers, die NLP-Bibliothek spaCy sowie die Datenverwaltungsbibliotheke pandas sind sehr praktisch.
Außerdem bietet die Bibliothek SQLite Unterstützung zur einfachen Anbindung von SQLite-Datenbanken.
Dazu ist Python sehr einfach zu verstehen und rechenaufwändige Operationen, wie die Matrixmultiplikationen in der transformers-Bibliothek, sind sehr performant in der Sprache C implementiert.

\section{Transkript-Segment Ranking}

\subsection{Ranking mit Embeddings}

Nachdem bei dem System eine Anfrage der nutzenden Person eingegangen ist, wird diese zunächst mit dem Embedding-Modell in einen Embeddingvektor umgewandelt.
Die Wahl des richtigen Embedding-Modells wird in \autoref{ch:experiments} beschrieben.
Das Ranking der Segmente übernimmt die Vektordatenbank Chroma.
Für die Distanzfunktion wird standardmäßig die quadratische L2-Norm verwendet.
Nachdem das Ranking erfolgte, werden die besten 10 Ergebnisse zurückgegeben.
Dafür wird bei Chroma der Parameter \verb|n_results| gesetzt.
Die einzelnen Einträge in Chroma besitzen außerdem einen Parameter \verb|segment_id|, mit dem man die Informationen zu diesem Segment aus der SQLite-Datenbank abrufen kann.

\myfigure{retrieval_schema.png}{Retrieval der relevanten Segmente~\cite{kumar2023}}{retrieval_schema}

\subsection{Anreicherung der Segmente}

Das Ranking der einzelnen Sätze liefert eine Auswahl der besten Kandidaten für den Podcast.
Einzelne Sätze, die aus verschiedenen Podcast-Episoden stammen, ohne Kontext hintereinander abzuspielen, bietet für den Zuhörenden nur ein geringes Hörerlebnis.
Es fehlt der Kontext zu den einzelnen Informationen.
Zum Beispiel liefert die Suchanfrage „Geschichte Amsterdam“ mit dem TF-IDF-Ansatz Sätze wie „Amsterdam, das bedeutet unbeschwerte Kinderjahre“ oder „Amsterdam gefiel ihm“.
Die Sätze an sich bieten kaum interessante Informationen.
Den Zuhörenden stellen sich sogar noch mehr Fragen, zum Beispiel, für wen Amsterdam unbeschwerte Kinderjahre bedeutete oder wem Amsterdam gefiel.

Um den Zuhörenden mehr Informationen zu ermöglichen, kann die Größe der einzelnen Segmente angepasst werden.
Dazu werden mehrere Ansätze betrachtet.

Der einfachste Ansatz ist, für jeden einzelnen Satz die umgebenden Sätze davor und dahinter miteinzubeziehen.
Die Anzahl der umgebenden Sätze ist dann ein Hyperparameter dieses Systems und kann vom Nutzer durch den Parameter Segmentlänge eingestellt werden, welcher die Anzahl der Sätze in einem Segment angibt.
Falls der auszuschneidende Satz am Anfang oder am Ende des Transkripts vorkommt, und die Segmentlänge so eingestellt ist, dass mehr Sätze als möglich dem Segment hinzugefügt werden sollten, so wird das Segment an der Transkriptgrenze abgeschnitten.
Die Evaluation der besten Segmentlänge wurde in dieser Arbeit nicht ausgeführt, als Defaultwert ist für die Segmentlänge 5 eingestellt.

Ein weiterer Ansatz wäre auch hier ein mächtiges LLM, wie ChatGPT, zu benutzen, um die richtige Segmentlänge für jedes Segment individuell einzustellen.
Dazu wird das LLM instruiert, aus dem gesamten Transkript einer Podcast-Episode und der dazugehörigen Anfrage die Anzahl der Kontextsätze auf das Wesentliche zu beschränken.
Ein Beispiel-Prompt ist zum Beispiel hier (\ref{ch:chatgpt-boundaries}) nachzulesen.
Dieser Ansatz führt in der Regel zu längeren Abschnitten, was es schwerer macht, die Zeitvorgaben zu erfüllen.

\subsection{Reranking mit ChatGPT}

Nachdem nun die einzelnen Sätze zu größeren Segmenten erweitert wurden, kann auch die Reihenfolge der einzelnen Segmente angepasst werden.
Wenn eine Person sich beispielsweise über die „Geschichte von Amsterdam“ informieren will, kommen dazu verschiedene Ausschnitte aus der Zeit der Gründung der Niederlande im Jahr 1581, den Besuchen Peter des Großen in Amsterdam im Jahr 1697 oder der Verfolgung der Juden im Zweiten Weltkrieg.
Die Reihenfolge dieser Ereignisse wird im Retrieval-Schritt dadurch bestimmt, wie stark die Ähnlichkeit zwischen diesen Segmenten und der Frage ist.
In diesem Fall wäre es sehr nützlich, die Reihenfolge so umzuändern, dass die Segmente zeitlich sortiert wären.
In anderen Fällen könnte es sinnvoll sein, dass Segmente hintereinander erscheinen, die einen inhaltlichen Zusammenhang besitzen.
Zum Beispiel würde zum Thema „Mauerfall Berlin“ mehrere Segmente vorkommen, die sich auch mit der DDR befassen.
In diesem Fall wäre es für Zuhörende spannend, diese Segmente hintereinander zu platzieren, um den Podcast flüssiger und zusammenhängender zu gestalten.

Ein solches Reranking der einzelnen Segmente kann mithilfe von leistungsstarken LLMs wie ChatGPT erreicht werden.
Die einzelnen Segmente werden nummeriert an die Inferenz-API von ChatGPT geschickt und das LLM soll die neue Reihenfolge als Liste zurückgeben.
Dafür wird der JSON-Modus von ChatGPT genutzt, welcher das LLM dazu bringt, die Antwort als JSON-String zurückzugeben.
Ein Beispiel-Prompt kann im Anhang (\ref{ch:chatgpt-reranking}) nachgelesen werden.

Da dies eine relativ einfache Aufgabe ist, in der nur eine Liste mit wenigen Zahlen zurückgegeben werden muss, wird in diesem Schritt auf Techniken des Prompt-Engineerings verzichtet.

\section{Audio-Zusammensetzung}

Für die Bearbeitung von Audiodateien in Python bietet sich das Python-Modul Pydub~\cite{zotero-567} an.
Mit diesem Modul kann ein Audiofile ähnlich wie ein Array behandelt werden.
Wenn sich zum Beispiel in einem Audiofile ein wichtiges Segment von Sekunde 34 bis Sekunde 64 erstreckt, kann dort einfach die Start- und Endzeit in Array-Schreibweise angegeben werden.
Im Hintergrund verwendet dieses Modul Software-Bibliotheken des Softwareprojekts FFmpeg~\cite{ffmpeg}, welche umfangreichen Support für die Bearbeitung fast aller Audioformate besitzt.
Für die Zeitstempel der Start- und Endzeit jedes Audiosegments nehmen wir die Daten aus den vorher aus der SQLite extrahierten relevanten Segmenten.
Diese werden dann als extra Audiofiles als WAV-Dateien in einem separaten Ordner abgespeichert, um einem Qualitätsverlust durch die Codierung des verlustbehafteten Codierungsformats MP3 vorzubeugen.

Um die Audios nun wieder zusammenzusetzen, verwenden wir das gleiche Modul Pydub.
Es bieten sich mehrere Möglichkeiten an, die Audiosegmente zusammenzusetzen.
Der primitivste Ansatz wäre, die Segmente einfach ohne Pause hintereinander abzuspielen.
Die einzelnen Segmente beginnen und enden meist abrupt und das Hintereinanderschalten mehrerer Segmente ohne Pause führt zu Verwirrungen, wo ein Segment endet und wo das nächste anfängt.
Dafür bietet sich ein kurzer Signalton zwischen den einzelnen Audiosegmenten an.
Dieser sollte nicht nervig sein, da er dem Zuhörenden öfter vorgespielt wird.
In dieser Arbeit wurde ein kleiner Jingle, der zum Anfang einer Episode abgespielt wird, als Signalton verwendet.

Eine weitere Möglichkeit wäre, zwischen jedem Segment der zuhörenden Person eine kurze Vorstellung der Episode und der Sprecherin bzw. des Sprechers zu ermöglichen oder sogar Kontext zu dieser zu geben.
Dazu könnte eine kurze Einleitung zu jedem Segment mit einer synthetisch generierten Stimme erstellt werden.
Dieser Ansatz wurde hier aber nicht weiter verfolgt.

\section{Weiterführende Themenvorschläge}

Um den Zuhörenden die Möglichkeit zu geben, auf verschiedene Themen, die im Podcast erwähnt wurden, weiter einzugehen, wurden entsprechende Themenvorschläge mithilfe von ChatGPT generiert.
Wenn zum Beispiel in einem Podcast zur „Geschichte von Amsterdam“ einzelne Segmente über die Anfangszeit der Niederlande handeln, könnte man sich in einer weiterführenden Episode zum Beispiel über die „Gründung der Niederlande“ informieren.
Zu diesem Zweck werden für jede generierte Podcast-Episode fünf weiterführende Themenvorschläge von ChatGPT generiert, die zu den Segmenten passen.
Dazu wird wieder ChatGPT-3.5 im JSON-Modus verwendet.
Ein Beispielprompt ist hier (\ref{ch:chatgpt-reranking}) zu lesen.
Die Themen werden explizit auf ein bis drei Wörter beschränkt, da sonst manchmal ganze Sätze als Themenvorschläge geführt werden, wie zum Beispiel „Die Erfahrungen von Juden in Amsterdam während der Nazibesetzung“.
Die Antwort von ChatGPT besteht aus einem JSON-Array, welches die weiterführenden Themen enthält.
Die einzelnen Themen können je nach Auslieferungsart des Systems zum Beispiel als Buttons angezeigt werden, die eine erneute Suche auslösen, sobald eine nutzende Person ihn drückt.

\section{Auslieferung}

Das System wird in diesem Prototyp als Webservice zur Nutzung angeboten.
Für das Deployment dieses Tools wurde das Webframework Flask genutzt.
Flask ist ein Web Service Gateway Interface-Server (WSGI), welcher das Bereitstellen von Webseiten über die WSGI-Schnittstelle ermöglicht, die dann von einem HTTP-Server ausgeliefert werden können.
In dieser Arbeit wird dafür der WSGI-HTTP-Server gunicorn~\cite{zotero-569} verwendet.
Gunicorn ist für Unix-Systeme geeignet, einfach zu konfigurieren und bietet Support für mehrere Worker, die mehrere Anfragen gleichzeitig bearbeiten können.

Für die Auslieferung wurde eine API und eine Webseite entwickelt.

\subsection{API}

Über die API kann der Podcast-Generator einfach von anderen Anwendungen aufgerufen werden.
Die API wird aufgerufen über eine GET-Request auf die Unterseite /api.

Als Parameter akzeptiert die API einen String „text“, welcher die Frage des Nutzers angibt.
Außerdem kann über den Parameter „time“ ein Integerwert angegeben werden, welcher die Zeit des Podcasts in Minuten angibt.
Der optionale Parameter „segment-length“ kann als Integer übergeben werden und bestimmt die Länge der einzelnen Segmente.

Die API gibt bei erfolgreicher Verarbeitung den Statuscode 200 zurück und liefert als Inhalt einen JSON-String, der eine einzige Variable „url“ enthält.
Diese URL verweist auf das generierte MP3-File, das ab diesem Zeitpunkt unter der Adresse /static/audios verfügbar ist.

Als Demonstration wurde außerdem ein Telegram-Bot entwickelt, der die API benutzt, um damit die automatische Podcast-Episoden zu erstellen.
Der Telegram-Bot ist öffentlich verfügbar und kann über die Adresse \url{https://t.me/PodcastGenerator} gefunden werden.

\subsection{Design der Webseite}

Die Webseite des Podcast-Generators bietet einen einfachen Weg, das System zu benutzen.
Es gibt die beiden Inputfelder Thema und Zeit, und einen Startbutton, bei dem die Generierung der Podcasts startet.
Nachdem die Podcast-Episode fertig generiert wurde, wird der vorher ausgegraute Audioplayer aktiv.
Dann können Nutzende die Audiodatei mithilfe des Players anhören.
Außerdem befindet sich neben dem Audioplayer eine Box mit den Transkriptabschnitten des Podcasts, sortiert nach der Erscheinung in der generierten Episode.
Diese Transkriptabschnitte können zur Navigation innerhalb der Podcast-Episode verwendet werden.

Das Design der Webseite beruht auf einem Template von \cite{erdem2024}.
Dieses Template wurde erweitert durch einen Suchbereich und einen Transkriptbereich.
Insgesamt soll es den nutzenden Personen eine intuitive Anwendung des Tools erlauben und die Möglichkeit liefern, die Parameter Thema, Zeit und Segmentlänge der Episode festzulegen.

\myfigure{screenshot_website.png}{Screenshot der Webseite}{screenshot_webbsite}