\chapter{Ausgangslage und theoretische Grundlagen}\label{ch:theoretical}


\section{Information Retrieval}

Das Ziel dieser Arbeit besteht darin, eine Schnittstelle für einen Hörenden zu erstellen, die ihm ermöglicht, einen Zusammenschnitt aus vielen verschiedenen Podcast Episoden zu erstellen, der genau zu einem Thema passt. 
Die wichtigste Aufgabe besteht also darin, die wesentlichen Abschnitte aus den Episoden herauszufinden. 
Dazu muss das System verstehen, welche Abschnitte zu verschiedenen Anfragen passen. 

Das Thema Information Retrieval (Informationsrückgewinnung) bezeichnet den Vorgang, aus einer großen Menge unsortierter Daten bestimmte Informationen wieder zu extrahieren.
Im Gegensatz zu Data-Mining werden dabei keine neuen Daten erschaffen, sondern lediglich bereits existierendes Wissen wieder zur Verfügung gestellt wird.
Ein bekanntes Beispiel eines Information Retrieval Algorithmus ist der PageRank Algorithmus für die Suchmaschine Google.
Suchmaschinen sind das Musterbeispiel eines Information Retrieval Systems.
Ein User gibt Stichworte ein, über die er mehr Informationen erhalten möchte und erhält daraufhin Links zu Webseiten, die diese Informationen wahrscheinlich enthalten.

In diesem System wird die Informationsrückgewinnung auf den Daten von Podcasttranskripten betrieben.
Dazu werden die Techniken der Text Embeddings benutzt, um relevante Informationen zu einem bestimmten Thema aus einer groen Anzahl an Dokumenten zurückzugewinnen.
SPLADE

\section{Embeddings}

Text Embeddings sind ein Weg die menschliche Sprache für Computer verständlich zu machen. 
Die menschliche Sprache ist ein hochkomplexes Konstrukt mit einer Grammatik, die sehr viel flexibler, kreativer, vieldeutiger und komplexer ist als Maschinensprache. 
Es gibt viele kleine Bedeutungsnuancen und zudem ist die Sprache stark von dem allgemeinen Wissen der Welt geprägt und sie ändert sich im Laufe der Zeit. 
Das alles macht es für Computer sehr schwierig die menschliche Sprache zu verstehen. 
Neuere Forschung im Bereich des Natural Language Processing bietet einige Ansätze, um dieses Problem zu lösen. 
Es gibt dazu viele verschiedene Methoden, die alle darauf abzielen, Worte oder Texte in Vektoren umzuwandeln, die den Inhalt dieser Worte oder dieser Texte repräsentieren.
Diese Vektoren nennt man Embeddings.
Das Ziel dieser Embeddings besteht für unseren Zweck darin, eine Suchfunktion zu beschreiben, die auf eine Anfrage (Query) hin, passende Dokumente aus einem großen Korpus an Dokumenten liefern kann.

Ein relativ moderner Ansatz besteht in der Suche mithilfe von Dichten Embedding Vektoren. 
Dabei wird versucht, die Semantik (also die Bedeutung) der einzelnen Segmente mathematisch mithilfe eines Vektors zu repräsentieren. 
Dieser Vektor soll dann zur Suche nach Ähnlichkeit dienen. 
Ein Vektor, der aus dem Satz „Ich sitze auf einer Bank im Grünen“ besteht und ein Vektor mit dem Satz „Im Park steht eine alte Bank“ sollen dabei sehr ähnlich zueinander sein, der Embeddingvektor des Satzes „Ich gehe in die Bank und hebe Geld ab“ aber sollte dazu wenig Ähnlichkeit aufweisen. 
Hierbei soll erkannt werden, dass sich die ersten beiden Sätze auf eine Bank zum Sitzen in der Natur beziehen und der dritte Satz das Geldinstitut meint. 
Ein solch filigranes Verständnis der Bedeutung ist nicht einfach zu erreichen. 
Ein stochastischer Algorithmus würde die Unterschiede bei solchen Homonymen in den meißten Fällen nicht erkennen können. 
Stochastische Algorithmen wie z.B. TF-IDF haben auch Probleme mit Negierungen, wie zum Beispiel in „Ich habe nichts gegessen.“ und „Nichts habe ich heute gemacht außer gegessen“.

Für unsere Aufgabe ist es sinnvoll nicht nur inhaltliche Vergleiche zu erstellen, die überprüfen, ob zwei Sätze ungefähr die gleiche Bedeutung haben, wie "Die Sonne bringt mich zum schwitzen" oder "Ich schmelze in der Hitze".
Besser wäre eine asymmetrische Ähnlichkeit, wie sie bei Frage und Antwortpaaren vorkommt.
Zum Beispiel wäre auf die Frage: "Welche Farbe hat der Himmel bei Sonnenuntergang?" ein gute Antwort: "Der Himmel hat bei Sonnenuntergang oft orange und rosa Farbtöne."  und nicht der Satz "Wie sieht der Himmel am Abend aus?"
Diese asymmetrische Ähnlichkeit ist schwerer zu ermitteln, da man trivialerweise in der Antwort Informationen findet, die in der Frage nicht vorkommen.

Auf der .... ??? 

Vergleich Elastic-Search
Embeddings können grob in zwei verschiedene Kategorien einge

\section{Geschichte der Embeddingverfahren}

Die Geschichte der Embeddings startet mit stochastischen Verfahren, wie dem TF-IDF Algorithmus und dem BM25 Algorithmus. 
Term Frequency - inverse Dokument Frequency (TF-IDF) ist ein Algorithmus um Dokumente nach Keyworten zu durchsuchen.
Das Verfahren verwendet Worthäufigkeiten (Term-Frequency), um Dokumente, in denen ein Wort häufiger vorkommt zu bevorzugen.
Außerdem verwendet es die generelle Seltenheit von Wörter (Inverse Document Frequency) um Wörtern, die seltener vorkommen, mehr Gewicht bei der Suche einräumen.

Daraus bildet der TF-IDF Algorithmus einen dünnbesetzten Vektor, der die Anzahl und Seltenheit jedes im Dokument vorkommenden Wortes abbildet. 
Dünnbesetzt heißt in diesem Zusammenhang, dass der Vektor viele Einträge mit dem Wert 0 besitzt. 
Das folgt aus der Bedingung, dass diese Vektoren untereinander vergleichbar sein müssen und dadurch Einträge für jedes Wort besitzen, welches im gesamten Korpus an Dokumenten vorkommt.
Jedes einzelne Dokument hat dabei aber nur ein Bruchteil der Gesamtwörter und damit viele Nullwerte.

Best Matching 25 (BM25) ist eine Famile von Algorithmen die versuchen, den TF-IDF Algorithmus zu verbessern indem sie die Priorisierung langer Dokumente und zu oft vorkommende Worte ausgleichen.

Außerdem kann man diese Algorithmen noch anpassen, sodass sie auch n-gramme als Einträge der Vektoren miteinbeziehen. 
Das heißt, dass nicht nur einzelne Wörter berücksichtigt werden, sondern auch häufig zusammen vorkommende Wörter als einzelnes Token in dem Vektor abgebildet werden. 
Das bewirkt zwar, dass die Ähnlichkeit der Dokumente in der Regel besser ermittelt werden kann, vergrößert aber die Vektoren überproportional.

Semantische Analysen begannen erst ab ca. 1990 mit der Latenten Semantischen Analyse (LSA).
Diese verwendet Eigenvektoren um aus einer Term-Frequency Matrix versteckte (latente) Eigenschaften zu ermitteln, welche die Dokumente besser repräsentieren als die TF Matrix an sich. 
Es werden von diesen Eigenvektoren nur die wichtigsten ausgewählt, sodass jedes Dokument sehr gut als eine Linearkombination dieser Vektoren repräsentiert werden kann.
Dann werden nur noch die Koeffizienten dieser Vektoren gespeichert und man kann von einer Dimensionsreduktion profitieren.
Die resultierenden Koeffizienten-Vektoren sind nun nicht mehr spärlich besetzt sondern dicht und die resultierenden Eigenvektoren bilden häufig latente Themen der Dokumente ab.

Wirkliche semantische Word Embeddings kamen erst mit der Veröffentlichung von Word2Vec (2013) zu einer breiteren Nutzung. 
Word2Vec verwendet ein neuronales Netz, um mithilfe der benachbarten Wörter Kontextinformationen über das eigentliche Wort zu erhalten. 
Dafür gibt es die beiden Ansätze Continous Bag of Words und Skipgramm.
Bei dem Ansatz Continous Bag of Words werden dem Neuronalen Netz die umgebenden Wörter als Input gegeben, und das Model hat die Aufgabe daraus das Wort zu ermitteln. 
Dieses Verfahren wird in einem Sliding Window für jedes Wort aus einem Text wiederholt. 
Für einen bestimmten Korpus aus verschiedenen Dokumenten kann dieses Modell dadurch die Beziehungen verschiedener Worte zueinander und die Ähnlichkeit verschieddener Worte, die oft in gleichem Kontext vorkommen, erlernen. 
Das Verfahren des Skipgramms funktioniert ähnlich, allerdings wird dabei für jedes Wort versucht, die umliegenden Wörter zu ermitteln. 
Dieser Ansatz dauert länger im Training, hat aber den Vorteil, dass er auch seltener vorkommende Wörter gut repräsentieren kann.

Ein Jahr später wurde GloVe (Global Vectors for Word Representations) als Forschungsprojekt von der Stanford Universität vorgestellt.
GloVe verbindet Konzepte aus LSA und Word2Vec, indem es die auf einer globalen Matrix Faktorisierung beruht aber die Distanz von Wörtern zueinander mitberücksichtigt.
Dadurch schaffte GloVe eine weitere Verbesserung der Embeddings und wird heutzutage noch unter anderem in NLP Bibliotheken wie SpaCy verwendet.[englische Wikipedia referiert auf Buch aus 2018]

Im Jahre 2018 wurde der Ansatz ELMo (Embeddings from Language Model) für Word Embeddings vorgestellt.
Dieser benutzt Bidirectionale Long Short Term Memory Modelle (LSTM) um daraus Embeddings zu generieren.
LSTM Modelle basieren auf Recurenten Neuronalen Netzen (RNN) und gehören zu den Sequenz to Sequenz Modellen. 
Sie bestehen dabei aus einem Encoder und einem Decoder.
Der Encoder codiert eine Inputsequenz, zum Beispiel ein Dokument, zu einem Embeddingvektor. 
Der Decoder dekodiert diesen Embeddingvektor wieder zu einer Sequenz, z.B. einer Zusammenfassung des Dokuments. [Quelle]
Für ELMo ist dabei nur der Encoder Teil wichtig.
Dabei werden zwei Layer von Forward und Backward LSTMs eingesetzt, die für jedes Wort den Kontext vor und nach dem Wort berücksichtigen, um sematische Embeddings zu erstellen. [Quellen]

Ein Jahr später wurden die Erstellung von Word und Sentence Embeddings durch BERT revolutioniert.
BERT (Bidirectional Encoder Representations from Transformers) \cite{devlin2019} ist ein auf NLP Aufgaben spezialisierter Transformer.
Das Model wurde von Google 2019 entwickelt und war seinerzeit das Beste Model, indem es in elf verschiedenen Aufgaben im Bereich des NLP State-of-the-art Ergebnisse lieferte.
Es ist auf zwei Datensätzen, dem BooksCorpus mit 800 Millionen Wörtern und der englischen Wikipedia mit 2,5 Milliarden Wörtern trainiert worden. \cite{devlin2019}
Dadurch hat BERT von einer große Auswahl an verschiedenen Themen gelernt und kann für diese sinnvolle Semantische Embedding Vektoren erstellen.













Unter diesen Aufgaben befinden sich Next Sentence Prediction (NSP) und Masked Language Modeling, auf die es auch trainiert wurde.



Außerdem kann man das Modell mit wenig aufwand auf andere Aufgaben finetunen und damit sehr gute Ergebnisse im Bereich der Named Entity Recognition, der Sentiment Analyse oder  










Der Wunsch die menschliche Sprache für den Computer verständlich zu machen ist fast so alt, wie die Computer an sich. 
In den 1940er Jahren, nach Ende des 2. Weltkrieges 
Der Bereich des NLP riesig. 
Es gibt [QUELLE] paper dazu. 
Es hat viele verschiedene Unterthemen. 
Die Aufgaben im Bereich des NlP sind vielseitig . Einige Ansätze 






\section{Transformer Architektur}


Eine Transformerarchitektur ist eine der modersten und leistungsfähigsten Architekturen, um eine Vielzahl von  NLP Aufgaben zu lösen. 
Sie bildet dabei den Nachfolger bzw. Konkurenten zu den bis dato vorherschenden Rekurenten Neuronalen Netzen (RNN), Gated Recurenten Units (GRU) oder Long-Short Term Memory Systemen (LSTM). 
Ähnlich wie diese Architekturen, ist auch der Transformer ein Sequenze to Sequenz Model, das der Encoder teil nimmt als Eingabe ein Folge von Tokens (ein Satz, eine Audiodatei) und der Decoder Teil bildet daraus eine andere Sequenz (ein Satz, eine Audiodatei). 
In diesem Abschnitt gehen wir nur auf den Encoder ein, weil er die Embeddings erstellt.

Der Unterschied von Sequenze to Sequenz Modellen zu einem normalen Neuronalen Netz besteht darin, dass die Eingabe und Ausgabe Sequenzen mit variablen Längen sein können, was bei Neuronalen Netzen nicht funktioniert, da sie eine feste Anzahl an Input und Output Neuronen besitzen.

RNNs sind eine weiterentwicklung normaler Neuronaler Netze, indem Sie mehrere Neuronale Netze hintereinander schalten. 
Dadurch wird für jedes Inputtoken ein Embedding generiert. 
Die nächste Zelle des RNN nimmt dann das Embedding der vorhergehenden Zelle und das nächste Input token und generiert daraus wieder ein Embedding.

Ein Problem welches diese Architekturen besitzen ist Vanishing Gradient
Diese Architekturen versuchen immer im Encoder die gesamte Input Sequenz von rechts nach links (forward)oder von links nach rechts (backward) zu verarbeiten, um daraus ein einzelnes Embedding zu erstellen.
Informationen, die in der Mitte der Sequenz vorkommen werden wahrscheinlich von danach folgenden Informationen überschrieben, weil das Modell versuchen muss alle Informationen in die selbe Matrix zu speichern. 

Der Ansatz des LSTM versucht dieses Problem zu lösen, indem er Gated Langzeit Gedächtniszellen einführt, die zusätzlich zu den kurzzeit Gedächtniszellen agieren und nicht so leicht überschrieben werden.
Dadurch können sie wichtige Informationen länger speichern.

Der Unterschied von Transformern zu RNNs, GRUs und LSTMs ist der Attention Mechanismus.
RNNs, GRUs und LSTMs sind darauf angewiesen, den Input sequentiell Token für Token zu verarbeiten, da jede Zelle asl Input das Embedding der vorrausgehenden Zelle benötigt.
Das macht das Training eines Models sehr Zeitaufwendig.

Der Große Vorteil der Transformer ist, dass sie parallelisierbarer sind.
Alle Tokens einer Input Sequenz können synchron verarbeitet werden. 
Dabei wird für jedes Token zunächst durch ein vorher ermitteltes Word Embedding ersetzt.
Der Mechanismus des Positional Encodings stellt sicher, dass das Model die Reihenfolge der tokens mitberücksichtigen kann.
Dieser Mechnisus nimmt berechnet aufgrund der Position des Tokens in der Sequenz ein positional Embedding auf der Grundlage von Sinus und Cosinus funktionen und addiert es auf das vorherige Word Embedding.

Der positional encoded Input wird dann von mehreren self Attention Köpfen verarbeitet, der sogenannten Multihead self attention. 
Jeder dieser Köpfe spaltet den Input in eine Query, Key und Value Matrix auf, indem es das ursprüngliche Embedding jeweils mit einer vortrainierten Query, Key und Value Matrix multipliziert.
Die resultierenden Matrizen werden dann mithilfe folgender Formel zur Attention Matrix umgewandelt:

$Attention(Q,K,V)=softmax(\frac{QK^t}{\sqrt{d_k}})*V$

Der Parameter d stellt dabei die Dimension der Matrix k dar.


Um die Formel besser zu verstehen, können wir an einem Beispiel sehen, was die einzelnen Matrizen für Aufgaben besitzen.
Als Eingabe nehmen wir den Satz: "Alice fuhr gestern durch die Nürnberger Innenstadt".
Die Query Matrix beschreibt nun eine Frage an diesen Satz, zum Beispiel "Wer?".
Die Key Matrix hat nun die Antwort darauf, wo diese Informationen im Satz stehen könnten, also wahrscheinlich im ersten Wort. 
Die beiden Matrizen werden nun Multipliziert, um die Position der Antwort auf die Frage zu erhalten. 
Das Ergebnis wird durch Division mit der Dimension der Matrix skaliert und anschließender Softmax funkion normiert, um die Relevanz der Schlüssel anzupassen.
Anschließend wird die Angepasste Fragen-schlüssel Matrix mit der Value Matrix multipliziert, welche die Informationen der Sequenz beinhaltet. 
An der ersten Stelle steht Alice.

Diese daraus resultierende Matrix wird nocheinmal mit der ursprünglichen Matrix in einer sogenannten residual Connection addiert und normiert, bevor sie in ein einfaches Neuronales Netz gespeist werden.

\section{Large Language Model}

Large Language Model bezeichnet ein Language Model, das auf vielen Parametern trainiert wurde. 
Language Model bezeichnet den Bezug zu dem Verständnis Natürlicher Sprache.
Large Language Modelle werden vor allem für ChatBots, Text-Übersetzungen, und Sprach-Übersetzungen eingesetzt.
Bekannte LLMs sind zum Beispiel ChatGPT con OpenAI, welches auf ca. 175 Milliarden Parameter trainiert wurde. [https://www.ankursnewsletter.com/p/gpt-4-gpt-3-and-gpt-35-turbo-a-review]
Weitere bekannte LLMs sind Google Bart und Gemini, Megatron von NVIDIA oder Ayax von Apple (das erst noch kommt).
Zu den bekannten kommerziellen LLMs, die von großen Tech Firmen entwickelt werden, kommen immer mehr open-source Modelle in den Vordergrund, die zwar (noch) nicht so Leistungsstark sind, aber auf eigener Hardware laufen können.



\section{Huggingface}

Da es mittlerweile eine Vielzahl unterschiedlicher LLMs gibt, die alle verschiedene spezielle Aufgaben besitzen und in manchen Aspekten Vor- und Nachteile haben, ist es Sinnvoll, diese untereinander vergleichen zu können.
Eine solche Vergleichsmöglichkeit bietet die Plattform Huggingface, auf der über 400.000 Machine Learning Modelle, davon mehr als 200.000 Transformer gehostet werden.
In der Kategorie Question Answering, gibt es über 1300 Einträge und in der Kategorie "Sentence Similarity" ca. 3000.
In beiden Kategorien gibt es Embedding Modelle zu finden.
Die einzelnen Modelle können  kostenlos heruntergeladen werden, sie sind ausführlich dokumentiert und man kann sie untereinander mithilfe einer Punkteabgabe auf verschiedenen Kontroll-Datensätzen vergleichen.
Durch Leaderboards kann man die besten Modelle bestimmen.
Zum Beispiel das Massive Text Embedding Benchmark Leaderbord https://huggingface.co/spaces/mteb/leaderboard.\cite{muennighoff2023}

Auf diesem Leaderboard werden Embedding Modelle für unterschiedliche Kategorien evaluiert.
Semantic Textual Similarity 
Classification






\section{Dimensionalität}

\section{SpaCy und dictionaries}
