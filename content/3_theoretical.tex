\chapter{Ausgangslage und theoretische Grundlagen}\label{ch:theoretical}

\section{Information Retrieval}

Das Ziel dieser Arbeit besteht darin, eine Schnittstelle für einen Zuhörer zu bieten, die automatisch Podcast-Episoden erstellen kann. 
Die generierte Podcast-Episode besteht dabei aus einem Zusammenschnitt aus verschiedenen bereits vorhandenen Podcast-Episoden, die alle zu dem vom Zuhörer gewünschten Thema passen.
Die wichtigste Aufgabe besteht also darin, die wesentlichen Abschnitte aus den Episoden herauszufinden.
Dazu muss das System verstehen, welche Abschnitte zu verschiedenen Anfragen passen.

Das Thema Information Retrieval (Informationsrückgewinnung) bezeichnet den Vorgang, aus einer großen Menge unsortierter Daten bestimmte Informationen wieder zu extrahieren \cite{chowdhury2010}.
Im Gegensatz zum Data-Mining werden dabei keine neuen Daten erschaffen, sondern lediglich bereits existierendes Wissen wieder zur Verfügung gestellt.
Ein bekanntes Beispiel eines Information Retrieval Algorithmus ist der PageRank-Algorithmus für die Suchmaschine Google \cite{page}.
Suchmaschinen sind das Paradebeispiel eines Information Retrieval Systems.
Ein User gibt Stichworte ein, über die er mehr Informationen erhalten möchte und erhält daraufhin Links zu Webseiten, die diese Informationen wahrscheinlich enthalten.

In dieser Arbeit wird die Informationsrückgewinnung auf Basis der Daten von Podcasttranskripten betrieben.
Dazu werden Techniken der Text Embeddings benutzt, um relevante Informationen zu einem bestimmten Thema aus einer großen Anzahl an Dokumenten zurückzugewinnen.

\section{Embeddings}

\subsection{Embeddings als Computersprache}

Text Embeddings sind ein Weg, die menschliche Sprache für Computer verständlich zu machen.
Die menschliche Sprache ist ein hochkomplexes Konstrukt mit einer Grammatik, die sehr viel flexibler, kreativer, vieldeutiger und komplexer ist als die Maschinensprache.
Es gibt viele kleine Bedeutungsnuancen, die Sprache ist stark von dem allgemeinen Wissensstand der Welt geprägt und sie ändert sich im Laufe der Zeit.
Das alles macht es für Computer sehr schwierig, die menschliche Sprache zu verstehen.
Neuere Forschung im Bereich des Natural Language Processing bietet einige Ansätze, um dieses Problem zu lösen.
Es gibt dazu viele verschiedene Methoden, die alle darauf abzielen, Worte oder Texte in Vektoren umzuwandeln, die den Inhalt dieser Worte oder dieser Texte repräsentieren~\cite{mikolov2013}~\cite{peters2018}.
Diese Vektoren nennt man Embeddings.
Das Ziel dieser Embeddings besteht darin, eine Suchfunktion zu beschreiben, die auf eine Anfrage (Query) hin passende Dokumente aus einem großen Korpus an Dokumenten liefern kann.

\subsection{Entwicklung der Embeddingverfahren}

Die ersten Algorithmen mit dem Ziel, eine große Anzahl von Dokumenten durchsuchen zu können, waren Suchalgorithmen wie TF-IDF~\cite{sparckjones1972} oder BM25~\cite{harman1995}, die in den 1970er Jahren aufkamen.
Diese versuchten, Worthäufigkeiten als Anhaltspunkt zu nehmen, um Ähnlichkeiten zwischen Dokumenten zu erkennen und Suchfunktionen aufzubauen.

Semantische Analysen begannen erst ab ca. 1990 mit der Latenten Semantischen Analyse (LSA)~\cite{landauer1997}.
Diese verwendet Eigenvektoren, um aus einer Term-Frequency Matrix versteckte (latente) Eigenschaften zu ermitteln, welche die Dokumente besser repräsentieren als die TF-Matrix an sich.
Es werden von diesen Eigenvektoren nur die wichtigsten ausgewählt, sodass jedes Dokument sehr gut als eine Linearkombination dieser Vektoren repräsentiert werden kann.
So werden nur noch die Koeffizienten dieser Vektoren gespeichert und man kann von einer Dimensionsreduktion profitieren.
Die resultierenden Koeffizienten-Vektoren sind nun nicht mehr spärlich besetzt, sondern dicht, und die resultierenden Eigenvektoren bilden häufig latente Themen der Dokumente ab.

Semantische Word Embeddings erlangten erst mit der Veröffentlichung von Word2Vec~\cite{mikolov2013} im Jahr 2013 eine breitere Nutzung.
Word2Vec verwendet ein neuronales Netz, um mithilfe der benachbarten Wörter Kontextinformationen über das eigentliche Wort zu erhalten.
Dafür gibt es die beiden Ansätze Continuous Bag of Words und Skip-Gram.
Beim Continuous Bag of Words-Ansatz erhält das neuronale Netz die umgebenden Wörter als Input und das Modell hat die Aufgabe, daraus das Zielwort zu ermitteln.
Dieses Verfahren wird in einem Sliding Window für jedes Wort aus einem Text wiederholt.
Für einen bestimmten Korpus aus verschiedenen Dokumenten kann dieses Modell dadurch die Beziehungen verschiedener Worte zueinander und die Ähnlichkeit verschiedener Worte, die oft im gleichen Kontext vorkommen, erlernen.
Das Verfahren des Skip-Gram funktioniert ähnlich, allerdings wird dabei für jedes Wort versucht, die umliegenden Worte zu ermitteln.
Dieser Ansatz dauert länger im Training, hat aber den Vorteil, dass er auch seltener vorkommende Worte gut repräsentieren kann.~\cite{mikolov2013}

Ein Jahr später wurde GloVe (Global Vectors for Word Representation)~\cite{pennington2014} als Forschungsprojekt von der Stanford Universität vorgestellt.
GloVe verbindet Konzepte aus LSA und Word2Vec, indem es auf der Faktorisierung einer globalen Matrix beruht, aber die Distanz von Worten zueinander mitberücksichtigt.
Dadurch schaffte GloVe eine weitere Verbesserung der Embeddings und wird heutzutage noch unter anderem in NLP-Bibliotheken wie spaCy verwendet~\cite{honnibal2017}.

Im Jahre 2018 wurde der Ansatz ELMo (Embeddings from Language Models)~\cite{peters2018} für Word Embeddings vorgestellt.
Dieser benutzt bidirektionale Long Short-Term Memory Modelle (LSTM)~\cite{hochreiter1997}, um daraus Embeddings zu generieren.
LSTM-Modelle basieren auf rekurrenten neuronalen Netzen (RNN)~\cite{dafontouracosta1996} und gehören zu den Sequenz-zu-Sequenz-Modellen.
Sie bestehen dabei aus einem Encoder und einem Decoder.
Der Encoder codiert eine Inputsequenz, zum Beispiel ein Dokument, zu einem Embeddingvektor.
Der Decoder dekodiert diesen Embeddingvektor wieder in eine Sequenz, z.B. eine Zusammenfassung des Dokuments.
Für ELMo ist dabei nur der Encoder-Teil wichtig.
Dabei werden zwei Layer von Forward- und Backward-LSTMs eingesetzt, die für jedes Wort den Kontext vor und nach dem Wort berücksichtigen, um semantische Embeddings zu erstellen.

Ein halbes Jahr später wurden die Erstellung von Word- und Sentence-Embeddings durch BERT (Bidirectional Encoder Representations from Transformers)~\cite{devlin2019}revolutioniert.
BERT ist ein auf NLP-Aufgaben spezialisierter Transformer.
Das Modell wurde von Google im Jahr 2019 entwickelt und war seinerzeit das beste Modell, da es in elf verschiedenen Aufgaben im Bereich des NLP State-of-the-Art-Ergebnisse lieferte~\cite{devlin2019}.
Es ist auf zwei Datenbeständen, dem BooksCorpus mit 800 Millionen Worten und der englischen Wikipedia mit 2,5 Milliarden Worten trainiert worden.
Die Aufgaben im Trainingsprozess waren die Next Sentence Prediction (NSP), bei der das Modell entscheiden muss, ob zwei Sätze wahrscheinlich zusammen vorkommen, und das Masked Language Modeling, bei dem das Modell unkenntlich gemachte Wörter in Sätzen wiederherstellen soll.
Dadurch hat BERT sowohl auf Wortebene als auch auf Satzebene wichtige Merkmale der Sprache gelernt.
Durch die große Auswahl an verschiedenen Themen im Trainingsprozess kann das Modell für viele verschiedene Sachverhalte sinnvolle semantische Embeddingvektoren erstellen.~\cite{devlin2019}

Seit der Entdeckung der Transformerarchitektur und der Entstehung von BERT entwickelt sich der Bereich der Embedding-Modelle rasant.
Täglich werden neue Modelle publiziert und in Abständen von wenigen Wochen erreichen neue Modelle State-of-the-Art-Performance auf populären Benchmarks wie die Massive Text Embedding Benchmark~\cite{muennighoff2023}.

\subsection{Tokenisierer}

Um Textsequenzen mithilfe von Embeddings zu verarbeiten, werden diese Texte zunächst in Tokens zerlegt, bevor sie von einem Embedding-Modell embedded werden.
Ein Token kann dabei je nach Modell ein einziges Wort, mehrere Worte oder nur ein Bruchteil eines Wortes sein.~\cite{he2006}
In Machine Learning-Anwendungen nutzt man dafür verschiedene Tokenisierer~\cite{zouhar2023}.
Es gibt einfache Tokenisierer, wie Whitespace-Tokenisierer, die einen Text einfach an Leerzeichen teilen und die einzelnen Wörter als Tokens behandeln.
Etwas besser sind Wort-Tokenisierer, die auch Satzzeichen erkennen und dadurch Punkte und Kommata nicht an das Ende des vorangegangenen Wortes anhängen.
Einen solchen Tokenisierer nutzt beispielsweise spaCy, um \qt{Linguistic Features} zu bestimmen~\cite{honnibal2017}.
Es gibt Subword-Tokenisierer, die Worte noch einmal in kleinere Abschnitte teilen.
Das Byte-Pair Encoding versucht aufgrund von statistischen Methoden, häufig vorkommende zusammenhängende Zeichenfolgen als Tokens zu identifizieren.
Diese Art von Tokenisierer wird am häufigsten für die Eingabe bei Transformern genutzt~\cite{zouhar2023}.
Die Transformermodelle von OpenAI benutzen beispielsweise tiktoken als Tokenisierer~\cite{tiktoken2024}, das bekannte BERT-Modell von Google benutzt den WordPiece-Tokenisierer~\cite{devlin2019}.

\subsection{Embeddingvektoren}

\subsubsection{Sparse Vektoren}

Embeddings können in zwei verschiedene Kategorien eingeteilt werden: dünne (sparse) Vektoren und dichte (dense) Vektoren.

Sparse Vektoren, wie der TF-IDF-Algorithmus~\cite{sparckjones1972} und der BM25-Algorithmus~\cite{harman1995}, versuchen syntaktische Eigenschaften von Dokumenten zu erfassen.
Term Frequency-Inverse Document Frequency (TF-IDF) ist ein Algorithmus, der es erlaubt, Dokumente nach Keywords zu durchsuchen.
Das Verfahren verwendet Worthäufigkeiten (Term Frequency), um Dokumente, in denen ein Wort häufiger vorkommt, zu bevorzugen.
Außerdem verwendet es die generelle Auftrittsrate von Wörtern (Inverse Document Frequency), um Wörtern, die seltener vorkommen, mehr Gewicht bei der Suche einzuräumen.

Daraus bildet der TF-IDF-Algorithmus einen dünnbesetzten Vektor, der die Anzahl und Seltenheit jedes im Dokument vorkommenden Wortes abbildet.
Dünnbesetzt heißt in diesem Zusammenhang, dass der Vektor viele Einträge mit dem Wert 0 besitzt.
Das folgt aus der Bedingung, dass diese Vektoren untereinander vergleichbar sein müssen und dadurch Einträge für jedes Wort besitzen, das im gesamten Korpus an Dokumenten vorkommt.
Jedes einzelne Dokument enthält dabei aber nur einen Bruchteil der Gesamtwörter und damit viele Nullwerte.~\cite{sparckjones1972} 

Best Matching 25 (BM25) ist eine Familie von Algorithmen, die versuchen, den TF-IDF-Algorithmus zu verbessern, indem sie die Priorisierung langer Dokumente und zu oft vorkommende Worte ausgleichen.~\cite{harman1995}

Außerdem kann man diese Algorithmen noch anpassen, sodass sie auch N-Gramme als Einträge der Vektoren miteinbeziehen.
Das heißt, dass nicht nur einzelne Wörter berücksichtigt werden, sondern auch häufig zusammen vorkommende Wörter (N-Gramme) als einzelne Tokens in dem Vektor abgebildet werden.
Das bewirkt zwar, dass die Ähnlichkeit der Dokumente in der Regel besser ermittelt werden kann, vergrößert aber die Vektoren.~\cite{dey2017}

\subsubsection{Dichte Vektoren}

Ein relativ moderner Ansatz besteht in der Suche mithilfe von dichten Embeddingvektoren.
Dabei wird versucht, die Semantik (also die Bedeutung) der einzelnen Segmente mathematisch mithilfe eines Vektors zu repräsentieren.
Dieser Vektor soll dann zur Suche nach Ähnlichkeit eingesetzt werden.
Ein Vektor, der aus dem Satz „Ich sitze auf einer Bank im Grünen“ besteht, und ein Vektor mit dem Satz „Im Park steht eine alte Bank“ sollen dabei sehr ähnlich zueinander sein.
Der Embeddingvektor des Satzes „Ich gehe in die Bank und hebe Geld ab“ sollte dazu wenig Ähnlichkeit aufweisen.
Hierbei soll erkannt werden, dass sich die ersten beiden Sätze auf eine Bank zum Sitzen in der Natur beziehen und der dritte Satz ein Geldinstitut betrifft.
Ein solch filigranes Verständnis der Bedeutung ist nicht einfach zu erreichen.
Ein stochastischer Algorithmus würde die Unterschiede bei solchen Homonymen in den meisten Fällen nicht erkennen können. 
Stochastische Algorithmen wie z.B. TF-IDF haben auch Probleme mit Negierungen, wie zum Beispiel in „Ich habe nichts gegessen.“ und „Nichts habe ich heute gemacht, außer gegessen“.

Für unsere Aufgabe ist es sinnvoll, nicht nur zu überprüfen, ob zwei Sätze ungefähr die gleiche Bedeutung haben, wie „Die Sonne bringt mich zum Schwitzen“ oder „Ich schmelze in der Hitze“.
Besser wäre eine asymmetrische Ähnlichkeit, wie sie bei Frage- und Antwortpaaren vorkommt.
Zum Beispiel wäre auf die Frage: „Welche Farbe hat der Himmel bei Sonnenuntergang?“ eine gute Antwort: „Der Himmel hat bei Sonnenuntergang oft orange und rosa Farbtöne.“ und nicht der Satz „Wie sieht der Himmel am Abend aus?“
Diese asymmetrische Ähnlichkeit ist schwerer zu ermitteln, da man normalerweise in der Antwort Informationen findet, die in der Frage nicht vorkommen.

\subsubsection{Dimensionalität von Embeddingvektoren}

Die Dimensionalität von verschiedenen Embeddings hat Einfluss auf deren Performance.
Kleinere Embeddings verbrauchen weniger Speicherplatz und lassen sich schneller untereinander vergleichen, was den Retrieval-Prozess beschleunigt.
Andererseits bietet eine kleinere Dimensionalität auch weniger Möglichkeiten für die LLMs, semantische Informationen darin zu speichern.
Das heißt aber nicht, dass größere Embeddingvektoren auch gleichzeitig die Semantik besser repräsentieren müssen.
Sparse Vektoren sind meist größer als Dense Vektoren, können aber nur lexikalische Eigenschaften repräsentieren.
Aber auch bei Dense Vektoren bedeuted eine Dimensionalität nicht gleich eine bessere Performance.
Dies hängt vor allem damit zusammen, wie leistungsstark die Architektur des Modells ist und welche sowie wie viele Trainingsdaten zum Trainieren verwendet wurden.
Das Modell voyage-lite-02-instruct belegt zum Beispiel auf dem Massive Text Embedding Benchmark Leaderboard mit einer Dimensionalität von 1024 den zweiten Platz~\cite{mteb}.
Auf den Plätzen drei bis fünf liegen jedoch Embedding-Modelle mit einer Dimensionalität von 4096.

\subsection{Ähnlichkeitsvergleiche}

Mithilfe der Embedding-Vektoren können Sätze gefunden werden, die eine semantische Ähnlichkeit zueinander aufweisen.
Zum Beispiel besitzen die Vektoren des all-MiniLM-L6-v2-Modells~\cite{minilm2024} 384 Dimensionen, haben also 384 Gleitkommazahlen gespeichert, die zwischen -1 und 1 liegen.
Diese Gleitkommazahlen-Vektoren könnte man auch als Feature-Vektoren begreifen.
Zum Beispiel könnte die erste Zahl dieses Vektors für die Erwähnung von Professoren in dem Satz stehen (-1 für keine Professoren; 1 für viele Professoren).
Die zweite Zahl könnte für das Thema Essen stehen (-1 für wenig mit Essen zu tun; 1 für sehr viel mit Essen zu tun).
Damit hätte der Satz „In der Mensa gibt es jeden Tag Currywurst mit Pommes“ an der ersten Stelle vielleicht eine 0,1, weil der Begriff „Mensa“ leicht mit Uni und Professoren konnotiert wird.
An der zweite Stelle würde der Wert zum Beispiel bei 0,94 liegen, da es in dem Satz offensichtlich um das Essen handelt.
In der Realität wird das Modell sehr wahrscheinlich komplexere Merkmale lernen, die für Menschen nicht so offensichtliche sind~\cite{chen2018}.
Ein Grund dafür ist, dass das Modell vor allem pro Eintrag eine Linearkombination von verschiedenen Merkmalen lernt, also jede Zahl eine Überlagerung vieler verschiedener Eigenschaften darstellt~\cite{chen2018}.

Die Vergleiche der Embeddings können mit verschiedenen Algorithmen berechnet werden.
Im Bereich des Machine Learnings werden vor allem die euklidische Distanz und die Kosinusdistanz angewendet.

\subsubsection{Euklidische Distanz}

Die euklidische Distanz zwischen zwei Vektoren kann über den Satz des Pythagoras berechnet werden. 
Dazu wird für jede einzelne Dimension die Differenz der beiden Vektoren in dieser Dimension gebildet. 
Diese Differenzen werden quadriert und anschließend wird die Summe dieser Quadrate gebildet. 
Zum Schluss wird die Quadratwurzel aus dieser Summe gezogen.
Für die beiden Vektoren
$\begin{pmatrix}0\\1\end{pmatrix}$
und
$\begin{pmatrix}-1\\0\end{pmatrix}$
wäre die Euklidische Distanz
$\frac{1}{\sqrt{2}}$, \vspace{5mm} \newline da $\sqrt{(0-(-1))^2 + (1-0)^2}=\frac{1}{\sqrt{2}}$

Dieses Distanzmaß ist sensibel gegenüber der Länge der Vektoren.
Wenn die Vektoren nicht normiert wurden, kann die euklidische Distanz sehr groß werden und die Ergebnisse einer Ähnlichkeitssuche verfälschen.
Da die Berechnung der Wurzel für den Vergleich von verschiedenen Distanzen nicht relevant ist, wird in Machine-Learning-Anwendungen häufig die quadratische L2-Norm verwendet.
Diese bildet auch das Produkt der Quadrate der einzelnen Differenzen, verzichtet aber auf die Wurzel.~\cite{kryszkiewicz2013}

$euclidean\_L2\_norm(a,b) = \sum_{i=1}^{n} (a_i-b_i)^2$


\subsubsection{Kosinusdistanz}

Ein weiteres Ähnlichkeitsmaß kann über den Winkel zwischen zwei Vektoren definiert werden.
Dieser Winkel ist unabhängig davon, wie lang die Vektoren sind, immer gleich.
Die weitverbreitetste Methode, um diesen Winkel zu messen, ist über die Kosinusähnlichkeit.
Diese berechnet den Kosinus des Winkels und kann einfach über die Formel

$\text{cosine\_similarity}(\mathbf{a}, \mathbf{b}) = \cos{\theta} = \frac{\mathbf{a} \cdot \mathbf{b}}{\|\mathbf{a}\| \|\mathbf{b}\|}$

berechnet werden.
Dabei wird für beide Vektoren zunächst das Skalarprodukt bestimmt.
Das Skalarprodukt besitzt die Eigenschaft, groß zu sein, wenn beide Vektoren in ähnliche Richtungen zeigen, und kleiner, wenn beide Vektoren in unterschiedliche Richtungen zeigen.
Dabei hat die Länge der unterschiedlichen Vektoren einen Einfluss auf die Größe des Skalarproduktes.
Um dieser Verzerrung entgegenzuwirken, wird das Skalarprodukt durch das Produkt der Beträge der Vektoren geteilt.
Dadurch erhält man den Kosinus des Winkels zwischen diesen beiden Vektoren.

Wenn beide Vektoren vorher normiert wurden, das heißt der Betrag der Vektoren gleich Eins ist, ist die Kosinusdistanz gleich der Skalarproduktdistanz der beiden Vektoren.
Außerdem kann in diesem Fall die euklidische Distanz durch folgende Formel aus der Kosinusdistanz errechnet werden.~\cite{kryszkiewicz2013}

$\text{euclidean\_distance} = \sqrt{2 - 2 \cos(\theta)}$

\section{Transformer-Architektur}

\subsection{Sequenz-zu-Sequenz-Modelle}

Eine Transformerarchitektur ist eine der modernsten und leistungsfähigsten Architekturen, um eine Vielzahl von NLP-Aufgaben zu lösen~\cite{patwardhan2023}.
Sie bildet dabei den Nachfolger bzw. Konkurrenten zu den bis dato vorherrschenden rekurrenten neuronalen Netzen (RNN), Gated Recurrent Units (GRU) oder Long-Short Term Memory Systems (LSTM)~\cite{hochreiter1997}.
Ähnlich zu diesen Architekturen ist auch der Transformer ein Sequenz-zu-Sequenz-Modell.
Der Encoderteil nimmt als Eingabe eine Sequenz von Tokens (einen Satz, eine Audiodatei) und der Decoderteil bildet daraus eine andere Sequenz von Tokens (einen Satz, eine Audiodatei).

Der Unterschied von Sequenz-zu-Sequenz-Modellen zu einem gewöhnlichen feedforward neuronalen Netz besteht darin, dass die Eingabe- und Ausgabesignale auch Sequenzen mit variabler Länge sein können.
Gewöhnliche künstliche neuronale Netze haben eine feste Anzahl an Eingangs- und Ausgangsneuronen, was es ihnen nur erlaubt, Aufgaben zu lösen, bei denen die Eingangs- und Ausgangsdaten eine bestimmte Länge aufweisen.
Anwendungsfälle für künstliche neuronale Netze sind zum Beispiel Frühwarnsysteme oder Klassifikationsaufgaben (z.B. Ziffererkennung)~\cite{duncan2013}~\cite{lee1991}.

\subsection{RNNs, LSTMs und GRUs}

Rekurrente neuronale Netze (RNNs)~\cite{dafontouracosta1996} stellen eine Weiterentwicklung von feedforward neuronalen Netzen dar, indem sie eine Rückkoppelung des Ausgabesignals erlauben und somit eine variable Länge an Eingabesignalen verarbeiten können.
Das Eingabesignal muss einer Sequenz entsprechen und wird dafür in Tokens zerlegt.
Jedes Token wird dabei einmal durch das RNN propagiert, bis ein Ausgabeembedding für dieses Token produziert wurde.
Das Ausgabeembedding aus der vorherigen Iteration wird dann zusammen mit dem nächsten Token in der Sequenz als Eingabe für die nächste Iteration verwendet.
Dadurch wird die gesamte Sequenz nacheinander in diesem Netz verarbeitet, wobei jedes Token den Kontext der vorherigen Tokens als Embedding mitgeliefert bekommt.
Am Ende liefert ein RNN dann ein einziges Embedding, welches die gesamten Informationen der Sequenz repräsentieren soll.

Ein Problem, welches diese Architekturen besitzen, ist das Problem des verschwindenden Gradienten~\cite{bengio1994}.
RNNs versuchen immer im Encoder, die gesamte Inputsequenz von links nach rechts (forward) oder von rechts nach links (backward) zu verarbeiten, um daraus ein einzelnes Embedding zu erstellen.
Informationen, die in der Mitte der Sequenz vorkommen, werden wahrscheinlich von danach folgenden Informationen überschrieben, weil das Modell versucht, alle Informationen in derselben Matrix zu speichern.

Die Ansätze des Long-Short Term Memory Systems (LSTM)~\cite{hochreiter1997} und der Gated Recurrent Unit (GRU)~\cite{cho2014} versuchen dieses Problem zu lösen, indem sie Gates einsetzen.
Diese Gates bestimmen, welche Informationen aus dem vorherigen Embedding durch das Embedding der nächsten Iteration überschrieben werden dürfen.
Dadurch können sie wichtige Informationen länger speichern.
Bei einem LSTM werden dafür drei verschiedene Gates benutzt: das Forget Gate, das Input Gate und das Output Gate.
Jedes dieser Gates hat wiederum eine bestimmte Anzahl an Parametern, die bestimmen, welche Informationen vergessen, welche für den Input der nächsten Iteration benutzt und welche bei der nächsten Iteration ausgegeben werden.
Alle diese Parameter müssen beim Training des LSTMs mittrainiert werden, was zu einem komplizierten Modell und zu einem langsamen Trainingsprozess führt.
Bei GRUs gibt es nur ein Update Gate.
Sie haben dadurch weniger Parameter, die trainiert werden müssen und vereinfachen damit den Trainingsprozess.~\cite{pirani2022}

\subsection{Transformer-Encoder}

Der größte Unterschied der Transformerarchitektur zu RNNs, GRUs und LSTMs ist der Attention-Mechanismus.
RNNs, GRUs und LSTMs sind darauf angewiesen, den Input sequentiell Token für Token zu verarbeiten, da jede Zelle als Input das Embedding der vorausgehenden Zelle benötigt.
Das macht das Training eines Modells sehr zeitaufwendig.

Der große Vorteil der Transformer ist, dass sie parallelisierbar sind.
Alle Tokens einer Inputsequenz können synchron verarbeitet werden.
Dabei wird jedes Token zunächst durch ein vorher ermitteltes Word Embedding ersetzt.
Der Mechanismus des Positional Encodings stellt sicher, dass das Modell die Reihenfolge der Token mitberücksichtigen kann.
Dieser Mechanismus berechnet aufgrund der Position des Tokens in der Sequenz ein Positional Embedding auf der Grundlage von Sinus- und Kosinusfunktionen und addiert es mit dem vorherigen Word Embedding.
Das resultierende Embedding enthält damit sowohl Informationen über das Token selbst durch das Token Embedding als auch über die Position in der Sequenz durch das Positional Encoding.

\subsection{Multihead Self-Attention}

Transformer setzen Multihead Self-Attention ein.
Der positional encodierte Input wird von mehreren Self-Attention-Köpfen verarbeitet, die sogenannte Multihead Self-Attention.
Jeder dieser Köpfe spaltet den Input in Query-, Key- und Value-Matrix auf, indem er die ursprüngliche Embeddingmatrix jeweils mit einer vortrainierten Query-, Key- und Valuematrix multipliziert.
Die resultierenden Matrizen werden dann mithilfe folgender Formel zur Attention-Matrix umgewandelt:

$Attention(Q,K,V)=softmax(\frac{QK^t}{\sqrt{d_k}})*V$

Der Parameter \textit{d} stellt dabei die Dimension der Matrix \textit{k} dar.

Um die Formel besser zu verstehen, wird an einem Beispiel demonstriert, was die Aufgaben der einzelnen Matrizen sind.
Als Eingabe wird der Satz: „Alice fuhr gestern durch die Nürnberger Innenstadt“ benutzt.
Die Query-Matrix beschreibt nun eine Frage an diesen Satz, zum Beispiel „Wer?“.
Wie genau diese Fragen aussehen, ist nicht leicht zu ermitteln und wird nur in seltenen Fällen so ausfallen, wie auch Menschen nach bestimmten Informationen fragen.
Wichtig ist jedoch, dass in jedem Attention-Kopf eine bestimmte Frage gestellt wird.
Die Key-Matrix hat nun die Antwort darauf, wo diese Informationen im Satz stehen könnten, also wahrscheinlich im ersten Wort.
Die beiden Matrizen werden multipliziert, um die Position der Antwort auf die Frage zu erhalten.
Das Ergebnis wird über Division mit der Dimension der Matrix skaliert und anschließend mit der Softmax-Funktion normiert, um die Relevanz der Schlüssel anzupassen.
Anschließend wird die angepasste Fragen-Schlüssel-Matrix mit der Value-Matrix multipliziert, welche die Informationen der Sequenz beinhaltet.
Die resultierende Matrix würde dann die Information beinhalten, dass die Antwort auf die Frage „Wer?“ für den Satz „Alice fuhr gestern durch die Nürnberger Innenstadt“ wahrscheinlich „Alice“ ist.~\cite{ng2023}

Die verschiedenen Köpfe der Multihead Self-Attention liefern nun verschiedene Antworten auf verschiedene Fragen und versuchen damit, die Informationen der Sequenz zu codieren.
Im Anschluss werden die verschiedenen Matrizen konkateniert und in einem weiteren feedforward neuronalen Netz in ihrer Dimensionalität reduziert.
Die finale Embeddingmatrix kann für verschiedene Funktionalitäten eingesetzt werden.

Die Transformerarchitektur benutzt dieses Embedding, um damit im Decoder wieder neue Tokens zu erstellen.
Dafür wird eine ähnliche Vorgehensweise wie im Encoder betrieben.
Der größte Unterschied zwischen dem Encoder und dem Decoder ist, dass der Encoder den Input nur ein einziges Mal verarbeitet und die daraus entstandene Embeddingmatrix nicht mehr neu berechnet werden muss.
Dadurch ist dieser Schritt sehr schnell.
Im Gegensatz dazu wird die Decoderphase für jedes einzelne Outputtoken neu durchlaufen.
Zunächst wird dabei wieder eine Self-Attention-Matrix über den gesamten bisherigen Ausgabetokens generiert.
Die Self-Attention-Matrix wird dann genutzt, um eine weitere Attention-Matrix zu errechnen.
Diese benötigt nun wieder eine separate Query-, Key- und Value-Matrix.
Im Vergleich zur Self-Attention werden dabei aber nicht alle Matrizen aus derselben Embeddingmatrix errechnet.
Nur die Query-Matrix wird aus der vorher generierten Self-Attention-Matrix generiert.
Die Key- und die Value-Matrix werden aus der Embedding-Matrix des Encoders berechnet.
Dadurch wird die Information aus dem Userprompt bei der Generierung jedes einzelnen Tokens mitberücksichtigt.
Die resultierenden Matrizen aller Attentionköpfe werden wieder durch Feedforward-Netze propagiert, um sie zusammenzuführen und die Dimensionalität anzupassen.
Am Ende wird die ganze Matrix durch eine Softmax-Funktion geleitet, die die Ausgabewahrscheinlichkeiten für alle Tokens generiert.
Das heißt, die Ausgabe ist ein Vektor, der als Dimensionalität die Länge des Vokabulars der Tokens besitzt.
Welches Token am Ende generiert wird, kann zum Beispiel mit einem Greedy-Algorithmus bestimmt werden, der immer das Wort mit der größten Wahrscheinlichkeit auswählt.
Andere Auswahlmöglichkeiten wären zum Beispiel mit einem Beam-Search-Algorithmus möglich, bei dem mehrere Sequenzen generiert werden, um erst später zu schauen, welche Sequenz insgesamt die höchste Wahrscheinlichkeit besitzt.
In dieser Phase kann auch eine Zufallsvariable eingeführt werden, die immer ein Token aus den wahrscheinlichsten Token auswählt, um bei mehrfacher Wiederholung eines Prompts verschiedene Ergebnisse zu liefern.~\cite{vaswani2023}

Bei dem Beispiel von oben würde der Decoder ggf. versuchen, den Satz zu vollenden mit „Alice fuhr gestern durch die Nürnberger Innenstadt und bewunderte die beeindruckenden mittelalterlichen Bauwerke.“

\myfigure{transformer_architecture.png}{Transformer Architektur aus \cite{vaswani2023}}{transformer_architecture}


\section{Large Language Model}

Large Language Model (LLM) bezeichnet ein Sprachmodell, das auf vielen Parametern trainiert wurde.
Viele modernen LLMs basieren auf der Transformerarchitektur~\cite{patwardhan2023}, um aus einer Inputsequenz mithilfe des Encoders und des Decoders wieder eine Outputsequenz zu generieren.

Technisch gesehen ist die Funktionsweise eines LLMs nur eine Next-Token-Prediction.
Dabei wird nach Verarbeitung der Inputsequenz eine Wahrscheinlichkeit des Auftretens verschiedener Token ermittelt.
Das LLM generiert so einfach nur Sätze, die in dem vorherigen Kontext Sinn ergeben.
Durch weitere Techniken des Finetunings können die LLMs dazu gebracht werden, auf Fragen zu antworten oder Anweisungen zu befolgen.
Die LLMs werden mit weiteren Daten trainiert, in denen zum Beispiel das Antworten auf Fragen explizit demonstriert wird~\cite{ouyang}.
Verschiedene LLMs können dabei auf unterschiedliche Aufgaben spezialisiert werden.
LLMs werden zum Beispiel für Chatbots, Text-Übersetzungen, Sprach-Übersetzungen, Named Entity Recognition, Sentimentanalyse oder Klassifikation eingesetzt~\cite{patwardhan2023}.

\section{Chatbots}

Besonders die LLM-gestützten Chatbots werden aufgrund ihrer Leistungsfähigkeit immer beliebter.
Der bekannteste Chatbot ist ChatGPT von OpenAI~\cite{chatgpt}, welcher über 100 Millionen Nutzer und über eine Milliarde Aufrufe im Monat hat~\cite{tong2023}.
Weitere bekannte Chatbots sind Google Gemini~\cite{gemini}, die Claude-Chatbots~\cite{claude} von der Firma Anthropic oder Pi von Inflection AI~\cite{pi}.

Neben den bekannten kommerziellen Chatbots, die von großen Tech-Firmen entwickelt werden, entstehen immer mehr Open-Source-Modelle, die zwar noch nicht so leistungsstark sind, aber auf eigener Hardware lokal laufen können.
Bekannte Open-Source-Modelle sind zum Beispiel Gemma von Google~\cite{banks2024}, LLama 2 von Meta~\cite{touvron2023} oder Mixtral von Mistral AI~\cite{jiang2024}.
Diese Open-Source-Modelle sind meist wesentlich kleiner als die kommerziellen Chatbots und sind somit auf lokaler Consumer-Hardware ausführbar.
Typische Größen für Open-Source-LLMs sind 7 Milliarden-, 13 Milliarden-, 30 Milliarden- und 70 Milliarden-Parameter~\cite{touvron2023}, das neueste Google-Gemma-Modell hat sogar eine Variante mit nur zwei Milliarden-Parametern~\cite{banks2024}.

Die Vorteile eines selbst gehosteten LLMs sind vor allem die Datensicherheit und die Möglichkeit, die Modelle auf die eigenen Aufgaben zu spezialisieren durch die Technik des Fine-Tunings~\cite{ouyang}.
Dabei werden Basis-Chatbots wie LLama 2 auf weiteren spezifischen Beispielen trainiert, um bestimmte Fähigkeiten des Modells zu verbessern.

In dieser Arbeit wird aufgrund von fehlenden konsistenten Hardwareressourcen und zugunsten der Schnelligkeit noch auf das kommerzielle Produkt ChatGPT~\cite{chatgpt} zugegriffen, um Datenverarbeitungen durchzuführen.
In Zukunft könnte aufgrund der immer besseren und kleineren Open-Source-LLMs diese Funktionalität auch auf selbstgehostete LLMs verlagert werden.

\section{Hugging Face}

Da es mittlerweile eine Vielzahl unterschiedlicher LLMs gibt, die alle verschiedene spezielle Aufgaben besitzen und in bestimmten Aspekten jeweils Vor- und Nachteile aufweisen, ist es sinnvoll, diese untereinander vergleichen zu können.
Eine solche Vergleichsmöglichkeit bietet die Plattform Hugging Face~\cite{wolf2020}, auf der über 500.000 Machine-Learning-Modelle, davon mehr als 200.000 Transformer, gehostet werden.

In der Kategorie „Question Answering“ gibt es über 1300 Einträge und in der Kategorie „Sentence Similarity“ ca. 3000~\cite{2024}.
In beiden Kategorien existieren Embedding-Modelle.
Die einzelnen Modelle können kostenlos heruntergeladen werden, sie sind ausführlich dokumentiert und sie sind untereinander mithilfe einer Punkteabgabe auf verschiedenen Kontroll-Datensätzen vergleichbar.
Durch Leaderboards können die besten Modelle bestimmt werden.

\section{Massive Text Embedding Benchmark}

Hier wird beispielhaft das Massive Text Embedding Benchmark (MTEB)~\cite{muennighoff2023} Leaderboard  betrachtet.

Auf diesem Leaderboard werden Embedding-Modelle auf 58 verschiedenen Datenbeständen in 112 Sprachen evaluiert~\cite{muennighoff2023}.
Die Datenbestände sind dabei in acht verschiedene Aufgabenkategorien geteilt:
Bitext Mining, Classification, Clustering, Pair Classification, Reranking, Retrieval, Semantic Textual Similarity und Summarization.

Für diese Arbeit sind vor allem die einzelnen Aufgabenbereiche Retrieval und Reranking interessant. 
Im Bereich Retrieval besteht die Aufgabe, aus einer großen Menge an Daten, relevante Dokumente zu einem bestimmten Thema zu finden.
Bei der Reranking-Aufgabe gilt es, eine Liste an potenziell passenden Dokumenten nach Wichtigkeit zu sortieren.
Im Aufgabenbereich Retrieval werden die einzelnen Embeddings auf 15 verschiedene Datenbeständen evaluiert.~\cite{muennighoff2023}

Leider ist eine Evaluation auf deutscher Sprache noch nicht möglich, da zu wenig Datenbestände zur Verfügung stehen.
In Zukunft, wenn mehr Datenbestände für die deutsche Sprache entwickelt wurden, wird die Evaluation um eine deutsche Kategorie erweitert.~\cite{muennighoff2023a}
In \autoref{ch:experiments} wird versucht, die besten Embedding-Modelle für die Retrieval-Aufgabe des Podcast-Generators zu ermitteln.

\myfigure{mteb.png}{Die einzelnen Datenbestände in MTEB aus \cite{muennighoff2023}}{mteb}

\myfigure{mteb_screenshot.png}{Screenshot des MTEB Leaderboards \cite{mteb}}{mteb_screenshot}
