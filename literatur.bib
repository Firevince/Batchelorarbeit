@misc{ard,
  title = {{Online-Nutzung}},
  author = {ARD},
  journal = {Die ARD},
  urldate = {2023-12-20},
  abstract = {Die ARD weist monatliche Nutzungszahlen f{\"u}r ihre Gemeinschaftsangebote ARD-Audiothek, ARD-Mediathek, KiKA, Sportschau und Tagesschau sowie die Landesrundfunkanstalten aus.},
  howpublished = {https://www.ard.de/die-ard/Onlinenutzung-100},
  langid = {ngerman},
  file = {/Users/br/Zotero/storage/P6FHM7LS/Onlinenutzung-100.html}
}

@article{barthel,
  title = {Measuring {{News Consumption}} in a {{Digital Era}}},
  author = {Barthel, Michael and Mitchell, Amy and {Asare-Marfo}, Dorene and Kennedy, Courtney},
  langid = {english},
  keywords = {noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/barthel.pdf}
}

@article{biron2021,
  title = {Automatic Detection of Prosodic Boundaries in Spontaneous Speech},
  author = {Biron, Tirza and Baum, Daniel and Freche, Dominik and Matalon, Nadav and Ehrmann, Netanel and Weinreb, Eyal and Biron, David and Moses, Elisha},
  year = {2021},
  month = may,
  journal = {PLOS ONE},
  volume = {16},
  number = {5},
  pages = {e0250969},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0250969},
  urldate = {2024-03-11},
  abstract = {Automatic speech recognition (ASR) and natural language processing (NLP) are expected to benefit from an effective, simple, and reliable method to automatically parse conversational speech. The ability to parse conversational speech depends crucially on the ability to identify boundaries between prosodic phrases. This is done naturally by the human ear, yet has proved surprisingly difficult to achieve reliably and simply in an automatic manner. Efforts to date have focused on detecting phrase boundaries using a variety of linguistic and acoustic cues. We propose a method which does not require model training and utilizes two prosodic cues that are based on ASR output. Boundaries are identified using discontinuities in speech rate (pre-boundary lengthening and phrase-initial acceleration) and silent pauses. The resulting phrases preserve syntactic validity, exhibit pitch reset, and compare well with manual tagging of prosodic boundaries. Collectively, our findings support the notion of prosodic phrases that represent coherent patterns across textual and acoustic parameters.},
  langid = {english},
  keywords = {Acoustic signals,Acoustics,Language,Speech,Speech signal processing,Syllables,Syntax,Verbal communication},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/biron2021.pdf}
}

@misc{blueteamai,
  title = {Open-{{Source Vector Database Benchmarking}}},
  author = {Blueteam Ai},
  urldate = {2024-02-23},
  howpublished = {https://marketing.fmops.ai/blog/vector-benchmarking/},
  file = {/Users/br/Zotero/storage/CD2NWQAD/vector-benchmarking.html}
}

@misc{chartable,
  title = {Hacker {{News Recap Podcast}} - {{Data}} and {{Chart Rankings}}},
  author = {{chartable}},
  urldate = {2024-02-21},
  abstract = {View data about Hacker News Recap on Chartable. See historical chart positions, all episodes, and more.},
  howpublished = {https://chartable.com/podcasts/hacker-news-recap/charts},
  file = {/Users/br/Zotero/storage/PH5GLPHZ/charts.html}
}

@misc{chen2017,
  title = {Reading {{Wikipedia}} to {{Answer Open-Domain Questions}}},
  author = {Chen, Danqi and Fisch, Adam and Weston, Jason and Bordes, Antoine},
  year = {2017},
  month = apr,
  number = {arXiv:1704.00051},
  eprint = {1704.00051},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-30},
  abstract = {This paper proposes to tackle open- domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/chen2017.pdf;/Users/br/Zotero/storage/7EIFC6Y4/1704.html}
}

@misc{choi2018,
  title = {{{QuAC}} : {{Question Answering}} in {{Context}}},
  shorttitle = {{{QuAC}}},
  author = {Choi, Eunsol and He, He and Iyyer, Mohit and Yatskar, Mark and Yih, Wen-tau and Choi, Yejin and Liang, Percy and Zettlemoyer, Luke},
  year = {2018},
  month = aug,
  number = {arXiv:1808.07036},
  eprint = {1808.07036},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-08},
  abstract = {We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-of-the-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at http://quac.ai.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/choi2018.pdf;/Users/br/Zotero/storage/7Y37KALC/1808.html}
}

@misc{chroma,
  title = {Embeddings {\textbar} {{Chroma}}},
  author = {{chroma}},
  urldate = {2024-03-11},
  abstract = {Embeddings are the A.I-native way to represent any kind of data, making them the perfect fit for working with all kinds of A.I-powered tools and algorithms. They can represent text, images, and soon audio and video. There are many options for creating embeddings, whether locally using an installed library, or by calling an API.},
  howpublished = {https://www.trychroma.com/embeddings},
  langid = {english},
  file = {/Users/br/Zotero/storage/PH3U9GQE/embeddings.html}
}

@misc{clark2020,
  title = {{{ELECTRA}}: {{Pre-training Text Encoders}} as {{Discriminators Rather Than Generators}}},
  shorttitle = {{{ELECTRA}}},
  author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
  year = {2020},
  month = mar,
  number = {arXiv:2003.10555},
  eprint = {2003.10555},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-08},
  abstract = {Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/clark2020.pdf;/Users/br/Zotero/storage/KQEAMJR3/2003.html}
}

@misc{devlin2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-21},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Gelesen,Obsidian Notes},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/devlin2019.pdf;/Users/br/Zotero/storage/WC9M8QI2/1810.html}
}

@misc{du2017,
  title = {Learning to {{Ask}}: {{Neural Question Generation}} for {{Reading Comprehension}}},
  shorttitle = {Learning to {{Ask}}},
  author = {Du, Xinya and Shao, Junru and Cardie, Claire},
  year = {2017},
  month = apr,
  number = {arXiv:1705.00106},
  eprint = {1705.00106},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-08},
  abstract = {We study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence- vs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequence-to-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (i.e., grammaticality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/du2017.pdf;/Users/br/Zotero/storage/N4IELHG4/1705.html}
}

@misc{faster-whisper2024,
  title = {{{SYSTRAN}}/Faster-Whisper},
  author = {{faster-whisper}},
  year = {2024},
  month = mar,
  urldate = {2024-03-10},
  abstract = {Faster Whisper transcription with CTranslate2},
  copyright = {MIT},
  howpublished = {SYSTRAN},
  keywords = {deep-learning,inference,openai,quantization,speech-recognition,speech-to-text,transformer,whisper}
}

@misc{ffmpeg,
  title = {{{FFmpeg}}},
  author = {FFmpeg},
  urldate = {2024-03-08},
  howpublished = {https://ffmpeg.org/},
  file = {/Users/br/Zotero/storage/YJ79LMRY/ffmpeg.org.html}
}

@inproceedings{formal2021,
  title = {{{SPLADE}}: {{Sparse Lexical}} and {{Expansion Model}} for {{First Stage Ranking}}},
  shorttitle = {{{SPLADE}}},
  booktitle = {Proceedings of the 44th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Formal, Thibault and Piwowarski, Benjamin and Clinchant, St{\'e}phane},
  year = {2021},
  month = jul,
  pages = {2288--2292},
  publisher = {ACM},
  address = {Virtual Event Canada},
  doi = {10.1145/3404835.3463098},
  urldate = {2024-02-12},
  isbn = {978-1-4503-8037-9},
  langid = {english},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/formal2021.pdf}
}

@book{freiling2019,
  title = {{Entrepreneurship: Gr{\"u}ndung und Skalierung von Startups}},
  shorttitle = {{Entrepreneurship}},
  author = {Freiling, J{\"o}rg and Harima, Jan},
  year = {2019},
  publisher = {Springer Fachmedien Wiesbaden},
  address = {Wiesbaden},
  doi = {10.1007/978-3-658-26117-7},
  urldate = {2023-11-20},
  isbn = {978-3-658-26116-0 978-3-658-26117-7},
  langid = {ngerman},
  keywords = {LeanStartup,noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/freiling2019.pdf;/Users/br/My Drive/Vincent' Vault/Sources/PDFs/freiling22.pdf}
}

@misc{gehring2017,
  title = {Convolutional {{Sequence}} to {{Sequence Learning}}},
  author = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N.},
  year = {2017},
  month = jul,
  number = {arXiv:1705.03122},
  eprint = {1705.03122},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-22},
  abstract = {The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/gehring2017.pdf;/Users/br/Zotero/storage/KGJHIXAE/1705.html}
}

@misc{gotting2023,
  title = {Number of Podcast Listeners Worldwide 2024},
  author = {G{\"o}tting, Marie Charlotte},
  year = {2023},
  journal = {Statista},
  urldate = {2023-12-19},
  abstract = {According to a study from July 2021 on global podcast consumption, the number of podcast listeners worldwide has steadily increased and is predicted to rise even further.},
  howpublished = {https://www.statista.com/statistics/1291360/podcast-listeners-worldwide/},
  langid = {english},
  file = {/Users/br/Zotero/storage/KDEFHKPN/podcast-listeners-worldwide.html}
}

@misc{gris2023,
  title = {Evaluating {{OpenAI}}'s {{Whisper ASR}} for {{Punctuation Prediction}} and {{Topic Modeling}} of Life Histories of the {{Museum}} of the {{Person}}},
  author = {Gris, Lucas Rafael Stefanel and Marcacini, Ricardo and Junior, Arnaldo Candido and Casanova, Edresson and Soares, Anderson and Alu{\'i}sio, Sandra Maria},
  year = {2023},
  month = may,
  number = {arXiv:2305.14580},
  eprint = {2305.14580},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-08},
  abstract = {Automatic speech recognition (ASR) systems play a key role in applications involving human-machine interactions. Despite their importance, ASR models for the Portuguese language proposed in the last decade have limitations in relation to the correct identification of punctuation marks in automatic transcriptions, which hinder the use of transcriptions by other systems, models, and even by humans. However, recently Whisper ASR was proposed by OpenAI, a general-purpose speech recognition model that has generated great expectations in dealing with such limitations. This chapter presents the first study on the performance of Whisper for punctuation prediction in the Portuguese language. We present an experimental evaluation considering both theoretical aspects involving pausing points (comma) and complete ideas (exclamation, question, and fullstop), as well as practical aspects involving transcript-based topic modeling - an application dependent on punctuation marks for promising performance. We analyzed experimental results from videos of Museum of the Person, a virtual museum that aims to tell and preserve people's life histories, thus discussing the pros and cons of Whisper in a real-world scenario. Although our experiments indicate that Whisper achieves state-of-the-art results, we conclude that some punctuation marks require improvements, such as exclamation, semicolon and colon.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/gris2023.pdf;/Users/br/Zotero/storage/NC6IENYQ/2305.html}
}

@misc{hackl2023,
  title = {Is {{GPT-4}} a Reliable Rater? {{Evaluating Consistency}} in {{GPT-4 Text Ratings}}},
  shorttitle = {Is {{GPT-4}} a Reliable Rater?},
  author = {Hackl, Veronika and M{\"u}ller, Alexandra Elena and Granitzer, Michael and Sailer, Maximilian},
  year = {2023},
  month = aug,
  number = {arXiv:2308.02575},
  eprint = {2308.02575},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-13},
  abstract = {This study investigates the consistency of feedback ratings generated by OpenAI's GPT-4, a state-of-the-art artificial intelligence language model, across multiple iterations, time spans and stylistic variations. The model rated responses to tasks within the Higher Education (HE) subject domain of macroeconomics in terms of their content and style. Statistical analysis was conducted in order to learn more about the interrater reliability, consistency of the ratings across iterations and the correlation between ratings in terms of content and style. The results revealed a high interrater reliability with ICC scores ranging between 0.94 and 0.99 for different timespans, suggesting that GPT-4 is capable of generating consistent ratings across repetitions with a clear prompt. Style and content ratings show a high correlation of 0.87. When applying a non-adequate style the average content ratings remained constant, while style ratings decreased, which indicates that the large language model (LLM) effectively distinguishes between these two criteria during evaluation. The prompt used in this study is furthermore presented and explained. Further research is necessary to assess the robustness and reliability of AI models in various use cases.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/hackl2023.pdf;/Users/br/Zotero/storage/QEZ45MIM/2308.html}
}

@article{hassija2024,
  title = {Interpreting {{Black-Box Models}}: {{A Review}} on {{Explainable Artificial Intelligence}}},
  shorttitle = {Interpreting {{Black-Box Models}}},
  author = {Hassija, Vikas and Chamola, Vinay and Mahapatra, Atmesh and Singal, Abhinandan and Goel, Divyansh and Huang, Kaizhu and Scardapane, Simone and Spinelli, Indro and Mahmud, Mufti and Hussain, Amir},
  year = {2024},
  month = jan,
  journal = {Cognitive Computation},
  volume = {16},
  number = {1},
  pages = {45--74},
  issn = {1866-9956, 1866-9964},
  doi = {10.1007/s12559-023-10179-8},
  urldate = {2024-03-09},
  abstract = {Recent years have seen a tremendous growth in Artificial Intelligence (AI)-based methodological development in a broad range of domains. In this rapidly evolving field, large number of methods are being reported using machine learning (ML) and Deep Learning (DL) models. Majority of these models are inherently complex and lacks explanations of the decision making process causing these models to be termed as 'Black-Box'. One of the major bottlenecks to adopt such models in mission-critical application domains, such as banking, e-commerce, healthcare, and public services and safety, is the difficulty in interpreting them. Due to the rapid proleferation of these AI models, explaining their learning and decision making process are getting harder which require transparency and easy predictability. Aiming to collate the current state-of-the-art in interpreting the black-box models, this study provides a comprehensive analysis of the explainable AI (XAI) models. To reduce false negative and false positive outcomes of these back-box models, finding flaws in them is still difficult and inefficient. In this paper, the development of XAI is reviewed meticulously through careful selection and analysis of the current state-of-the-art of XAI research. It also provides a comprehensive and in-depth evaluation of the XAI frameworks and their efficacy to serve as a starting point of XAI for applied and theoretical researchers. Towards the end, it highlights emerging and critical issues pertaining to XAI research to showcase major, model-specific trends for better explanation, enhanced transparency, and improved prediction accuracy.},
  langid = {english},
  file = {/Users/br/Zotero/storage/MGJL3KYS/Hassija et al. - 2024 - Interpreting Black-Box Models A Review on Explain.pdf}
}

@misc{hendrycks2021,
  title = {{{CUAD}}: {{An Expert-Annotated NLP Dataset}} for {{Legal Contract Review}}},
  shorttitle = {{{CUAD}}},
  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},
  year = {2021},
  month = nov,
  number = {arXiv:2103.06268},
  eprint = {2103.06268},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-30},
  abstract = {Many specialized domains remain untouched by deep learning, as large labeled datasets require expensive expert annotators. We address this bottleneck within the legal domain by introducing the Contract Understanding Atticus Dataset (CUAD), a new dataset for legal contract review. CUAD was created with dozens of legal experts from The Atticus Project and consists of over 13,000 annotations. The task is to highlight salient portions of a contract that are important for a human to review. We find that Transformer models have nascent performance, but that this performance is strongly influenced by model design and training dataset size. Despite these promising results, there is still substantial room for improvement. As one of the only large, specialized NLP benchmarks annotated by experts, CUAD can serve as a challenging research benchmark for the broader NLP community.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/hendrycks2021.pdf;/Users/br/Zotero/storage/SBRTI5BL/2103.html}
}

@article{honnibal2017,
  title = {{{spaCy}} 2: {{Natural}} Language Understanding with {{Bloom}} Embeddings, Convolutional Neural Networks and Incremental Parsing},
  shorttitle = {{{spaCy}} 2},
  author = {Honnibal, Matthew and Montani, Ines},
  year = {2017},
  journal = {To appear},
  volume = {7},
  number = {1},
  pages = {411--420}
}

@misc{hu2021,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and {Allen-Zhu}, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  year = {2021},
  month = oct,
  number = {arXiv:2106.09685},
  eprint = {2106.09685},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.09685},
  urldate = {2023-11-18},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/hu2021.pdf;/Users/br/Zotero/storage/4RYBBQ57/2106.html}
}

@article{index2023,
  title = {{{TIOBE Index}}---{{May}} 2023},
  author = {Index, {\relax TIOBE}},
  year = {2023},
  journal = {Accessed: Jun},
  volume = {2}
}

@misc{jiang2024,
  title = {Mixtral of {{Experts}}},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Hanna, Emma Bou and Bressand, Florian and Lengyel, Gianna and Bour, Guillaume and Lample, Guillaume and Lavaud, L{\'e}lio Renard and Saulnier, Lucile and Lachaux, Marie-Anne and Stock, Pierre and Subramanian, Sandeep and Yang, Sophia and Antoniak, Szymon and Scao, Teven Le and Gervet, Th{\'e}ophile and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'e}e and Sayed, William El},
  year = {2024},
  month = jan,
  number = {arXiv:2401.04088},
  eprint = {2401.04088},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-16},
  abstract = {We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/jiang2024.pdf;/Users/br/Zotero/storage/GSKMV5FV/2401.html}
}

@misc{jones2021,
  title = {{{TREC}} 2020 {{Podcasts Track Overview}}},
  author = {Jones, Rosie and Carterette, Ben and Clifton, Ann and Eskevich, Maria and Jones, Gareth J. F. and Karlgren, Jussi and Pappu, Aasish and Reddy, Sravana and Yu, Yongze},
  year = {2021},
  month = mar,
  number = {arXiv:2103.15953},
  eprint = {2103.15953},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.15953},
  urldate = {2023-11-06},
  abstract = {The Podcast Track is new at the Text Retrieval Conference (TREC) in 2020. The podcast track was designed to encourage research into podcasts in the information retrieval and NLP research communities. The track consisted of two shared tasks: segment retrieval and summarization, both based on a dataset of over 100,000 podcast episodes (metadata, audio, and automatic transcripts) which was released concurrently with the track. The track generated considerable interest, attracted hundreds of new registrations to TREC and fifteen teams, mostly disjoint between search and summarization, made final submissions for assessment. Deep learning was the dominant experimental approach for both search experiments and summarization. This paper gives an overview of the tasks and the results of the participants' experiments. The track will return to TREC 2021 with the same two tasks, incorporating slight modifications in response to participant feedback.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/jones2021.pdf;/Users/br/Zotero/storage/339BCE5J/2103.html}
}

@article{kang2012,
  title = {Effects of Podcast Tours on Tourist Experiences in a National Park},
  author = {Kang, Myunghwa and Gretzel, Ulrike},
  year = {2012},
  month = apr,
  journal = {Tourism Management},
  volume = {33},
  number = {2},
  pages = {440--455},
  issn = {02615177},
  doi = {10.1016/j.tourman.2011.05.005},
  urldate = {2023-11-08},
  abstract = {This study examines the influence of podcast tours on tourist experiences. Based on theoretical accounts that human voices convey rich social information, this study proposes that podcast tours increase perceived social presence and mindfulness that lead to enhanced tourist experiences and environmental stewardship. A field experiment was conducted at a national park using MP3 players containing podcast tours based on four experimental conditions: 2 information source compositions (single vs. multiple narrator voices) {\^A} 2 narrating styles (formal vs. conversational). The results support that even if communicated through audio-only media, the human voice creates a positive social context for meaningful interaction which influences tourist experiences and stewardship. Mindfulness was also found to be an important construct affecting the quality of experiences. The findings support the usefulness of podcast tours as interpretative media.},
  langid = {english},
  keywords = {Obsidian Notes,uberflogen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/kang2012.pdf}
}

@misc{karpukhin2020,
  title = {Dense {{Passage Retrieval}} for {{Open-Domain Question Answering}}},
  author = {Karpukhin, Vladimir and O{\u g}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  year = {2020},
  month = sep,
  number = {arXiv:2004.04906},
  eprint = {2004.04906},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-08},
  abstract = {Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9\%-19\% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/karpukhin2020.pdf;/Users/br/Zotero/storage/62QDY3FJ/2004.html}
}

@inproceedings{kharis2021,
  title = {How to {{Lemmatize German Words}} with {{NLP-Spacy Lemmatizer}}?:},
  shorttitle = {How to {{Lemmatize German Words}} with {{NLP-Spacy Lemmatizer}}?},
  booktitle = {International {{Seminar}} on {{Language}}, {{Education}}, and {{Culture}} ({{ISoLEC}} 2021)},
  author = {Kharis, M. and {Kisyani} and {Suhartono} and Pairin, Udjang and {Darni}},
  year = {2021},
  address = {Malang, Indonesia},
  doi = {10.2991/assehr.k.211212.036},
  urldate = {2024-02-09},
  abstract = {Simple algorithms for the lemmatization process have been developed to recognize changes in a word as a result of grammatical processes and changes. Lemmatizer tools can analyze the types of word changes in the German language. Thus, this paper aims at investigating how the lemmatization of German words is aided by the Lemmatizer software. NLP Lemmatizer spacy, in cooperation with Python and Visual Studio Code, is utilized to find out the primary form of the word changes in German language. Based on the lemmatization analysis results, Lemmatizer SpaCy can analyze the shape of token, lemma, and PoS-tag of words in German. However, there are some errors identified during the process of finding out the word changes in German language.},
  langid = {english},
  file = {/Users/br/Zotero/storage/5IIBD4BU/Kharis et al. - 2021 - How to Lemmatize German Words with NLP-Spacy Lemma.pdf}
}

@misc{kingma2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-22},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/kingma2017.pdf;/Users/br/Zotero/storage/IDKZQ8S4/1412.html}
}

@inproceedings{klein2017,
  title = {{{OpenNMT}}: {{Open-Source Toolkit}} for {{Neural Machine Translation}}},
  shorttitle = {{{OpenNMT}}},
  booktitle = {Proceedings of {{ACL}} 2017, {{System Demonstrations}}},
  author = {Klein, Guillaume and Kim, Yoon and Deng, Yuntian and Senellart, Jean and Rush, Alexander},
  year = {2017},
  pages = {67--72},
  publisher = {Association for Computational Linguistics},
  address = {Vancouver, Canada},
  doi = {10.18653/v1/P17-4012},
  urldate = {2024-03-07},
  langid = {english},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/klein2017.pdf}
}

@inproceedings{laban2022,
  title = {{{NewsPod}}: {{Automatic}} and {{Interactive News Podcasts}}},
  shorttitle = {{{NewsPod}}},
  booktitle = {27th {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Laban, Philippe and Ye, Elicia and Korlakunta, Srujay and Canny, John and Hearst, Marti},
  year = {2022},
  month = mar,
  pages = {691--706},
  publisher = {ACM},
  address = {Helsinki Finland},
  doi = {10.1145/3490099.3511147},
  urldate = {2023-11-06},
  isbn = {978-1-4503-9144-3},
  langid = {english},
  keywords = {Gelesen,Obsidian Notes},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/laban2022.pdf}
}

@misc{lemur,
  title = {Lemur {{Project Components}}: {{Indri}}},
  author = {{lemur}},
  urldate = {2024-03-09},
  howpublished = {https://www.lemurproject.org/indri.php},
  file = {/Users/br/Zotero/storage/V2T3RMF3/indri.html}
}

@misc{listennotes,
  title = {Podcast {{Stats}}: {{How}} Many Podcasts Are There?},
  shorttitle = {Podcast {{Stats}}},
  author = {{listennotes}},
  journal = {Listen Notes},
  urldate = {2024-03-02},
  abstract = {Podcast industry data through the lens of Listen Notes, the best podcast search engine and database.},
  howpublished = {https://www.listennotes.com/podcast-stats/},
  langid = {english},
  file = {/Users/br/Zotero/storage/SHB6B2LM/podcast-stats.html}
}

@misc{liu2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  number = {arXiv:1907.11692},
  eprint = {1907.11692},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-21},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/liu2019.pdf;/Users/br/Zotero/storage/AI2G9PVC/1907.html}
}

@inproceedings{lochrie2018,
  title = {Designing {{Immersive Audio Experiences}} for {{News}} and {{Information}} in the {{Internet}} of {{Things}} Using {{Text-to-Speech Objects}}},
  booktitle = {Proceedings of the 32nd {{International BCS Human Computer Interaction Conference}}},
  author = {Lochrie, Mark and {De-Neef}, Robin and Mills, John and Davenport, Jack},
  year = {2018},
  doi = {10.14236/ewic/HCI2018.90},
  urldate = {2023-11-08},
  langid = {english},
  keywords = {noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/lochrie2018.pdf}
}

@article{maroni2020,
  title = {{K{\"U}NSTLICHE INTELLIGENZ IM PRODUKTIVEN EINSATZ F{\"U}R DIE AUTOMATISIERTE ERSCHLIESSUNG IM MULTIMEDIALEN PRODUKTIONSPROZESS}},
  author = {Maroni, Dirk and K{\"o}hler, Joachim and Fisseler, Jens and Becker, Sven},
  year = {2020},
  langid = {ngerman},
  keywords = {noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/maroni2020.pdf}
}

@misc{mindline-media2023,
  title = {Bericht-{{OAM}}\_2023.Pdf},
  author = {{mindline-media}},
  year = {2023},
  urldate = {2024-02-04},
  file = {/Users/br/Zotero/storage/RDVZIUUX/Bericht-OAM_2023.pdf}
}

@inproceedings{moldovan2000,
  title = {The {{Structure}} and {{Performance}} of an {{Open-Domain Question Answering System}}},
  booktitle = {Proceedings of the 38th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Moldovan, Dan and Harabagiu, Sanda and Pasca, Marius and Mihalcea, Rada and Girju, Roxana and Goodrum, Richard and Rus, Vasile},
  year = {2000},
  month = oct,
  pages = {563--570},
  publisher = {Association for Computational Linguistics},
  address = {Hong Kong},
  doi = {10.3115/1075218.1075289},
  urldate = {2023-11-08},
  keywords = {noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/moldovan2000.pdf}
}

@misc{mteb,
  title = {{{MTEB Leaderboard}} - a {{Hugging Face Space}} by Mteb},
  author = {{mteb}},
  urldate = {2024-03-11},
  abstract = {Discover amazing ML apps made by the community},
  howpublished = {https://huggingface.co/spaces/mteb/leaderboard},
  file = {/Users/br/Zotero/storage/WVLA5S5Q/leaderboard.html}
}

@misc{muennighoff2023,
  title = {{{MTEB}}: {{Massive Text Embedding Benchmark}}},
  shorttitle = {{{MTEB}}},
  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"i}c and Reimers, Nils},
  year = {2023},
  month = mar,
  number = {arXiv:2210.07316},
  eprint = {2210.07316},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-12},
  abstract = {Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/br/Zotero/storage/XN992GU2/Muennighoff et al. - 2023 - MTEB Massive Text Embedding Benchmark.pdf;/Users/br/Zotero/storage/YGG5N42T/2210.html}
}

@misc{muennighoff2023a,
  title = {Mteb/Leaderboard {$\cdot$} {{Restart}} the Space for New Models},
  author = {Muennighoff},
  year = {2023},
  month = sep,
  urldate = {2024-03-05},
  abstract = {Hi, @ Muennighoff Thanks for the great work! I submitted two new Chinese Text Embedding models: "stella-base-zh" and "stella-large-zh" , can you help restart this space?},
  howpublished = {https://huggingface.co/spaces/mteb/leaderboard/discussions/28},
  file = {/Users/br/Zotero/storage/ZN22FB4K/28.html}
}

@inproceedings{naismith2023,
  title = {Automated Evaluation of Written Discourse Coherence Using {{GPT-4}}},
  booktitle = {Proceedings of the 18th {{Workshop}} on {{Innovative Use}} of {{NLP}} for {{Building Educational Applications}} ({{BEA}} 2023)},
  author = {Naismith, Ben and Mulcaire, Phoebe and Burstein, Jill},
  editor = {Kochmar, Ekaterina and Burstein, Jill and Horbach, Andrea and {Laarmann-Quante}, Ronja and Madnani, Nitin and Tack, Ana{\"i}s and Yaneva, Victoria and Yuan, Zheng and Zesch, Torsten},
  year = {2023},
  month = jul,
  pages = {394--403},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.bea-1.32},
  urldate = {2024-03-11},
  abstract = {The popularization of large language models (LLMs) such as OpenAI's GPT-3 and GPT-4 have led to numerous innovations in the field of AI in education. With respect to automated writing evaluation (AWE), LLMs have reduced challenges associated with assessing writing quality characteristics that are difficult to identify automatically, such as discourse coherence. In addition, LLMs can provide rationales for their evaluations (ratings) which increases score interpretability and transparency. This paper investigates one approach to producing ratings by training GPT-4 to assess discourse coherence in a manner consistent with expert human raters. The findings of the study suggest that GPT-4 has strong potential to produce discourse coherence ratings that are comparable to human ratings, accompanied by clear rationales. Furthermore, the GPT-4 ratings outperform traditional NLP coherence metrics with respect to agreement with human ratings. These results have implications for advancing AWE technology for learning and assessment.},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/naismith2023.pdf}
}

@article{newman2022,
  title = {Reuters {{Institute Digital News Report}} 2022},
  author = {Newman, Nic and Fletcher, Richard and Robertson, Craig T and Eddy, Kirsten and Nielsen, Rasmus Kleis},
  year = {2022},
  langid = {english},
  file = {/Users/br/Zotero/storage/WE8MUI4S/Newman et al. - 2022 - Reuters Institute Digital News Report 2022.pdf}
}

@article{nilsson,
  title = {{{GPT-4}} as an {{Automatic Grader}}},
  author = {Nilsson, Filippa and Tuvstedt, Jonatan},
  abstract = {Education is a field with a lot of time consuming tasks outside the core charge of teaching. One of these arduous tasks is grading, which can be monotonous and very time consuming. An emerging field that could potentially alleviate this is Artificial Intelligence (AI), or more specifically, Large Language Models (LLM:s), that have advanced immensely in the last year following the release of ChatGPT. This thesis investigates the accuracy of grading by GPT-4 compared to Teachers Assistants on introductory programming assignments. The work of a total of 73 students in the introductory programming courses INDA at KTH Royal Institute of Technology was examined by GPT and graded. The grading was accomplished by sending a prompt to GPT, consisting of plain text copies of the assignment, the grading criteria, the student submission and instructions on how to grade. The results were very promising, with GPT having an overall accuracy of 75\% when compared to grades by Teachers Assistants. However, it was significantly worse at correctly identifying submissions that failed compared to those that passed. The results indicated that AI could be able to grade students' work reliably in the future, if the development of LLM:s continue progressing. In the meantime, AI can be used as a grading tool for educators around the world, alleviating their workload.},
  langid = {english},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/nilsson.pdf}
}

@book{nilsson2023,
  title = {{{GPT-4}} as an {{Automatic Grader}} : {{The}} Accuracy of Grades Set by {{GPT-4}} on Introductory Programming Assignments},
  shorttitle = {{{GPT-4}} as an {{Automatic Grader}}},
  author = {Nilsson, Filippa and Tuvstedt, Jonatan},
  year = {2023},
  urldate = {2024-03-11},
  abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
  langid = {english},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/nilsson2023.pdf}
}

@misc{oord2016,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveNet}}},
  author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  year = {2016},
  month = sep,
  number = {arXiv:1609.03499},
  eprint = {1609.03499},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-08},
  abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-ofthe-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/oord2016.pdf}
}

@inproceedings{pennington2014,
  title = {Glove: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {Glove},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  year = {2014},
  pages = {1532--1543},
  publisher = {Association for Computational Linguistics},
  address = {Doha, Qatar},
  doi = {10.3115/v1/D14-1162},
  urldate = {2023-12-21},
  abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
  langid = {english},
  file = {/Users/br/Zotero/storage/G6HZD3UC/Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf}
}

@misc{phillips2014,
  title = {How {{Diversity Makes Us Smarter}}},
  author = {Phillips, Katherine W.},
  year = {2014},
  month = oct,
  journal = {Scientific American},
  urldate = {2023-12-15},
  abstract = {Being around people who are different from us makes us more creative, more diligent and harder-working},
  howpublished = {https://www.scientificamerican.com/article/how-diversity-makes-us-smarter/},
  langid = {english},
  file = {/Users/br/Zotero/storage/TPDE2L98/how-diversity-makes-us-smarter.html}
}

@misc{pickle,
  title = {Pickle --- {{Python}} Object Serialization},
  author = {{pickle}},
  journal = {Python documentation},
  urldate = {2024-03-11},
  abstract = {Source code: Lib/pickle.py The pickle module implements binary protocols for serializing and de-serializing a Python object structure. ``Pickling'' is the process whereby a Python object hierarchy is...},
  howpublished = {https://docs.python.org/3/library/pickle.html},
  langid = {english},
  file = {/Users/br/Zotero/storage/XQVHCTRD/pickle.html}
}

@inproceedings{pirani2022,
  title = {A {{Comparative Analysis}} of {{ARIMA}}, {{GRU}}, {{LSTM}} and {{BiLSTM}} on {{Financial Time Series Forecasting}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Distributed Computing}} and {{Electrical Circuits}} and {{Electronics}} ({{ICDCECE}})},
  author = {Pirani, Muskaan and Thakkar, Paurav and Jivrani, Pranay and Bohara, Mohammed Husain and Garg, Dweepna},
  year = {2022},
  month = apr,
  pages = {1--6},
  publisher = {IEEE},
  address = {Ballari, India},
  doi = {10.1109/ICDCECE53908.2022.9793213},
  urldate = {2024-03-06},
  abstract = {Machine learning and profound learning algorithms were one in every of the effective techniques to statistical prediction. Once it involves time series prediction, these algorithms shelled classic regression-based solutions in terms of accuracy. Long short-term memory (LSTM), one of the recurrent neural networks (RNN), has been incontestable to outperform typical prediction methods. The LSTM-based models are incorporated with further ``gates'' such that it will consider input data of longer sequences. LSTM-based models outperform Autoregressive Integrated Moving Average models attributable to these further capabilities (ARIMA). GatedRecurrent Unit (GRU) and bidirectional long short-term memory (BiLSTM) are extended versions of LSTM. The major question is that an algorithmic program would shell the other two by giving smart predictions with minimum error. Bidirectional LSTMs provide extra training because it may be a 2-way formula, thus, it'll traverse the training information double (1. Left-to-right 2. Right-to-left). GRU has one gate below the LSTM architecture. Hence, our analysis is especially centred on that algorithm outperforms the opposite two and it conjointly deals with behavioural analysis of the algorithms, their comparison and therefore the standardization of hyperparameters.},
  isbn = {978-1-66548-316-2},
  langid = {english},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/pirani2022.pdf}
}

@misc{podcastindex.org,
  title = {Stats},
  author = {{podcastindex.org}},
  journal = {Podcastindex.org},
  urldate = {2024-03-02},
  abstract = {The Podcast Index is here to preserve, protect and extend the open, independent podcasting ecosystem.},
  howpublished = {https://podcastindex.org/stats?utm\_source=podnews.net\&utm\_medium=web\&utm\_campaign=podnews.net:2022-03-10},
  langid = {english},
  file = {/Users/br/Zotero/storage/8WKGJHDI/stats.html}
}

@misc{podcastle2023,
  title = {The {{Future}} of {{Podcasting}} with {{Artificial Intelligence}}},
  author = {Podcastle},
  year = {2023},
  month = jan,
  journal = {Podcastle Blog},
  urldate = {2024-02-21},
  abstract = {AI in podcasting can help make podcasts more interactive, personalized, and engaging for listeners. Let's explore how AI is transforming the podcasting industry.},
  howpublished = {https://podcastle.ai/blog/ai-in-podcasting/},
  langid = {english},
  file = {/Users/br/Zotero/storage/E4ZGX3A6/ai-in-podcasting.html}
}

@article{radford,
  title = {Robust {{Speech Recognition}} via {{Large-Scale Weak Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
  langid = {english},
  file = {/Users/br/Zotero/storage/Y6YAD4ZI/Radford et al. - Robust Speech Recognition via Large-Scale Weak Sup.pdf}
}

@article{reddy2019,
  title = {{{CoQA}}: {{A Conversational Question Answering Challenge}}},
  shorttitle = {{{CoQA}}},
  author = {Reddy, Siva and Chen, Danqi and Manning, Christopher D.},
  year = {2019},
  month = nov,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {7},
  pages = {249--266},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00266},
  urldate = {2023-11-08},
  abstract = {Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets (e.g., coreference and pragmatic reasoning). We evaluate strong dialogue and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4\%, which is 23.4 points behind human performance (88.8\%), indicating that there is ample room for improvement. We present CoQA as a challenge to the community at https://stanfordnlp.github. io/coqa.},
  langid = {english},
  keywords = {noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/reddy2019.pdf}
}

@misc{reimers2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = aug,
  number = {arXiv:1908.10084},
  eprint = {1908.10084},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-21},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/reimers2019.pdf;/Users/br/Zotero/storage/6WCCSTA9/1908.html}
}

@inproceedings{sahijwani2020,
  title = {Would {{You Like}} to {{Hear}} the {{News}}?: {{Investigating Voice-Based Suggestions}} for {{Conversational News Recommendation}}},
  shorttitle = {Would {{You Like}} to {{Hear}} the {{News}}?},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Human Information Interaction}} and {{Retrieval}}},
  author = {Sahijwani, Harshita and Choi, Jason Ingyu and Agichtein, Eugene},
  year = {2020},
  month = mar,
  pages = {437--441},
  publisher = {ACM},
  address = {Vancouver BC Canada},
  doi = {10.1145/3343413.3378013},
  urldate = {2023-11-08},
  abstract = {One of the key benefits of voice-based personal assistants is the potential to proactively recommend relevant and interesting information. One of the most valuable sources of such information is the News. However, in order for the user to hear the news that is useful and relevant to them, it must be recommended in an interesting and informative way. However, to the best of our knowledge, how to present a news item for a voice-based recommendation remains an open question. In this paper, we empirically compare different ways of recommending news, or specific news items, in a voice-based conversational setting. Specifically, we study the user engagement and satisfaction with five different variants of presenting news recommendations: (1) a generic news briefing; (2) news about a specific entity relevant to the current conversation; (3) news about an entity from a past conversation; (4) news on a trending news topic; and (5) the default - a suggestion to talk about news in general. Our results show that entity-based news recommendations exhibit 29\% higher acceptance compared to briefing recommendations, and almost 100\% higher acceptance compared to recommending generic or trending news. Our investigation into the presentation of news recommendations and the resulting insights could make voice assistants more informative and engaging.},
  isbn = {978-1-4503-6892-6},
  langid = {english},
  keywords = {noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/sahijwani2020.pdf}
}

@article{shanahan2024,
  title = {Talking about {{Large Language Models}}},
  author = {Shanahan, Murray},
  year = {2024},
  month = feb,
  journal = {Communications of the ACM},
  volume = {67},
  number = {2},
  pages = {68--79},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3624724},
  urldate = {2024-03-05},
  abstract = {Interacting with a contemporary LLM-based conversational agent can create an illusion of being in the presence of a thinking creature. Yet, in their very nature, such systems are fundamentally not like us.},
  langid = {english},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/shanahan2024.pdf}
}

@inproceedings{shi2023,
  title = {Evaluating and {{Personalizing User-Perceived Quality}} of {{Text-to-Speech Voices}} for {{Delivering Mindfulness Meditation}} with {{Different Physical Embodiments}}},
  booktitle = {Proceedings of the 2023 {{ACM}}/{{IEEE International Conference}} on {{Human-Robot Interaction}}},
  author = {Shi, Zhonghao and Chen, Han and Velentza, Anna-Maria and Liu, Siqi and Dennler, Nathaniel and O'Connell, Allison and Mataric, Maja},
  year = {2023},
  month = mar,
  pages = {516--524},
  publisher = {ACM},
  address = {Stockholm Sweden},
  doi = {10.1145/3568162.3576987},
  urldate = {2023-12-20},
  isbn = {978-1-4503-9964-7},
  langid = {english},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/shi2023.pdf}
}

@misc{spacy2024,
  title = {German {$\cdot$} {{spaCy Models Documentation}}},
  author = {{spacy}},
  year = {2024},
  journal = {German},
  urldate = {2024-02-17},
  abstract = {spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more.},
  howpublished = {https://spacy.io/models/de},
  langid = {english},
  file = {/Users/br/Zotero/storage/BL54RMCR/de.html}
}

@misc{statista-a,
  title = {{Reichweite der ARD-Gemeinschaftsangebote 2024}},
  author = {{statista-a}},
  journal = {Statista},
  urldate = {2024-03-10},
  abstract = {Im Januar 2024 war die ARD-Mediathek mit rund 191,5 Millionen Visits das meistaufgerufene Onlineangebot der ARD.},
  howpublished = {https://de.statista.com/statistik/daten/studie/1249938/umfrage/online-nutzung-anhand-von-aufrufzahlen-der-ard-gemeinschaftsangebote/},
  langid = {ngerman},
  file = {/Users/br/Zotero/storage/R7SMNYFW/online-nutzung-anhand-von-aufrufzahlen-der-ard-gemeinschaftsangebote.html}
}

@misc{tiktoken2024,
  title = {Openai/Tiktoken},
  author = {{tiktoken}},
  year = {2024},
  month = mar,
  urldate = {2024-03-11},
  abstract = {tiktoken is a fast BPE tokeniser for use with OpenAI's models.},
  copyright = {MIT},
  howpublished = {OpenAI}
}

@article{tong2023,
  title = {Exclusive: {{ChatGPT}} Traffic Slips Again for Third Month in a Row},
  shorttitle = {Exclusive},
  author = {Tong, Anna and Tong, Anna},
  year = {2023},
  month = sep,
  journal = {Reuters},
  urldate = {2024-03-12},
  abstract = {OpenAI's ChatGPT, the wildly popular artificial intelligence chatbot launched in November, saw monthly website visits decline for the third month in a row in August, though there are signs the decline is coming to an end, according to analytics firm Similarweb.},
  chapter = {Technology},
  langid = {english},
  file = {/Users/br/Zotero/storage/DA6EFFD9/chatgpt-traffic-slips-again-third-month-row-2023-09-07.html}
}

@misc{touvron2023,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  year = {2023},
  month = jul,
  number = {arXiv:2307.09288},
  eprint = {2307.09288},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-29},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/touvron2023.pdf;/Users/br/Zotero/storage/8KXLYRP7/2307.html}
}

@misc{vaswani2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2023},
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-18},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Gelesen,Obsidian Notes},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/vaswani2023.pdf;/Users/br/Zotero/storage/UCE2RMK5/1706.html}
}

@article{wagner2023,
  title = {Dawn of the {{Transformer Era}} in {{Speech Emotion Recognition}}: {{Closing}} the {{Valence Gap}}},
  shorttitle = {Dawn of the {{Transformer Era}} in {{Speech Emotion Recognition}}},
  author = {Wagner, Johannes and Triantafyllopoulos, Andreas and Wierstorf, Hagen and Schmitt, Maximilian and Burkhardt, Felix and Eyben, Florian and Schuller, Bj{\"o}rn W.},
  year = {2023},
  month = sep,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {45},
  number = {9},
  pages = {10745--10759},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2023.3263585},
  urldate = {2023-12-20},
  abstract = {Recent advances in transformer-based architectures have shown promise in several machine learning tasks. In the audio domain, such architectures have been successfully utilised in the field of speech emotion recognition (SER). However, existing works have not evaluated the influence of model size and pre-training data on downstream performance, and have shown limited attention to generalisation, robustness, fairness, and efficiency. The present contribution conducts a thorough analysis of these aspects on several pre-trained variants of wav2vec 2.0 and HuBERT that we fine-tuned on the dimensions arousal, dominance, and valence of MSP-Podcast, while additionally using IEMOCAP and MOSI to test cross-corpus generalisation. To the best of our knowledge, we obtain the top performance for valence prediction without use of explicit linguistic information, with a concordance correlation coefficient (CCC) of. 638 on MSP-Podcast. Our investigations reveal that transformer-based architectures are more robust compared to a CNN-based baseline and fair with respect to gender groups, but not towards individual speakers. Finally, we show that their success on valence is based on implicit linguistic information, which explains why they perform on-par with recent multimodal approaches that explicitly utilise textual information. To make our findings reproducible, we release the best performing model to the community.},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/wagner2023.pdf;/Users/br/Zotero/storage/ZPH72A55/10089511.html}
}

@misc{wang2023,
  title = {{{PodReels}}: {{Human-AI Co-Creation}} of {{Video Podcast Teasers}}},
  shorttitle = {{{PodReels}}},
  author = {Wang, Sitong and Ning, Zheng and Truong, Anh and Dontcheva, Mira and Li, Dingzeyu and Chilton, Lydia B.},
  year = {2023},
  month = nov,
  number = {arXiv:2311.05867},
  eprint = {2311.05867},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-04},
  abstract = {Video podcast teasers are short videos that can be shared on social media platforms to capture interest in the full episodes of a video podcast. These teasers enable long-form podcasters to reach new audiences and gain new followers. However, creating a compelling teaser from an hour-long episode is challenging. Selecting interesting clips requires significant mental effort; editing the chosen clips into a cohesive, well-produced teaser is time-consuming. To support the creation of video podcast teasers, we first investigate what makes a good teaser. We combine insights from both audience comments and creator interviews to determine a set of essential ingredients. We also identify a common workflow shared by creators during the process. Based on these findings, we introduce a human-AI co-creative tool called PodReels to assist video podcasters in creating teasers. Our user study shows that PodReels significantly reduces creators' mental demand and improves their efficiency in producing video podcast teasers.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/wang2023.pdf;/Users/br/Zotero/storage/F4GZKLWC/2311.html}
}

@inproceedings{zhang2020,
  title = {{{PEGASUS}}},
  shorttitle = {{{PEGASUS}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter},
  year = {2020},
  month = nov,
  pages = {11328--11339},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-11-08},
  abstract = {Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.},
  langid = {english},
  keywords = {uberflogen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/zhang2020.pdf}
}

@misc{zotero-268,
  title = {(14) {{AI}} in {{Podcasting}}: {{Transforming Content Creation}} and {{Listener Experiences}} {\textbar} {{LinkedIn}}},
  urldate = {2023-12-19},
  howpublished = {https://www.linkedin.com/pulse/ai-podcasting-transforming-content-creation-listener-experiences/},
  file = {/Users/br/Zotero/storage/ZR9792E8/ai-podcasting-transforming-content-creation-listener-experiences.html}
}

@techreport{zotero-393,
  title = {Interoperable {{Master Format}} --- {{Core Constraints}}},
  institution = {IEEE},
  doi = {10.5594/SMPTE.ST2067-2.2020},
  urldate = {2024-03-08},
  isbn = {9781683032113},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/zotero-393.pdf}
}

@misc{zouhar2023,
  title = {A {{Formal Perspective}} on {{Byte-Pair Encoding}}},
  author = {Zouhar, Vil{\'e}m and Meister, Clara and Gastaldi, Juan Luis and Du, Li and Vieira, Tim and Sachan, Mrinmaya and Cotterell, Ryan},
  year = {2023},
  month = jun,
  number = {arXiv:2306.16837},
  eprint = {2306.16837},
  primaryclass = {cs, math},
  publisher = {arXiv},
  urldate = {2024-03-11},
  abstract = {Byte-Pair Encoding (BPE) is a popular algorithm used for tokenizing data in NLP, despite being devised initially as a compression method. BPE appears to be a greedy algorithm at face value, but the underlying optimization problem that BPE seeks to solve has not yet been laid down. We formalize BPE as a combinatorial optimization problem. Via submodular functions, we prove that the iterative greedy version is a \${\textbackslash}frac\{1\}\{\{{\textbackslash}sigma({\textbackslash}boldsymbol\{{\textbackslash}mu\}\^{}{\textbackslash}star)\}\}(1-e\^{}\{-\{{\textbackslash}sigma({\textbackslash}boldsymbol\{{\textbackslash}mu\}\^{}{\textbackslash}star)\}\})\$-approximation of an optimal merge sequence, where \$\{{\textbackslash}sigma({\textbackslash}boldsymbol\{{\textbackslash}mu\}\^{}{\textbackslash}star)\}\$ is the total backward curvature with respect to the optimal merge sequence \${\textbackslash}boldsymbol\{{\textbackslash}mu\}\^{}{\textbackslash}star\$. Empirically the lower bound of the approximation is \${\textbackslash}approx 0.37\$. We provide a faster implementation of BPE which improves the runtime complexity from \${\textbackslash}mathcal\{O\}{\textbackslash}left(N M{\textbackslash}right)\$ to \${\textbackslash}mathcal\{O\}{\textbackslash}left(N {\textbackslash}log M{\textbackslash}right)\$, where \$N\$ is the sequence length and \$M\$ is the merge count. Finally, we optimize the brute-force algorithm for optimal BPE using memoization.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Mathematics - Optimization and Control},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/zouhar2023.pdf;/Users/br/Zotero/storage/IKHUQGNF/2306.html}
}
