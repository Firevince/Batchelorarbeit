@misc{2021,
  title = {Free {{German Dictionary}}},
  year = {2021},
  month = oct,
  journal = {SourceForge},
  urldate = {2024-03-14},
  abstract = {Download Free German Dictionary for free. A German word list for GNU Aspell. A free word list of contemporary German, for spell-checking and other purposes. Please note that this is NOT a bilingual English-German dictionary!},
  howpublished = {https://sourceforge.net/projects/germandict/},
  langid = {english},
  file = {/Users/br/Zotero/storage/4UPU8WKW/germandict.html}
}

@misc{2024,
  title = {Models - {{Hugging Face}}},
  year = {2024},
  month = feb,
  urldate = {2024-03-14},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/models},
  file = {/Users/br/Zotero/storage/PT7WNRLF/models.html}
}

@misc{2024a,
  title = {{ARD Mediathek erreicht t{\"a}glich rund 2,3 Millionen Menschen}},
  year = {2024},
  month = mar,
  urldate = {2024-03-14},
  abstract = {H{\"o}rspiel "YUME\_Leaks" mit Sophia-Lin Schirmer (alias sosopinkypie) startet in der ARD Audiothek},
  howpublished = {https://www.swr.de/un ternehmen/kommunikation/pressemeldungen/jahrespressegespraech-2024-ard-mediat hek-ard-audiothek-100.html},
  langid = {ngerman},
  file = {/Users/br/Zotero/storage/GY3R3CGA/jahrespressegespraech-2024-ard-mediathek-ard-audiothek-100.html}
}

@misc{2024b,
  title = {{Journalistische Grunds{\"a}tze}},
  year = {2024},
  month = feb,
  journal = {Die ARD},
  urldate = {2024-03-14},
  abstract = {Die Sender der ARD berichten t{\"a}glich {\"u}ber Deutschland und die Welt. Gleichzeitig sind wir in allen Regionen Deutschlands verwurzelt. Jeder Sender der ARD arbeitet unabh{\"a}ngig. Alle teilen aber gemeinsame journalistische Grunds{\"a}tze.},
  howpublished = {https://www.ard.de/die-ard/Gemeinsame-journalistische-Grundsaetze-der-ARD-100},
  langid = {ngerman},
  file = {/Users/br/Zotero/storage/IF4U4NH8/Gemeinsame-journalistische-Grundsaetze-der-ARD-100.html}
}

@misc{2024c,
  title = {{{SYSTRAN}}/Faster-Whisper},
  year = {2024},
  month = mar,
  urldate = {2024-03-14},
  abstract = {Faster Whisper transcription with CTranslate2},
  copyright = {MIT},
  howpublished = {SYSTRAN},
  keywords = {deep-learning,inference,openai,quantization,speech-recognition,speech-to-text,transformer,whisper}
}

@misc{2024d,
  title = {Our Next-Generation Model: {{Gemini}} 1.5},
  shorttitle = {Our Next-Generation Model},
  year = {2024},
  month = feb,
  journal = {Google},
  urldate = {2024-03-15},
  abstract = {Gemini 1.5 delivers dramatically enhanced performance, with a breakthrough in long{\textbackslash}u002Dcontext understanding across modalities.},
  howpublished = {https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024/},
  langid = {american},
  file = {/Users/br/Zotero/storage/JTJT333T/google-gemini-next-generation-model-february-2024.html}
}

@misc{ard,
  title = {{Online-Nutzung}},
  author = {ARD},
  journal = {Die ARD},
  urldate = {2023-12-20},
  abstract = {Die ARD weist monatliche Nutzungszahlen f{\"u}r ihre Gemeinschaftsangebote ARD-Audiothek, ARD-Mediathek, KiKA, Sportschau und Tagesschau sowie die Landesrundfunkanstalten aus.},
  howpublished = {https://www.ard.de/die-ard/Onlinenutzung-100},
  langid = {ngerman},
  file = {/Users/br/Zotero/storage/P6FHM7LS/Onlinenutzung-100.html}
}

@misc{banks2024,
  title = {Gemma: {{Introducing}} New State-of-the-Art Open Models},
  shorttitle = {Gemma},
  author = {Banks, jeanine and {warkentin}, tris},
  year = {2024},
  month = feb,
  journal = {Google},
  urldate = {2024-03-14},
  abstract = {Gemma is a family of lightweight, state{\textbackslash}u002Dof{\textbackslash}u002Dthe art open models built from the same research and technology used to create the Gemini models.},
  howpublished = {https://blog.google/technology/developers/gemma-open-models/},
  langid = {american},
  file = {/Users/br/Zotero/storage/NTD7X3NC/gemma-open-models.html}
}

@article{barthel,
  title = {Measuring {{News Consumption}} in a {{Digital Era}}},
  author = {Barthel, Michael and Mitchell, Amy and {Asare-Marfo}, Dorene and Kennedy, Courtney},
  langid = {english},
  keywords = {noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/barthel.pdf}
}

@article{biron2021,
  title = {Automatic Detection of Prosodic Boundaries in Spontaneous Speech},
  author = {Biron, Tirza and Baum, Daniel and Freche, Dominik and Matalon, Nadav and Ehrmann, Netanel and Weinreb, Eyal and Biron, David and Moses, Elisha},
  year = {2021},
  month = may,
  journal = {PLOS ONE},
  volume = {16},
  number = {5},
  pages = {e0250969},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0250969},
  urldate = {2024-03-11},
  abstract = {Automatic speech recognition (ASR) and natural language processing (NLP) are expected to benefit from an effective, simple, and reliable method to automatically parse conversational speech. The ability to parse conversational speech depends crucially on the ability to identify boundaries between prosodic phrases. This is done naturally by the human ear, yet has proved surprisingly difficult to achieve reliably and simply in an automatic manner. Efforts to date have focused on detecting phrase boundaries using a variety of linguistic and acoustic cues. We propose a method which does not require model training and utilizes two prosodic cues that are based on ASR output. Boundaries are identified using discontinuities in speech rate (pre-boundary lengthening and phrase-initial acceleration) and silent pauses. The resulting phrases preserve syntactic validity, exhibit pitch reset, and compare well with manual tagging of prosodic boundaries. Collectively, our findings support the notion of prosodic phrases that represent coherent patterns across textual and acoustic parameters.},
  langid = {english},
  keywords = {Acoustic signals,Acoustics,Language,Speech,Speech signal processing,Syllables,Syntax,Verbal communication},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/biron2021.pdf}
}

@misc{blueteamai,
  title = {Open-{{Source Vector Database Benchmarking}}},
  author = {Blueteam Ai},
  urldate = {2024-02-23},
  howpublished = {https://marketing.fmops.ai/blog/vector-benchmarking/},
  file = {/Users/br/Zotero/storage/CD2NWQAD/vector-benchmarking.html}
}

@inproceedings{brown2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  volume = {33},
  pages = {1877--1901},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-03-15},
  abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting.  For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model.  GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/brown2020.pdf}
}

@misc{chartable,
  title = {Hacker {{News Recap Podcast}} - {{Data}} and {{Chart Rankings}}},
  author = {{chartable}},
  urldate = {2024-02-21},
  abstract = {View data about Hacker News Recap on Chartable. See historical chart positions, all episodes, and more.},
  howpublished = {https://chartable.com/podcasts/hacker-news-recap/charts},
  file = {/Users/br/Zotero/storage/PH5GLPHZ/charts.html}
}

@misc{chatgpt,
  title = {{{ChatGPT}}},
  author = {{chatgpt}},
  urldate = {2024-03-14},
  abstract = {A conversational AI system that listens, learns, and challenges},
  howpublished = {https://chat.openai.com},
  langid = {american},
  file = {/Users/br/Zotero/storage/6A2J2LQE/chat.openai.com.html}
}

@misc{chen2017,
  title = {Reading {{Wikipedia}} to {{Answer Open-Domain Questions}}},
  author = {Chen, Danqi and Fisch, Adam and Weston, Jason and Bordes, Antoine},
  year = {2017},
  month = apr,
  number = {arXiv:1704.00051},
  eprint = {1704.00051},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-30},
  abstract = {This paper proposes to tackle open- domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/chen2017.pdf;/Users/br/Zotero/storage/7EIFC6Y4/1704.html}
}

@article{chen2018,
  title = {Visual Exploration and Comparison of Word Embeddings},
  author = {Chen, Juntian and Tao, Yubo and Lin, Hai},
  year = {2018},
  journal = {Journal of Visual Languages \& Computing},
  volume = {48},
  pages = {178--186},
  publisher = {Elsevier},
  urldate = {2024-03-14},
  file = {/Users/br/Zotero/storage/VSNWK48N/S1045926X18301241.html}
}

@inproceedings{cho2014,
  title = {On the {{Properties}} of {{Neural Machine Translation}}: {{Encoder}}--{{Decoder Approaches}}},
  shorttitle = {On the {{Properties}} of {{Neural Machine Translation}}},
  booktitle = {Proceedings of {{SSST-8}}, {{Eighth Workshop}} on {{Syntax}}, {{Semantics}} and {{Structure}} in {{Statistical Translation}}},
  author = {Cho, Kyunghyun and Van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  year = {2014},
  pages = {103--111},
  publisher = {Association for Computational Linguistics},
  address = {Doha, Qatar},
  doi = {10.3115/v1/W14-4012},
  urldate = {2024-03-14},
  abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
  langid = {english},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/cho2014.pdf}
}

@misc{choi2018,
  title = {{{QuAC}} : {{Question Answering}} in {{Context}}},
  shorttitle = {{{QuAC}}},
  author = {Choi, Eunsol and He, He and Iyyer, Mohit and Yatskar, Mark and Yih, Wen-tau and Choi, Yejin and Liang, Percy and Zettlemoyer, Luke},
  year = {2018},
  month = aug,
  number = {arXiv:1808.07036},
  eprint = {1808.07036},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-08},
  abstract = {We present QuAC, a dataset for Question Answering in Context that contains 14K information-seeking QA dialogs (100K questions in total). The dialogs involve two crowd workers: (1) a student who poses a sequence of freeform questions to learn as much as possible about a hidden Wikipedia text, and (2) a teacher who answers the questions by providing short excerpts from the text. QuAC introduces challenges not found in existing machine comprehension datasets: its questions are often more open-ended, unanswerable, or only meaningful within the dialog context, as we show in a detailed qualitative evaluation. We also report results for a number of reference models, including a recently state-of-the-art reading comprehension architecture extended to model dialog context. Our best model underperforms humans by 20 F1, suggesting that there is significant room for future work on this data. Dataset, baseline, and leaderboard available at http://quac.ai.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/choi2018.pdf;/Users/br/Zotero/storage/7Y37KALC/1808.html}
}

@book{chowdhury2010,
  title = {Introduction to {{Modern Information Retrieval}}},
  author = {Chowdhury, Gobinda G.},
  year = {2010},
  publisher = {Facet Publishing},
  abstract = {This textbook, for students of library and information studies undertaking courses in information retrieval, information organization, information use and knowledge-based systems, explains the theory, techniques and tools of traditional approaches to the organization and processing of information.},
  googlebooks = {cN4qDgAAQBAJ},
  isbn = {978-1-85604-694-7},
  langid = {english},
  keywords = {Computers / System Administration / Storage & Retrieval,Language Arts & Disciplines / Library & Information Science / General}
}

@misc{chroma,
  title = {Embeddings {\textbar} {{Chroma}}},
  author = {{chroma}},
  urldate = {2024-03-11},
  abstract = {Embeddings are the A.I-native way to represent any kind of data, making them the perfect fit for working with all kinds of A.I-powered tools and algorithms. They can represent text, images, and soon audio and video. There are many options for creating embeddings, whether locally using an installed library, or by calling an API.},
  howpublished = {https://www.trychroma.com/embeddings},
  langid = {english},
  file = {/Users/br/Zotero/storage/PH3U9GQE/embeddings.html}
}

@misc{clark2020,
  title = {{{ELECTRA}}: {{Pre-training Text Encoders}} as {{Discriminators Rather Than Generators}}},
  shorttitle = {{{ELECTRA}}},
  author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
  year = {2020},
  month = mar,
  number = {arXiv:2003.10555},
  eprint = {2003.10555},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-08},
  abstract = {Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/clark2020.pdf;/Users/br/Zotero/storage/KQEAMJR3/2003.html}
}

@misc{claude,
  title = {Claude},
  author = {{claude}},
  journal = {Anthropic},
  urldate = {2024-03-14},
  abstract = {Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.},
  howpublished = {https://www.anthropic.com/claude},
  langid = {english},
  file = {/Users/br/Zotero/storage/BWHR4W9Y/claude.html}
}

@misc{dafontouracosta1996,
  title = {Fundamentals of Neural Networks: {{By Laurene Fausett}}. {{Prentice-Hall}}, 1994, Pp. 461, {{ISBN}} 0-13-334186-0},
  shorttitle = {Fundamentals of Neural Networks},
  author = {{da Fontoura Costa}, Luciano and Travieso, Gonzalo},
  year = {1996},
  publisher = {Elsevier}
}

@misc{dai2023,
  title = {{{SAMAug}}: {{Point Prompt Augmentation}} for {{Segment Anything Model}}},
  shorttitle = {{{SAMAug}}},
  author = {Dai, Haixing and Ma, Chong and Liu, Zhengliang and Li, Yiwei and Shu, Peng and Wei, Xiaozheng and Zhao, Lin and Wu, Zihao and Zeng, Fang and Zhu, Dajiang and Liu, Wei and Li, Quanzheng and Liu, Tianming and Li, Xiang},
  year = {2023},
  month = oct,
  number = {arXiv:2307.01187},
  eprint = {2307.01187},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-14},
  abstract = {This paper introduces SAMAug, a novel visual point augmentation method for the Segment Anything Model (SAM) that enhances interactive image segmentation performance. SAMAug generates augmented point prompts to provide more information about the user's intention to SAM. Starting with an initial point prompt, SAM produces an initial mask, which is then fed into our proposed SAMAug to generate augmented point prompts. By incorporating these extra points, SAM can generate augmented segmentation masks based on both the augmented point prompts and the initial prompt, resulting in improved segmentation performance. We conducted evaluations using four different point augmentation strategies: random sampling, sampling based on maximum difference entropy, maximum distance, and saliency. Experiment results on the COCO, Fundus, COVID QUEx, and ISIC2018 datasets show that SAMAug can boost SAM's segmentation results, especially using the maximum distance and saliency. SAMAug demonstrates the potential of visual prompt augmentation for computer vision. Codes of SAMAug are available at github.com/yhydhx/SAMAug},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/dai2023.pdf;/Users/br/Zotero/storage/BJ5YPUFK/2307.html}
}

@misc{devlin2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = may,
  number = {arXiv:1810.04805},
  eprint = {1810.04805},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-21},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Gelesen,Obsidian Notes},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/devlin2019.pdf;/Users/br/Zotero/storage/WC9M8QI2/1810.html}
}

@inproceedings{dey2017,
  title = {Lexical {{TF-IDF}}: {{An}} n-Gram {{Feature Space}} for~{{Cross-Domain Classification}} of {{Sentiment~Reviews}}},
  shorttitle = {Lexical {{TF-IDF}}},
  booktitle = {Pattern {{Recognition}} and {{Machine Intelligence}}},
  author = {Dey, Atanu and Jenamani, Mamata and Thakkar, Jitesh J.},
  editor = {Shankar, B. Uma and Ghosh, Kuntal and Mandal, Deba Prasad and Ray, Shubhra Sankar and Zhang, David and Pal, Sankar K.},
  year = {2017},
  pages = {380--386},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-69900-4_48},
  abstract = {Feature extraction and selection is a vital step in sentiment classification using machine learning approach. Existing methods use only TF-IDF rating to represent either unigram or n-gram feature vectors. Some approaches leverage upon the use of existing sentiment dictionaries and use the score of a unigram sentiment word as the feature vector and ignore TF-IDF rating. In this work, we construct n-gram sentiment features by extracting the sentiment words and their intensifiers or negations from a review. Then the score of an n-gram constructed from lexicon of semantic unigram and its intensifier or negation is multiplied to TF-IDF rating to determine the feature score. We experiment with two benchmark data sets for sentiment classification using Support Vector Machine and Maximum Entropy method with cross domain validation by considering training and testing data from two different sets and obtain a substantial improvement in terms of various performance measures compared to existing methods. Cross-domain validation ensures proposed method can be applied for sentiment classification of data sets where example patterns are not available, which typically is the case with commercial data sets.},
  isbn = {978-3-319-69900-4},
  langid = {english},
  keywords = {Cross-domain,Lexical TFIDF,Maximum Entropy (ME),n-gram,Sentiment analysis,Support Vector Machine (SVM)},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/dey2017.pdf}
}

@misc{du2017,
  title = {Learning to {{Ask}}: {{Neural Question Generation}} for {{Reading Comprehension}}},
  shorttitle = {Learning to {{Ask}}},
  author = {Du, Xinya and Shao, Junru and Cardie, Claire},
  year = {2017},
  month = apr,
  number = {arXiv:1705.00106},
  eprint = {1705.00106},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-08},
  abstract = {We study automatic question generation for sentences from text passages in reading comprehension. We introduce an attention-based sequence learning model for the task and investigate the effect of encoding sentence- vs. paragraph-level information. In contrast to all previous work, our model does not rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead trainable end-to-end via sequence-to-sequence learning. Automatic evaluation results show that our system significantly outperforms the state-of-the-art rule-based system. In human evaluations, questions generated by our system are also rated as being more natural (i.e., grammaticality, fluency) and as more difficult to answer (in terms of syntactic and lexical divergence from the original text and reasoning needed to answer).},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/du2017.pdf;/Users/br/Zotero/storage/N4IELHG4/1705.html}
}

@misc{erdem2024,
  title = {Muhammed/Mini-Player},
  author = {Erdem, Muhammed},
  year = {2024},
  month = mar,
  urldate = {2024-03-14},
  keywords = {mini-player,music,music-player,player,vue,vue-transition}
}

@misc{eurovox2024,
  title = {{{EuroVOX}}},
  author = {{eurovox}},
  year = {2024},
  month = jan,
  urldate = {2024-03-12},
  abstract = {EuroVOX is an open toolbox to reduce the cost and complexity of transcription and translation.},
  howpublished = {https://tech.ebu.ch/eurovox},
  langid = {english},
  file = {/Users/br/Zotero/storage/EIWTQZ7B/eurovox.html}
}

@misc{faster-whisper2024,
  title = {{{SYSTRAN}}/Faster-Whisper},
  author = {{faster-whisper}},
  year = {2024},
  month = mar,
  urldate = {2024-03-10},
  abstract = {Faster Whisper transcription with CTranslate2},
  copyright = {MIT},
  howpublished = {SYSTRAN},
  keywords = {deep-learning,inference,openai,quantization,speech-recognition,speech-to-text,transformer,whisper}
}

@misc{ffmpeg,
  title = {{{FFmpeg}}},
  author = {FFmpeg},
  urldate = {2024-03-08},
  howpublished = {https://ffmpeg.org/},
  file = {/Users/br/Zotero/storage/YJ79LMRY/ffmpeg.org.html}
}

@inproceedings{formal2021,
  title = {{{SPLADE}}: {{Sparse Lexical}} and {{Expansion Model}} for {{First Stage Ranking}}},
  shorttitle = {{{SPLADE}}},
  booktitle = {Proceedings of the 44th {{International ACM SIGIR Conference}} on {{Research}} and {{Development}} in {{Information Retrieval}}},
  author = {Formal, Thibault and Piwowarski, Benjamin and Clinchant, St{\'e}phane},
  year = {2021},
  month = jul,
  pages = {2288--2292},
  publisher = {ACM},
  address = {Virtual Event Canada},
  doi = {10.1145/3404835.3463098},
  urldate = {2024-02-12},
  isbn = {978-1-4503-8037-9},
  langid = {english},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/formal2021.pdf}
}

@article{forsyth1996,
  title = {Feature-Finding for Text Classification},
  author = {Forsyth, Richard S. and Holmes, David I.},
  year = {1996},
  journal = {Literary and Linguistic Computing},
  volume = {11},
  number = {4},
  pages = {163--174},
  publisher = {Oxford University Press},
  urldate = {2024-03-14},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/forsyth1996.pdf}
}

@misc{fowler,
  title = {Microservices},
  author = {{fowler}, martin},
  journal = {martinfowler.com},
  urldate = {2024-03-14},
  abstract = {Defining the microservices architectural style by describing their nine common characteristics},
  howpublished = {https://martinfowler.com/articles/microservices.html},
  file = {/Users/br/Zotero/storage/C7NG6W83/microservices.html}
}

@book{freiling2019,
  title = {{Entrepreneurship: Gr{\"u}ndung und Skalierung von Startups}},
  shorttitle = {{Entrepreneurship}},
  author = {Freiling, J{\"o}rg and Harima, Jan},
  year = {2019},
  publisher = {Springer Fachmedien Wiesbaden},
  address = {Wiesbaden},
  doi = {10.1007/978-3-658-26117-7},
  urldate = {2023-11-20},
  isbn = {978-3-658-26116-0 978-3-658-26117-7},
  langid = {ngerman},
  keywords = {LeanStartup,noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/freiling2019.pdf;/Users/br/My Drive/Vincent' Vault/Sources/PDFs/freiling22.pdf}
}

@misc{gehring2017,
  title = {Convolutional {{Sequence}} to {{Sequence Learning}}},
  author = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N.},
  year = {2017},
  month = jul,
  number = {arXiv:1705.03122},
  eprint = {1705.03122},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-22},
  abstract = {The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/gehring2017.pdf;/Users/br/Zotero/storage/KGJHIXAE/1705.html}
}

@misc{gemini,
  title = {{Gemini -- chatten und inspirieren lassen}},
  author = {{gemini}},
  journal = {Gemini},
  urldate = {2024-03-14},
  abstract = {Bard hei{\ss}t jetzt Gemini Google~AI kann dich beim Schreiben, bei der Reiseplanung oder beim Lernen unterst{\"u}tzen.},
  howpublished = {https://gemini.google.com},
  langid = {ngerman},
  file = {/Users/br/Zotero/storage/9N4NV4ET/gemini.google.com.html}
}

@misc{gotting2023,
  title = {Number of Podcast Listeners Worldwide 2024},
  author = {G{\"o}tting, Marie Charlotte},
  year = {2023},
  journal = {Statista},
  urldate = {2023-12-19},
  abstract = {According to a study from July 2021 on global podcast consumption, the number of podcast listeners worldwide has steadily increased and is predicted to rise even further.},
  howpublished = {https://www.statista.com/statistics/1291360/podcast-listeners-worldwide/},
  langid = {english},
  file = {/Users/br/Zotero/storage/KDEFHKPN/podcast-listeners-worldwide.html}
}

@misc{gris2023,
  title = {Evaluating {{OpenAI}}'s {{Whisper ASR}} for {{Punctuation Prediction}} and {{Topic Modeling}} of Life Histories of the {{Museum}} of the {{Person}}},
  author = {Gris, Lucas Rafael Stefanel and Marcacini, Ricardo and Junior, Arnaldo Candido and Casanova, Edresson and Soares, Anderson and Alu{\'i}sio, Sandra Maria},
  year = {2023},
  month = may,
  number = {arXiv:2305.14580},
  eprint = {2305.14580},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-08},
  abstract = {Automatic speech recognition (ASR) systems play a key role in applications involving human-machine interactions. Despite their importance, ASR models for the Portuguese language proposed in the last decade have limitations in relation to the correct identification of punctuation marks in automatic transcriptions, which hinder the use of transcriptions by other systems, models, and even by humans. However, recently Whisper ASR was proposed by OpenAI, a general-purpose speech recognition model that has generated great expectations in dealing with such limitations. This chapter presents the first study on the performance of Whisper for punctuation prediction in the Portuguese language. We present an experimental evaluation considering both theoretical aspects involving pausing points (comma) and complete ideas (exclamation, question, and fullstop), as well as practical aspects involving transcript-based topic modeling - an application dependent on punctuation marks for promising performance. We analyzed experimental results from videos of Museum of the Person, a virtual museum that aims to tell and preserve people's life histories, thus discussing the pros and cons of Whisper in a real-world scenario. Although our experiments indicate that Whisper achieves state-of-the-art results, we conclude that some punctuation marks require improvements, such as exclamation, semicolon and colon.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/gris2023.pdf;/Users/br/Zotero/storage/NC6IENYQ/2305.html}
}

@misc{hackl2023,
  title = {Is {{GPT-4}} a Reliable Rater? {{Evaluating Consistency}} in {{GPT-4 Text Ratings}}},
  shorttitle = {Is {{GPT-4}} a Reliable Rater?},
  author = {Hackl, Veronika and M{\"u}ller, Alexandra Elena and Granitzer, Michael and Sailer, Maximilian},
  year = {2023},
  month = aug,
  number = {arXiv:2308.02575},
  eprint = {2308.02575},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-13},
  abstract = {This study investigates the consistency of feedback ratings generated by OpenAI's GPT-4, a state-of-the-art artificial intelligence language model, across multiple iterations, time spans and stylistic variations. The model rated responses to tasks within the Higher Education (HE) subject domain of macroeconomics in terms of their content and style. Statistical analysis was conducted in order to learn more about the interrater reliability, consistency of the ratings across iterations and the correlation between ratings in terms of content and style. The results revealed a high interrater reliability with ICC scores ranging between 0.94 and 0.99 for different timespans, suggesting that GPT-4 is capable of generating consistent ratings across repetitions with a clear prompt. Style and content ratings show a high correlation of 0.87. When applying a non-adequate style the average content ratings remained constant, while style ratings decreased, which indicates that the large language model (LLM) effectively distinguishes between these two criteria during evaluation. The prompt used in this study is furthermore presented and explained. Further research is necessary to assess the robustness and reliability of AI models in various use cases.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/hackl2023.pdf;/Users/br/Zotero/storage/QEZ45MIM/2308.html}
}

@book{harman1995,
  title = {Overview of the {{Third Text REtrieval Conference}} ({{TREC-3}})},
  author = {Harman, Donna K.},
  year = {1995},
  publisher = {DIANE Publishing},
  abstract = {Held in Gaithersburg, MD, August November 2-4, 1994. The conference was co-sponsored by the National Inst. of Standards and Technology (NIST) and the Advanced Research Projects Agency (ARPA) and was attended by 150 people involved in the 32 participating groups. Evaluates new technologies in text retrieval. Includes 34 papers: indexing structures, fragmentation schemes, probabilistic retrieval, latent semantic indexing, interactive document retrieval, and much more. Numerous graphs, tables and charts.},
  isbn = {978-0-7881-2945-2},
  langid = {english},
  keywords = {Computers / Document Management}
}

@article{hassija2024,
  title = {Interpreting {{Black-Box Models}}: {{A Review}} on {{Explainable Artificial Intelligence}}},
  shorttitle = {Interpreting {{Black-Box Models}}},
  author = {Hassija, Vikas and Chamola, Vinay and Mahapatra, Atmesh and Singal, Abhinandan and Goel, Divyansh and Huang, Kaizhu and Scardapane, Simone and Spinelli, Indro and Mahmud, Mufti and Hussain, Amir},
  year = {2024},
  month = jan,
  journal = {Cognitive Computation},
  volume = {16},
  number = {1},
  pages = {45--74},
  issn = {1866-9956, 1866-9964},
  doi = {10.1007/s12559-023-10179-8},
  urldate = {2024-03-09},
  abstract = {Recent years have seen a tremendous growth in Artificial Intelligence (AI)-based methodological development in a broad range of domains. In this rapidly evolving field, large number of methods are being reported using machine learning (ML) and Deep Learning (DL) models. Majority of these models are inherently complex and lacks explanations of the decision making process causing these models to be termed as 'Black-Box'. One of the major bottlenecks to adopt such models in mission-critical application domains, such as banking, e-commerce, healthcare, and public services and safety, is the difficulty in interpreting them. Due to the rapid proleferation of these AI models, explaining their learning and decision making process are getting harder which require transparency and easy predictability. Aiming to collate the current state-of-the-art in interpreting the black-box models, this study provides a comprehensive analysis of the explainable AI (XAI) models. To reduce false negative and false positive outcomes of these back-box models, finding flaws in them is still difficult and inefficient. In this paper, the development of XAI is reviewed meticulously through careful selection and analysis of the current state-of-the-art of XAI research. It also provides a comprehensive and in-depth evaluation of the XAI frameworks and their efficacy to serve as a starting point of XAI for applied and theoretical researchers. Towards the end, it highlights emerging and critical issues pertaining to XAI research to showcase major, model-specific trends for better explanation, enhanced transparency, and improved prediction accuracy.},
  langid = {english},
  file = {/Users/br/Zotero/storage/MGJL3KYS/Hassija et al. - 2024 - Interpreting Black-Box Models A Review on Explain.pdf}
}

@article{he2006,
  title = {A {{Comparison}} of 13 {{Tokenizers}} on {{MEDLINE}}},
  author = {He, Ying D and Kayaalp, Mehmet},
  year = {2006},
  publisher = {[object Object]},
  doi = {10.13140/2.1.1133.1206},
  urldate = {2024-03-13},
  langid = {english},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/he2006.pdf}
}

@misc{hendrycks2021,
  title = {{{CUAD}}: {{An Expert-Annotated NLP Dataset}} for {{Legal Contract Review}}},
  shorttitle = {{{CUAD}}},
  author = {Hendrycks, Dan and Burns, Collin and Chen, Anya and Ball, Spencer},
  year = {2021},
  month = nov,
  number = {arXiv:2103.06268},
  eprint = {2103.06268},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-30},
  abstract = {Many specialized domains remain untouched by deep learning, as large labeled datasets require expensive expert annotators. We address this bottleneck within the legal domain by introducing the Contract Understanding Atticus Dataset (CUAD), a new dataset for legal contract review. CUAD was created with dozens of legal experts from The Atticus Project and consists of over 13,000 annotations. The task is to highlight salient portions of a contract that are important for a human to review. We find that Transformer models have nascent performance, but that this performance is strongly influenced by model design and training dataset size. Despite these promising results, there is still substantial room for improvement. As one of the only large, specialized NLP benchmarks annotated by experts, CUAD can serve as a challenging research benchmark for the broader NLP community.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/hendrycks2021.pdf;/Users/br/Zotero/storage/SBRTI5BL/2103.html}
}

@article{hochreiter1997,
  title = {Long Short-Term Memory},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  journal = {Neural computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  publisher = {MIT press},
  urldate = {2024-03-13},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/hochreiter1997.pdf}
}

@article{honnibal2017,
  title = {{{spaCy}} 2: {{Natural}} Language Understanding with {{Bloom}} Embeddings, Convolutional Neural Networks and Incremental Parsing},
  shorttitle = {{{spaCy}} 2},
  author = {Honnibal, Matthew and Montani, Ines},
  year = {2017},
  journal = {To appear},
  volume = {7},
  number = {1},
  pages = {411--420}
}

@misc{hu2021,
  title = {{{LoRA}}: {{Low-Rank Adaptation}} of {{Large Language Models}}},
  shorttitle = {{{LoRA}}},
  author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and {Allen-Zhu}, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  year = {2021},
  month = oct,
  number = {arXiv:2106.09685},
  eprint = {2106.09685},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.09685},
  urldate = {2023-11-18},
  abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/hu2021.pdf;/Users/br/Zotero/storage/4RYBBQ57/2106.html}
}

@article{index2023,
  title = {{{TIOBE Index}}---{{May}} 2023},
  author = {Index, {\relax TIOBE}},
  year = {2023},
  journal = {Accessed: Jun},
  volume = {2}
}

@misc{jiang2024,
  title = {Mixtral of {{Experts}}},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Hanna, Emma Bou and Bressand, Florian and Lengyel, Gianna and Bour, Guillaume and Lample, Guillaume and Lavaud, L{\'e}lio Renard and Saulnier, Lucile and Lachaux, Marie-Anne and Stock, Pierre and Subramanian, Sandeep and Yang, Sophia and Antoniak, Szymon and Scao, Teven Le and Gervet, Th{\'e}ophile and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'e}e and Sayed, William El},
  year = {2024},
  month = jan,
  number = {arXiv:2401.04088},
  eprint = {2401.04088},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-16},
  abstract = {We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/jiang2024.pdf;/Users/br/Zotero/storage/GSKMV5FV/2401.html}
}

@misc{jones2021,
  title = {{{TREC}} 2020 {{Podcasts Track Overview}}},
  author = {Jones, Rosie and Carterette, Ben and Clifton, Ann and Eskevich, Maria and Jones, Gareth J. F. and Karlgren, Jussi and Pappu, Aasish and Reddy, Sravana and Yu, Yongze},
  year = {2021},
  month = mar,
  number = {arXiv:2103.15953},
  eprint = {2103.15953},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2103.15953},
  urldate = {2023-11-06},
  abstract = {The Podcast Track is new at the Text Retrieval Conference (TREC) in 2020. The podcast track was designed to encourage research into podcasts in the information retrieval and NLP research communities. The track consisted of two shared tasks: segment retrieval and summarization, both based on a dataset of over 100,000 podcast episodes (metadata, audio, and automatic transcripts) which was released concurrently with the track. The track generated considerable interest, attracted hundreds of new registrations to TREC and fifteen teams, mostly disjoint between search and summarization, made final submissions for assessment. Deep learning was the dominant experimental approach for both search experiments and summarization. This paper gives an overview of the tasks and the results of the participants' experiments. The track will return to TREC 2021 with the same two tasks, incorporating slight modifications in response to participant feedback.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/jones2021.pdf;/Users/br/Zotero/storage/339BCE5J/2103.html}
}

@article{kang2012,
  title = {Effects of Podcast Tours on Tourist Experiences in a National Park},
  author = {Kang, Myunghwa and Gretzel, Ulrike},
  year = {2012},
  month = apr,
  journal = {Tourism Management},
  volume = {33},
  number = {2},
  pages = {440--455},
  issn = {02615177},
  doi = {10.1016/j.tourman.2011.05.005},
  urldate = {2023-11-08},
  abstract = {This study examines the influence of podcast tours on tourist experiences. Based on theoretical accounts that human voices convey rich social information, this study proposes that podcast tours increase perceived social presence and mindfulness that lead to enhanced tourist experiences and environmental stewardship. A field experiment was conducted at a national park using MP3 players containing podcast tours based on four experimental conditions: 2 information source compositions (single vs. multiple narrator voices) {\^A} 2 narrating styles (formal vs. conversational). The results support that even if communicated through audio-only media, the human voice creates a positive social context for meaningful interaction which influences tourist experiences and stewardship. Mindfulness was also found to be an important construct affecting the quality of experiences. The findings support the usefulness of podcast tours as interpretative media.},
  langid = {english},
  keywords = {Obsidian Notes,uberflogen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/kang2012.pdf}
}

@misc{karpukhin2020,
  title = {Dense {{Passage Retrieval}} for {{Open-Domain Question Answering}}},
  author = {Karpukhin, Vladimir and O{\u g}uz, Barlas and Min, Sewon and Lewis, Patrick and Wu, Ledell and Edunov, Sergey and Chen, Danqi and Yih, Wen-tau},
  year = {2020},
  month = sep,
  number = {arXiv:2004.04906},
  eprint = {2004.04906},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-08},
  abstract = {Open-domain question answering relies on efficient passage retrieval to select candidate contexts, where traditional sparse vector space models, such as TF-IDF or BM25, are the de facto method. In this work, we show that retrieval can be practically implemented using dense representations alone, where embeddings are learned from a small number of questions and passages by a simple dual-encoder framework. When evaluated on a wide range of open-domain QA datasets, our dense retriever outperforms a strong Lucene-BM25 system largely by 9\%-19\% absolute in terms of top-20 passage retrieval accuracy, and helps our end-to-end QA system establish new state-of-the-art on multiple open-domain QA benchmarks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/karpukhin2020.pdf;/Users/br/Zotero/storage/62QDY3FJ/2004.html}
}

@inproceedings{kharis2021,
  title = {How to {{Lemmatize German Words}} with {{NLP-Spacy Lemmatizer}}?:},
  shorttitle = {How to {{Lemmatize German Words}} with {{NLP-Spacy Lemmatizer}}?},
  booktitle = {International {{Seminar}} on {{Language}}, {{Education}}, and {{Culture}} ({{ISoLEC}} 2021)},
  author = {Kharis, M. and {Kisyani} and {Suhartono} and Pairin, Udjang and {Darni}},
  year = {2021},
  address = {Malang, Indonesia},
  doi = {10.2991/assehr.k.211212.036},
  urldate = {2024-02-09},
  abstract = {Simple algorithms for the lemmatization process have been developed to recognize changes in a word as a result of grammatical processes and changes. Lemmatizer tools can analyze the types of word changes in the German language. Thus, this paper aims at investigating how the lemmatization of German words is aided by the Lemmatizer software. NLP Lemmatizer spacy, in cooperation with Python and Visual Studio Code, is utilized to find out the primary form of the word changes in German language. Based on the lemmatization analysis results, Lemmatizer SpaCy can analyze the shape of token, lemma, and PoS-tag of words in German. However, there are some errors identified during the process of finding out the word changes in German language.},
  langid = {english},
  file = {/Users/br/Zotero/storage/5IIBD4BU/Kharis et al. - 2021 - How to Lemmatize German Words with NLP-Spacy Lemma.pdf}
}

@misc{kingma2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  number = {arXiv:1412.6980},
  eprint = {1412.6980},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-22},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/kingma2017.pdf;/Users/br/Zotero/storage/IDKZQ8S4/1412.html}
}

@inproceedings{klein2017,
  title = {{{OpenNMT}}: {{Open-Source Toolkit}} for {{Neural Machine Translation}}},
  shorttitle = {{{OpenNMT}}},
  booktitle = {Proceedings of {{ACL}} 2017, {{System Demonstrations}}},
  author = {Klein, Guillaume and Kim, Yoon and Deng, Yuntian and Senellart, Jean and Rush, Alexander},
  year = {2017},
  pages = {67--72},
  publisher = {Association for Computational Linguistics},
  address = {Vancouver, Canada},
  doi = {10.18653/v1/P17-4012},
  urldate = {2024-03-07},
  langid = {english},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/klein2017.pdf}
}

@article{krettek2023,
  title = {{ChatGPT}},
  author = {Krettek, C.},
  year = {2023},
  month = mar,
  journal = {Die Unfallchirurgie},
  volume = {126},
  number = {3},
  pages = {252--254},
  issn = {2731-703X},
  doi = {10.1007/s00113-023-01296-y},
  urldate = {2024-03-14},
  langid = {ngerman},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/krettek2023.pdf}
}

@incollection{kryszkiewicz2013,
  title = {Determining {{Cosine Similarity Neighborhoods}} by {{Means}} of the {{Euclidean Distance}}},
  booktitle = {Rough {{Sets}} and {{Intelligent Systems}} - {{Professor Zdzis{\l}aw Pawlak}} in {{Memoriam}}: {{Volume}} 2},
  author = {Kryszkiewicz, Marzena},
  editor = {Skowron, Andrzej and Suraj, Zbigniew},
  year = {2013},
  pages = {323--345},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-30341-8_17},
  urldate = {2024-03-14},
  abstract = {Cosine similarity measure is often applied in the area of information retrieval, text classification, clustering, and ranking, where documents are usually represented as term frequency vectors or its variants such as tf-idf vectors. In these tasks, the most time-consuming operation is the calculation of most similar vectors or, alternatively, least dissimilar vectors. This operation has been commonly believed to be inefficient for large high-dimensional datasets. However, using the triangle inequality to determine neighborhoods based on a distance metric, offered recently, makes this operation feasible for such datasets. Although the cosine similarity measure is not a distance metric and, in particular, violates the triangle inequality, in this chapter, we present how to determine cosine similarity neighborhoods of vectors by means of the Euclidean distance applied to ({$\alpha$}\,-\,)normalized forms of these vectors and by using the triangle inequality. We address three types of sets of cosine similar vectors: all vectors, the similarity of which to a given vector is not less than an {$\varepsilon$} threshold value, and two variants of the k-nearest neighbors of a given vector.},
  isbn = {978-3-642-30341-8},
  langid = {english},
  keywords = {-neighborhood,data clustering,high-dimensional data,k-nearest neighbors,normalized vector,text clustering,the cosine similarity measure,the Euclidean distance,the triangle inequality},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/kryszkiewicz2013.pdf}
}

@misc{kumar2023,
  title = {Semantic Search with {{ElasticSearch}}},
  author = {Kumar, Selva},
  year = {2023},
  month = sep,
  journal = {Medium},
  urldate = {2024-03-14},
  abstract = {In the digital era, search engines plays a major role in retrieving data navigating the vast information available on the internet. This{\dots}},
  howpublished = {https://blog.gopenai.com/semantic-search-with-elasticsearch-1dab248d116f},
  langid = {english},
  file = {/Users/br/Zotero/storage/4A3STKPE/semantic-search-with-elasticsearch-1dab248d116f.html}
}

@inproceedings{laban2022,
  title = {{{NewsPod}}: {{Automatic}} and {{Interactive News Podcasts}}},
  shorttitle = {{{NewsPod}}},
  booktitle = {27th {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Laban, Philippe and Ye, Elicia and Korlakunta, Srujay and Canny, John and Hearst, Marti},
  year = {2022},
  month = mar,
  pages = {691--706},
  publisher = {ACM},
  address = {Helsinki Finland},
  doi = {10.1145/3490099.3511147},
  urldate = {2023-11-06},
  isbn = {978-1-4503-9144-3},
  langid = {english},
  keywords = {Gelesen,Obsidian Notes},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/laban2022.pdf}
}

@article{landauer1997,
  title = {A Solution to {{Plato}}'s Problem: {{The}} Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge.},
  shorttitle = {A Solution to {{Plato}}'s Problem},
  author = {Landauer, Thomas K. and Dumais, Susan T.},
  year = {1997},
  journal = {Psychological review},
  volume = {104},
  number = {2},
  pages = {211},
  publisher = {American Psychological Association},
  urldate = {2024-03-13},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/landauer1997.pdf}
}

@misc{lee2017,
  title = {End-to-End {{Neural Coreference Resolution}}},
  author = {Lee, Kenton and He, Luheng and Lewis, Mike and Zettlemoyer, Luke},
  year = {2017},
  month = dec,
  number = {arXiv:1707.07045},
  eprint = {1707.07045},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-14},
  abstract = {We introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or hand-engineered mention detector. The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each. The model computes span embeddings that combine context-dependent boundary representations with a head-finding attention mechanism. It is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions. Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/lee2017.pdf;/Users/br/Zotero/storage/YNWXJHG5/1707.html}
}

@misc{lemur,
  title = {Lemur {{Project Components}}: {{Indri}}},
  author = {{lemur}},
  urldate = {2024-03-09},
  howpublished = {https://www.lemurproject.org/indri.php},
  file = {/Users/br/Zotero/storage/V2T3RMF3/indri.html}
}

@misc{listennotes,
  title = {Podcast {{Stats}}: {{How}} Many Podcasts Are There?},
  shorttitle = {Podcast {{Stats}}},
  author = {{listennotes}},
  journal = {Listen Notes},
  urldate = {2024-03-02},
  abstract = {Podcast industry data through the lens of Listen Notes, the best podcast search engine and database.},
  howpublished = {https://www.listennotes.co m/podcast-stats/},
  langid = {english},
  file = {/Users/br/Zotero/storage/SHB6B2LM/podcast-stats.html}
}

@misc{liu2019,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  number = {arXiv:1907.11692},
  eprint = {1907.11692},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-21},
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/liu2019.pdf;/Users/br/Zotero/storage/AI2G9PVC/1907.html}
}

@inproceedings{lochrie2018,
  title = {Designing {{Immersive Audio Experiences}} for {{News}} and {{Information}} in the {{Internet}} of {{Things}} Using {{Text-to-Speech Objects}}},
  booktitle = {Proceedings of the 32nd {{International BCS Human Computer Interaction Conference}}},
  author = {Lochrie, Mark and {De-Neef}, Robin and Mills, John and Davenport, Jack},
  year = {2018},
  doi = {10.14236/ewic/HCI2018.90},
  urldate = {2023-11-08},
  langid = {english},
  keywords = {noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/lochrie2018.pdf}
}

@misc{malkov2020,
  title = {Efficient and {{Robust Approximate Nearest Neighbor Search Using Hierarchical Navigable Small World Graphs}}},
  author = {Malkov, Yu A. and Yashunin, D.A.},
  year = {2020},
  month = apr,
  urldate = {2024-03-13},
  file = {/Users/br/Zotero/storage/ANXW8CC9/stamp.html}
}

@article{maroni2020,
  title = {{K{\"U}NSTLICHE INTELLIGENZ IM PRODUKTIVEN EINSATZ F{\"U}R DIE AUTOMATISIERTE ERSCHLIESSUNG IM MULTIMEDIALEN PRODUKTIONSPROZESS}},
  author = {Maroni, Dirk and K{\"o}hler, Joachim and Fisseler, Jens and Becker, Sven},
  year = {2020},
  langid = {ngerman},
  keywords = {noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/maroni2020.pdf}
}

@misc{mikolov2013,
  title = {Efficient {{Estimation}} of {{Word Representations}} in {{Vector Space}}},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  year = {2013},
  month = sep,
  number = {arXiv:1301.3781},
  eprint = {1301.3781},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-13},
  abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/mikolov2013.pdf;/Users/br/Zotero/storage/A6EEE2U8/1301.html}
}

@misc{mindline-media2023,
  title = {Bericht-{{OAM}}\_2023.Pdf},
  author = {{mindline-media}},
  year = {2023},
  urldate = {2024-02-04},
  file = {/Users/br/Zotero/storage/RDVZIUUX/Bericht-OAM_2023.pdf}
}

@misc{minilm2024,
  title = {Sentence-Transformers/All-{{MiniLM-L6-v2}} {$\cdot$} {{Hugging Face}}},
  shorttitle = {All-{{MiniLM}}},
  author = {{miniLM}},
  year = {2024},
  month = mar,
  journal = {Huggingface},
  urldate = {2024-03-13},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingf ace.co/sentence-transformers/all-MiniLM-L6-v2},
  file = {/Users/br/Zotero/storage/42KKAZDA/all-MiniLM-L6-v2.html}
}

@inproceedings{moldovan2000,
  title = {The {{Structure}} and {{Performance}} of an {{Open-Domain Question Answering System}}},
  booktitle = {Proceedings of the 38th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Moldovan, Dan and Harabagiu, Sanda and Pasca, Marius and Mihalcea, Rada and Girju, Roxana and Goodrum, Richard and Rus, Vasile},
  year = {2000},
  month = oct,
  pages = {563--570},
  publisher = {Association for Computational Linguistics},
  address = {Hong Kong},
  doi = {10.3115/1075218.1075289},
  urldate = {2023-11-08},
  keywords = {noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/moldovan2000.pdf}
}

@misc{mteb,
  title = {{{MTEB Leaderboard}} - a {{Hugging Face Space}} by Mteb},
  author = {{mteb}},
  urldate = {2024-03-11},
  abstract = {Discover amazing ML apps made by the community},
  howpublished = {https://huggingface.co/spa ces/mteb/leaderboard},
  file = {/Users/br/Zotero/storage/WVLA5S5Q/leaderboard.html}
}

@misc{muennighoff2023,
  title = {{{MTEB}}: {{Massive Text Embedding Benchmark}}},
  shorttitle = {{{MTEB}}},
  author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Lo{\"i}c and Reimers, Nils},
  year = {2023},
  month = mar,
  number = {arXiv:2210.07316},
  eprint = {2210.07316},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-12},
  abstract = {Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity (STS) can be equally well applied to other tasks like clustering or reranking. This makes progress in the field difficult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark (MTEB). MTEB spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on MTEB, we establish the most comprehensive benchmark of text embeddings to date. We find that no particular text embedding method dominates across all tasks. This suggests that the field has yet to converge on a universal text embedding method and scale it up sufficiently to provide state-of-the-art results on all embedding tasks. MTEB comes with open-source code and a public leaderboard at https://github.com/embeddings-benchmark/mteb.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  file = {/Users/br/Zotero/storage/XN992GU2/Muennighoff et al. - 2023 - MTEB Massive Text Embedding Benchmark.pdf;/Users/br/Zotero/storage/YGG5N42T/2210.html}
}

@misc{muennighoff2023a,
  title = {Mteb/Leaderboard {$\cdot$} {{Restart}} the Space for New Models},
  author = {Muennighoff},
  year = {2023},
  month = sep,
  urldate = {2024-03-05},
  abstract = {Hi, @ Muennighoff Thanks for the great work! I submitted two new Chinese Text Embedding models: "stella-base-zh" and "stella-large-zh" , can you help restart this space?},
  howpublished = {https://huggingfa ce.co/spaces/mteb/leaderboard/discussions/28},
  file = {/Users/br/Zotero/storage/ZN22FB4K/28.html}
}

@inproceedings{naismith2023,
  title = {Automated Evaluation of Written Discourse Coherence Using {{GPT-4}}},
  booktitle = {Proceedings of the 18th {{Workshop}} on {{Innovative Use}} of {{NLP}} for {{Building Educational Applications}} ({{BEA}} 2023)},
  author = {Naismith, Ben and Mulcaire, Phoebe and Burstein, Jill},
  editor = {Kochmar, Ekaterina and Burstein, Jill and Horbach, Andrea and {Laarmann-Quante}, Ronja and Madnani, Nitin and Tack, Ana{\"i}s and Yaneva, Victoria and Yuan, Zheng and Zesch, Torsten},
  year = {2023},
  month = jul,
  pages = {394--403},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  doi = {10.18653/v1/2023.bea-1.32},
  urldate = {2024-03-11},
  abstract = {The popularization of large language models (LLMs) such as OpenAI's GPT-3 and GPT-4 have led to numerous innovations in the field of AI in education. With respect to automated writing evaluation (AWE), LLMs have reduced challenges associated with assessing writing quality characteristics that are difficult to identify automatically, such as discourse coherence. In addition, LLMs can provide rationales for their evaluations (ratings) which increases score interpretability and transparency. This paper investigates one approach to producing ratings by training GPT-4 to assess discourse coherence in a manner consistent with expert human raters. The findings of the study suggest that GPT-4 has strong potential to produce discourse coherence ratings that are comparable to human ratings, accompanied by clear rationales. Furthermore, the GPT-4 ratings outperform traditional NLP coherence metrics with respect to agreement with human ratings. These results have implications for advancing AWE technology for learning and assessment.},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/naismith2023.pdf}
}

@article{newman2022,
  title = {Reuters {{Institute Digital News Report}} 2022},
  author = {Newman, Nic and Fletcher, Richard and Robertson, Craig T and Eddy, Kirsten and Nielsen, Rasmus Kleis},
  year = {2022},
  langid = {english},
  file = {/Users/br/Zotero/storage/WE8MUI4S/Newman et al. - 2022 - Reuters Institute Digital News Report 2022.pdf}
}

@misc{ng2023,
  title = {Sequenz {{Models}}},
  author = {Ng, Andrew},
  year = {2023},
  address = {Coursera},
  urldate = {2023-10-20}
}

@article{nilsson,
  title = {{{GPT-4}} as an {{Automatic Grader}}},
  author = {Nilsson, Filippa and Tuvstedt, Jonatan},
  abstract = {Education is a field with a lot of time consuming tasks outside the core charge of teaching. One of these arduous tasks is grading, which can be monotonous and very time consuming. An emerging field that could potentially alleviate this is Artificial Intelligence (AI), or more specifically, Large Language Models (LLM:s), that have advanced immensely in the last year following the release of ChatGPT. This thesis investigates the accuracy of grading by GPT-4 compared to Teachers Assistants on introductory programming assignments. The work of a total of 73 students in the introductory programming courses INDA at KTH Royal Institute of Technology was examined by GPT and graded. The grading was accomplished by sending a prompt to GPT, consisting of plain text copies of the assignment, the grading criteria, the student submission and instructions on how to grade. The results were very promising, with GPT having an overall accuracy of 75\% when compared to grades by Teachers Assistants. However, it was significantly worse at correctly identifying submissions that failed compared to those that passed. The results indicated that AI could be able to grade students' work reliably in the future, if the development of LLM:s continue progressing. In the meantime, AI can be used as a grading tool for educators around the world, alleviating their workload.},
  langid = {english},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/nilsson.pdf}
}

@book{nilsson2023,
  title = {{{GPT-4}} as an {{Automatic Grader}} : {{The}} Accuracy of Grades Set by {{GPT-4}} on Introductory Programming Assignments},
  shorttitle = {{{GPT-4}} as an {{Automatic Grader}}},
  author = {Nilsson, Filippa and Tuvstedt, Jonatan},
  year = {2023},
  urldate = {2024-03-11},
  abstract = {DiVA portal is a finding tool for research publications and student theses written at the following 50 universities and research institutions.},
  langid = {english},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/nilsson2023.pdf;/Users/br/Zotero/storage/AAU9FPAG/LHNCBC-TR-2006-003 (1).pdf}
}

@misc{oord2016,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveNet}}},
  author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  year = {2016},
  month = sep,
  number = {arXiv:1609.03499},
  eprint = {1609.03499},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-08},
  abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-ofthe-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/oord2016.pdf}
}

@article{ouyang,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through a language model API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
  langid = {english},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/ouyang.pdf}
}

@article{page,
  title = {The {{PageRank Citation Ranking}}: {{Bringing Order}} to the {{Web}}},
  author = {Page, Lawrence and Brin, Sergey and Motwani, Rajeev and Winograd, Terry},
  langid = {english},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/page.pdf}
}

@article{patwardhan2023,
  title = {Transformers in the Real World: {{A}} Survey on Nlp Applications},
  shorttitle = {Transformers in the Real World},
  author = {Patwardhan, Narendra and Marrone, Stefano and Sansone, Carlo},
  year = {2023},
  journal = {Information},
  volume = {14},
  number = {4},
  pages = {242},
  publisher = {MDPI},
  urldate = {2024-03-14},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/patwardhan2023.pdf}
}

@inproceedings{pennington2014,
  title = {Glove: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {Glove},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  year = {2014},
  pages = {1532--1543},
  publisher = {Association for Computational Linguistics},
  address = {Doha, Qatar},
  doi = {10.3115/v1/D14-1162},
  urldate = {2023-12-21},
  abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75\% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.},
  langid = {english},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/pennington2014.pdf}
}

@misc{peters2018,
  title = {Deep Contextualized Word Representations},
  author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  month = mar,
  number = {arXiv:1802.05365},
  eprint = {1802.05365},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-13},
  abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/peters2018.pdf;/Users/br/Zotero/storage/JGXRWWFB/1802.html}
}

@misc{phillips2014,
  title = {How {{Diversity Makes Us Smarter}}},
  author = {Phillips, Katherine W.},
  year = {2014},
  month = oct,
  journal = {Scientific American},
  urldate = {2023-12-15},
  abstract = {Being around people who are different from us makes us more creative, more diligent and harder-working},
  howpublished = {https://www.scientificamerican.com/article/how-diversity-makes-us-smarter/},
  langid = {english},
  file = {/Users/br/Zotero/storage/TPDE2L98/how-diversity-makes-us-smarter.html}
}

@misc{pi,
  title = {Pi, Your Personal {{AI}}},
  author = {{pi}},
  urldate = {2024-03-14},
  abstract = {Hi, I'm Pi. I'm your personal AI, designed to be supportive, smart, and there for you anytime. Ask me for advice, for answers, or let's talk about whatever's on your mind.},
  howpublished = {https://pi.ai/discover},
  langid = {english},
  file = {/Users/br/Zotero/storage/DCRQRMQE/discover.html}
}

@misc{pickle,
  title = {Pickle --- {{Python}} Object Serialization},
  author = {{pickle}},
  journal = {Python documentation},
  urldate = {2024-03-11},
  abstract = {Source code: Lib/pickle.py The pickle module implements binary protocols for serializing and de-serializing a Python object structure. ``Pickling'' is the process whereby a Python object hierarchy is...},
  howpublished = {https://docs.python.org/3/library/pickle .html},
  langid = {english},
  file = {/Users/br/Zotero/storage/XQVHCTRD/pickle.html}
}

@inproceedings{pirani2022,
  title = {A {{Comparative Analysis}} of {{ARIMA}}, {{GRU}}, {{LSTM}} and {{BiLSTM}} on {{Financial Time Series Forecasting}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Distributed Computing}} and {{Electrical Circuits}} and {{Electronics}} ({{ICDCECE}})},
  author = {Pirani, Muskaan and Thakkar, Paurav and Jivrani, Pranay and Bohara, Mohammed Husain and Garg, Dweepna},
  year = {2022},
  month = apr,
  pages = {1--6},
  publisher = {IEEE},
  address = {Ballari, India},
  doi = {10.1109/ICDCECE53908.2022.9793213},
  urldate = {2024-03-06},
  abstract = {Machine learning and profound learning algorithms were one in every of the effective techniques to statistical prediction. Once it involves time series prediction, these algorithms shelled classic regression-based solutions in terms of accuracy. Long short-term memory (LSTM), one of the recurrent neural networks (RNN), has been incontestable to outperform typical prediction methods. The LSTM-based models are incorporated with further ``gates'' such that it will consider input data of longer sequences. LSTM-based models outperform Autoregressive Integrated Moving Average models attributable to these further capabilities (ARIMA). GatedRecurrent Unit (GRU) and bidirectional long short-term memory (BiLSTM) are extended versions of LSTM. The major question is that an algorithmic program would shell the other two by giving smart predictions with minimum error. Bidirectional LSTMs provide extra training because it may be a 2-way formula, thus, it'll traverse the training information double (1. Left-to-right 2. Right-to-left). GRU has one gate below the LSTM architecture. Hence, our analysis is especially centred on that algorithm outperforms the opposite two and it conjointly deals with behavioural analysis of the algorithms, their comparison and therefore the standardization of hyperparameters.},
  isbn = {978-1-66548-316-2},
  langid = {english},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/pirani2022.pdf}
}

@misc{podcastindex.org,
  title = {Stats},
  author = {{podcastindex.org}},
  journal = {Podcastindex.org},
  urldate = {2024-03-02},
  abstract = {The Podcast Index is here to preserve, protect and extend the open, independent podcasting ecosystem.},
  howpublished = {https://podcastindex.org/stats?utm\_source=podnews.net\&utm\_medium=web\&utm\_campaign=podnews.net:2022-03-10},
  langid = {english},
  file = {/Users/br/Zotero/storage/8WKGJHDI/stats.html}
}

@misc{podcastle2023,
  title = {The {{Future}} of {{Podcasting}} with {{Artificial Intelligence}}},
  author = {Podcastle},
  year = {2023},
  month = jan,
  journal = {Podcastle Blog},
  urldate = {2024-02-21},
  abstract = {AI in podcasting can help make podcasts more interactive, personalized, and engaging for listeners. Let's explore how AI is transforming the podcasting industry.},
  howpublished = {https://podcastle.ai/blog/ai-in-podcasting/},
  langid = {english},
  file = {/Users/br/Zotero/storage/E4ZGX3A6/ai-in-podcasting.html}
}

@article{radford,
  title = {Robust {{Speech Recognition}} via {{Large-Scale Weak Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
  langid = {english},
  file = {/Users/br/Zotero/storage/Y6YAD4ZI/Radford et al. - Robust Speech Recognition via Large-Scale Weak Sup.pdf}
}

@article{reddy2019,
  title = {{{CoQA}}: {{A Conversational Question Answering Challenge}}},
  shorttitle = {{{CoQA}}},
  author = {Reddy, Siva and Chen, Danqi and Manning, Christopher D.},
  year = {2019},
  month = nov,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {7},
  pages = {249--266},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00266},
  urldate = {2023-11-08},
  abstract = {Humans gather information through conversations involving a series of interconnected questions and answers. For machines to assist in information gathering, it is therefore essential to enable them to answer conversational questions. We introduce CoQA, a novel dataset for building Conversational Question Answering systems. Our dataset contains 127k questions with answers, obtained from 8k conversations about text passages from seven diverse domains. The questions are conversational, and the answers are free-form text with their corresponding evidence highlighted in the passage. We analyze CoQA in depth and show that conversational questions have challenging phenomena not present in existing reading comprehension datasets (e.g., coreference and pragmatic reasoning). We evaluate strong dialogue and reading comprehension models on CoQA. The best system obtains an F1 score of 65.4\%, which is 23.4 points behind human performance (88.8\%), indicating that there is ample room for improvement. We present CoQA as a challenge to the community at https://stanfordnlp.github. io/coqa.},
  langid = {english},
  keywords = {noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/reddy2019.pdf}
}

@misc{reimers2019,
  title = {Sentence-{{BERT}}: {{Sentence Embeddings}} Using {{Siamese BERT-Networks}}},
  shorttitle = {Sentence-{{BERT}}},
  author = {Reimers, Nils and Gurevych, Iryna},
  year = {2019},
  month = aug,
  number = {arXiv:1908.10084},
  eprint = {1908.10084},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-21},
  abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/reimers2019.pdf;/Users/br/Zotero/storage/6WCCSTA9/1908.html}
}

@misc{repodiac2023,
  title = {Repodiac/German\_compound\_splitter},
  author = {{repodiac}},
  year = {2023},
  month = jul,
  urldate = {2024-03-14},
  abstract = {Compound splitter for German language ("Komposita-Zerlegung") based on large dictionary combined with highly efficient multi-pattern string search},
  copyright = {CC-BY-4.0},
  keywords = {compounds,deutsch,german,komposita,kompositum,nlp,splitter}
}

@misc{rundfunk2024,
  type = {{indexPage}},
  title = {{Manuskripte : radioWissen}},
  shorttitle = {{Manuskripte}},
  author = {Rundfunk, Bayerischer},
  year = {2024},
  month = feb,
  urldate = {2024-03-14},
  howpublished = {https://www.br.de/radio/bayern2/ service/manuskripte/radiowissen/radiowissen-manuskripte108.html},
  langid = {ngerman},
  file = {/Users/br/Zotero/storage/VQEJ7E2E/index.html}
}

@inproceedings{sahijwani2020,
  title = {Would {{You Like}} to {{Hear}} the {{News}}?: {{Investigating Voice-Based Suggestions}} for {{Conversational News Recommendation}}},
  shorttitle = {Would {{You Like}} to {{Hear}} the {{News}}?},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Human Information Interaction}} and {{Retrieval}}},
  author = {Sahijwani, Harshita and Choi, Jason Ingyu and Agichtein, Eugene},
  year = {2020},
  month = mar,
  pages = {437--441},
  publisher = {ACM},
  address = {Vancouver BC Canada},
  doi = {10.1145/3343413.3378013},
  urldate = {2023-11-08},
  abstract = {One of the key benefits of voice-based personal assistants is the potential to proactively recommend relevant and interesting information. One of the most valuable sources of such information is the News. However, in order for the user to hear the news that is useful and relevant to them, it must be recommended in an interesting and informative way. However, to the best of our knowledge, how to present a news item for a voice-based recommendation remains an open question. In this paper, we empirically compare different ways of recommending news, or specific news items, in a voice-based conversational setting. Specifically, we study the user engagement and satisfaction with five different variants of presenting news recommendations: (1) a generic news briefing; (2) news about a specific entity relevant to the current conversation; (3) news about an entity from a past conversation; (4) news on a trending news topic; and (5) the default - a suggestion to talk about news in general. Our results show that entity-based news recommendations exhibit 29\% higher acceptance compared to briefing recommendations, and almost 100\% higher acceptance compared to recommending generic or trending news. Our investigation into the presentation of news recommendations and the resulting insights could make voice assistants more informative and engaging.},
  isbn = {978-1-4503-6892-6},
  langid = {english},
  keywords = {noch nicht gelesen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/sahijwani2020.pdf}
}

@article{shanahan2024,
  title = {Talking about {{Large Language Models}}},
  author = {Shanahan, Murray},
  year = {2024},
  month = feb,
  journal = {Communications of the ACM},
  volume = {67},
  number = {2},
  pages = {68--79},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3624724},
  urldate = {2024-03-05},
  abstract = {Interacting with a contemporary LLM-based conversational agent can create an illusion of being in the presence of a thinking creature. Yet, in their very nature, such systems are fundamentally not like us.},
  langid = {english},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/shanahan2024.pdf}
}

@inproceedings{shi2023,
  title = {Evaluating and {{Personalizing User-Perceived Quality}} of {{Text-to-Speech Voices}} for {{Delivering Mindfulness Meditation}} with {{Different Physical Embodiments}}},
  booktitle = {Proceedings of the 2023 {{ACM}}/{{IEEE International Conference}} on {{Human-Robot Interaction}}},
  author = {Shi, Zhonghao and Chen, Han and Velentza, Anna-Maria and Liu, Siqi and Dennler, Nathaniel and O'Connell, Allison and Mataric, Maja},
  year = {2023},
  month = mar,
  pages = {516--524},
  publisher = {ACM},
  address = {Stockholm Sweden},
  doi = {10.1145/3568162.3576987},
  urldate = {2023-12-20},
  isbn = {978-1-4503-9964-7},
  langid = {english},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/shi2023.pdf}
}

@misc{shum2024,
  title = {Automatic {{Prompt Augmentation}} and {{Selection}} with {{Chain-of-Thought}} from {{Labeled Data}}},
  author = {Shum, KaShun and Diao, Shizhe and Zhang, Tong},
  year = {2024},
  month = feb,
  number = {arXiv:2302.12822},
  eprint = {2302.12822},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-14},
  abstract = {Chain-of-thought (CoT) advances the reasoning abilities of large language models (LLMs) and achieves superior performance in complex reasoning tasks. However, most CoT studies rely on carefully designed human-annotated rational chains to prompt LLMs, posing challenges for real-world applications where labeled data is available without rational chains. This paper proposes a new strategy, Automate-CoT (Automatic Prompt Augmentation and Selection with Chain-of-Thought), that can bypass human engineering of CoT by automatically augmenting rational chains from a small labeled dataset, and then pruning low-quality chains to construct a candidate pool of machine-generated rationale chains based on the labels. Finally, it selects the optimal combination of several rationale chains from the pool for CoT prompting by employing a variance-reduced policy gradient strategy to estimate the significance of each example. Automate-CoT enables a quick adaptation of the CoT technique to different tasks. Experimental results demonstrate the effectiveness of our method, where competitive results are achieved on arithmetic reasoning (+2.7\%), commonsense reasoning (+3.4\%), symbolic reasoning (+3.2\%), and non-reasoning tasks (+2.5\%). The code is available at https://github.com/SHUMKASHUN/Automate-CoT.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/shum2024.pdf;/Users/br/Zotero/storage/52HWLU4V/2302.html}
}

@misc{spacy2024,
  title = {German {$\cdot$} {{spaCy Models Documentation}}},
  author = {{spacy}},
  year = {2024},
  journal = {German},
  urldate = {2024-02-17},
  abstract = {spaCy is a free open-source library for Natural Language Processing in Python. It features NER, POS tagging, dependency parsing, word vectors and more.},
  howpublished = {https://spacy.io/models/de},
  langid = {english},
  file = {/Users/br/Zotero/storage/BL54RMCR/de.html}
}

@article{sparckjones1972,
  title = {A Statistical Interpretation of Term Specificity and Its Application in Retrieval},
  author = {Sparck Jones, Karen},
  year = {1972},
  journal = {Journal of documentation},
  volume = {28},
  number = {1},
  pages = {11--21},
  publisher = {MCB UP Ltd},
  urldate = {2024-03-13}
}

@misc{statista-a,
  title = {{Reichweite der ARD-Gemeinschaftsangebote 2024}},
  author = {{statista-a}},
  journal = {Statista},
  urldate = {2024-03-10},
  abstract = {Im Januar 2024 war die ARD-Mediathek mit rund 191,5 Millionen Visits das meistaufgerufene Onlineangebot der ARD.},
  howpublished = {https://de.statista.com/statistik/daten/studie/1249938/umfrage/online-nutzung-anhand-von-aufrufzahlen-der-ard-gemeinschaftsangebote/},
  langid = {ngerman},
  file = {/Users/br/Zotero/storage/R7SMNYFW/online-nutzung-anhand-von-aufrufzahlen-der-ard-gemeinschaftsangebote.html}
}

@misc{tiktoken2024,
  title = {Openai/Tiktoken},
  author = {{tiktoken}},
  year = {2024},
  month = mar,
  urldate = {2024-03-11},
  abstract = {tiktoken is a fast BPE tokeniser for use with OpenAI's models.},
  copyright = {MIT},
  howpublished = {OpenAI}
}

@article{tong2023,
  title = {Exclusive: {{ChatGPT}} Traffic Slips Again for Third Month in a Row},
  shorttitle = {Exclusive},
  author = {Tong, Anna and Tong, Anna},
  year = {2023},
  month = sep,
  journal = {Reuters},
  urldate = {2024-03-12},
  abstract = {OpenAI's ChatGPT, the wildly popular artificial intelligence chatbot launched in November, saw monthly website visits decline for the third month in a row in August, though there are signs the decline is coming to an end, according to analytics firm Similarweb.},
  chapter = {Technology},
  langid = {english},
  file = {/Users/br/Zotero/storage/DA6EFFD9/chatgpt-traffic-slips-again-third-month-row-2023-09-07.html}
}

@misc{touvron2023,
  title = {Llama 2: {{Open Foundation}} and {{Fine-Tuned Chat Models}}},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  year = {2023},
  month = jul,
  number = {arXiv:2307.09288},
  eprint = {2307.09288},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-14},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/touvron22.pdf;/Users/br/Zotero/storage/NBJRFHJE/2307.html}
}

@misc{vaswani2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2023},
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-18},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Gelesen,Obsidian Notes},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/vaswani2023.pdf;/Users/br/Zotero/storage/UCE2RMK5/1706.html}
}

@article{wagner2023,
  title = {Dawn of the {{Transformer Era}} in {{Speech Emotion Recognition}}: {{Closing}} the {{Valence Gap}}},
  shorttitle = {Dawn of the {{Transformer Era}} in {{Speech Emotion Recognition}}},
  author = {Wagner, Johannes and Triantafyllopoulos, Andreas and Wierstorf, Hagen and Schmitt, Maximilian and Burkhardt, Felix and Eyben, Florian and Schuller, Bj{\"o}rn W.},
  year = {2023},
  month = sep,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {45},
  number = {9},
  pages = {10745--10759},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2023.3263585},
  urldate = {2023-12-20},
  abstract = {Recent advances in transformer-based architectures have shown promise in several machine learning tasks. In the audio domain, such architectures have been successfully utilised in the field of speech emotion recognition (SER). However, existing works have not evaluated the influence of model size and pre-training data on downstream performance, and have shown limited attention to generalisation, robustness, fairness, and efficiency. The present contribution conducts a thorough analysis of these aspects on several pre-trained variants of wav2vec 2.0 and HuBERT that we fine-tuned on the dimensions arousal, dominance, and valence of MSP-Podcast, while additionally using IEMOCAP and MOSI to test cross-corpus generalisation. To the best of our knowledge, we obtain the top performance for valence prediction without use of explicit linguistic information, with a concordance correlation coefficient (CCC) of. 638 on MSP-Podcast. Our investigations reveal that transformer-based architectures are more robust compared to a CNN-based baseline and fair with respect to gender groups, but not towards individual speakers. Finally, we show that their success on valence is based on implicit linguistic information, which explains why they perform on-par with recent multimodal approaches that explicitly utilise textual information. To make our findings reproducible, we release the best performing model to the community.},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/wagner2023.pdf;/Users/br/Zotero/storage/ZPH72A55/10089511.html}
}

@misc{wang2023,
  title = {{{PodReels}}: {{Human-AI Co-Creation}} of {{Video Podcast Teasers}}},
  shorttitle = {{{PodReels}}},
  author = {Wang, Sitong and Ning, Zheng and Truong, Anh and Dontcheva, Mira and Li, Dingzeyu and Chilton, Lydia B.},
  year = {2023},
  month = nov,
  number = {arXiv:2311.05867},
  eprint = {2311.05867},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-04},
  abstract = {Video podcast teasers are short videos that can be shared on social media platforms to capture interest in the full episodes of a video podcast. These teasers enable long-form podcasters to reach new audiences and gain new followers. However, creating a compelling teaser from an hour-long episode is challenging. Selecting interesting clips requires significant mental effort; editing the chosen clips into a cohesive, well-produced teaser is time-consuming. To support the creation of video podcast teasers, we first investigate what makes a good teaser. We combine insights from both audience comments and creator interviews to determine a set of essential ingredients. We also identify a common workflow shared by creators during the process. Based on these findings, we introduce a human-AI co-creative tool called PodReels to assist video podcasters in creating teasers. Our user study shows that PodReels significantly reduces creators' mental demand and improves their efficiency in producing video podcast teasers.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/wang2023.pdf;/Users/br/Zotero/storage/F4GZKLWC/2311.html}
}

@misc{wolf2020,
  title = {{{HuggingFace}}'s {{Transformers}}: {{State-of-the-art Natural Language Processing}}},
  shorttitle = {{{HuggingFace}}'s {{Transformers}}},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R{\'e}mi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and {von Platen}, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
  year = {2020},
  month = jul,
  number = {arXiv:1910.03771},
  eprint = {1910.03771},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-14},
  abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. {\textbackslash}textit\{Transformers\} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. {\textbackslash}textit\{Transformers\} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at {\textbackslash}url\{https://github.com/huggingface/transformers\}.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/wolf2020.pdf;/Users/br/Zotero/storage/F8NFTCLX/1910.html}
}

@inproceedings{zhang2020,
  title = {{{PEGASUS}}},
  shorttitle = {{{PEGASUS}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter},
  year = {2020},
  month = nov,
  pages = {11328--11339},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2023-11-08},
  abstract = {Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.},
  langid = {english},
  keywords = {uberflogen},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/zhang2020.pdf}
}

@misc{zotero-268,
  title = {(14) {{AI}} in {{Podcasting}}: {{Transforming Content Creation}} and {{Listener Experiences}} {\textbar} {{LinkedIn}}},
  urldate = {2023-12-19},
  howpublished = {https://www.linkedin.com/pulse/ai-podcasting-transforming-content-creation-listener-experiences/},
  file = {/Users/br/Zotero/storage/ZR9792E8/ai-podcasting-transforming-content-creation-listener-experiences.html}
}

@techreport{zotero-393,
  title = {Interoperable {{Master Format}} --- {{Core Constraints}}},
  institution = {IEEE},
  doi = {10.5594/SMPTE.ST2067-2.2020},
  urldate = {2024-03-08},
  isbn = {9781683032113},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/zotero-393.pdf}
}

@misc{zotero-523,
  title = {{Giga Gr{\"u}nheide - Tesla in Brandenburg}},
  journal = {ARD Audiothek},
  urldate = {2024-03-14},
  abstract = {Im beschaulichen Gr{\"u}nheide investiert Elon Musk Milliarden, denn hier in Brandenburg entsteht die erste Tesla-Fabrik Europas. In dem 9.000-Seelen-Ort gibt es viele, die sich auf die neue Fabrik freuen und andere, die dagegen auf die Barrikaden gehen. Im Podcast "Giga Gr{\"u}nheide -- Tesla in Brandenburg" erz{\"a}hlt ein rbb-Reporter-Team Geschichten {\"u}ber ein Dorf, das zur Zukunft der Elektromobilit{\"a}t werden soll.},
  howpublished = {https://www.ardaudiothek.de/sendung/giga-gruenheide-tesla-in-brandenburg/80566220/},
  langid = {ngerman},
  file = {/Users/br/Zotero/storage/VAISZEBB/80566220.html}
}

@misc{zotero-525,
  title = {{Nutzungsbedingungen}},
  journal = {ARD Audiothek},
  urldate = {2024-03-14},
  abstract = {Alle Inhalte der ARD Mediathek und der ARD Audiothek sowie der Angebote von tagesschau.de und sportschau.de sind urheberrechtlich gesch{\"u}tzt.},
  howpublished = {https://www.ardaudiothek.de/nutzungsbedingungen/},
  langid = {ngerman},
  file = {/Users/br/Zotero/storage/8CRWVDUA/nutzungsbedingungen.html}
}

@misc{zotero-527,
  title = {{radioWissen}},
  journal = {ARD Audiothek},
  urldate = {2024-03-14},
  abstract = {radioWissen, ein sinnliches H{\"o}rerlebnis mit Anspruch. Die ganze Welt des Wissens, gut recherchiert, spannend erz{\"a}hlt. Interessantes aus Geschichte, Literatur und Musik, Faszinierendes {\"u}ber Mythen, Menschen und Religionen, Erhellendes aus Philosophie und Psychologie. Wissenswertes {\"u}ber Natur, Biologie und Umwelt, Hintergr{\"u}nde zu Wirtschaft und Politik.},
  howpublished = {https://www.ardaudiothek.de/sendung/radiowissen/5945518/},
  langid = {ngerman},
  file = {/Users/br/Zotero/storage/5ZN4VZGZ/5945518.html}
}

@misc{zotero-529,
  title = {Playground - {{https://api.ardaudiothek.de/graphql}}},
  urldate = {2024-03-14},
  howpublished = {https://api.ardaudiothek.de/graphql},
  file = {/Users/br/Zotero/storage/BSRF52SQ/graphql.html}
}

@misc{zotero-543,
  title = {{1968 - Das Ausnahmejahr}},
  journal = {ARD Audiothek},
  urldate = {2024-03-14},
  abstract = {"1968" hat einer ganzen Generation ein Etikett verpasst, das sie nie wieder losgeworden ist: Die "68er" gelten bis heute als Generation der Revolte, des Aufbegehrens gegen den Muff der Nachkriegsjahre. (BR 2018)  Autor: Michael Zametzer},
  howpublished = {https://www.ardaudiothek.de/episode/radiowissen/1968-das-ausnahmejahr/bayern-2/78757544/},
  langid = {ngerman},
  file = {/Users/br/Zotero/storage/7TDHQYWB/78757544.html}
}

@misc{zotero-553,
  title = {Appropriate {{Uses For SQLite}}},
  urldate = {2024-03-14},
  howpublished = {https://www.sqlite.org/whentouse.html},
  file = {/Users/br/Zotero/storage/HI75I3U2/whentouse.html}
}

@misc{zotero-555,
  title = {Datatypes {{In SQLite}}},
  urldate = {2024-03-14},
  howpublished = {https://www.sqlite.org/datatype3.html},
  file = {/Users/br/Zotero/storage/7GS4SE2H/datatype3.html}
}

@misc{zotero-557,
  title = {Implementation {{Limits For SQLite}}},
  urldate = {2024-03-14},
  howpublished = {https://www.sqlite.org/limits.html},
  file = {/Users/br/Zotero/storage/J3LHC9MN/limits.html}
}

@misc{zotero-559,
  title = {Home {\textbar} {{Chroma}}},
  urldate = {2024-03-14},
  abstract = {Chroma is the open-source embedding database. Chroma makes it easy to build LLM apps by making knowledge, facts, and skills pluggable for LLMs.},
  howpublished = {https://www.trychroma.com/},
  langid = {english},
  file = {/Users/br/Zotero/storage/Y4C438GC/docs.trychroma.com.html}
}

@misc{zotero-567,
  title = {Jiaaro/Pydub @ {{GitHub}}},
  urldate = {2024-03-14},
  howpublished = {https://pydub.com/},
  file = {/Users/br/Zotero/storage/8JRPNICD/pydub.com.html}
}

@misc{zotero-569,
  title = {Gunicorn - {{Python WSGI HTTP Server}} for {{UNIX}}},
  urldate = {2024-03-14},
  howpublished = {https://gunicorn.org/},
  file = {/Users/br/Zotero/storage/IZQPRFUK/gunicorn.org.html}
}

@misc{zotero-572,
  title = {Embeddings},
  journal = {Voyage AI},
  urldate = {2024-03-14},
  abstract = {Model ChoicesVoyage currently provides the following embedding models. ModelContext Length (tokens)Embedding DimensionDescriptionvoyage-large-2160001536Our most powerful generalist embedding model (e.g., better than OpenAI V3 Large).voyage-code-2160001536Optimized for code retrieval (17\% better than...},
  howpublished = {https://docs.voyageai.com/docs/embeddings},
  langid = {english},
  file = {/Users/br/Zotero/storage/X8MQVW5L/embeddings.html}
}

@misc{zotero-574,
  title = {{{OpenAI Platform}}},
  urldate = {2024-03-14},
  abstract = {Explore developer resources, tutorials, API docs, and dynamic examples to get the most out of OpenAI's platform.},
  howpublished = {https://platform.openai.com},
  langid = {english},
  file = {/Users/br/Zotero/storage/RCL8V6IG/embeddings.html}
}

@misc{zotero-576,
  title = {Salesforce/{{SFR-Embedding-Mistral}} {$\cdot$} {{Hugging Face}}},
  urldate = {2024-03-14},
  abstract = {We're on a journey to advance and democratize artificial intelligence through open source and open science.},
  howpublished = {https://huggingface.co/Salesforce/SFR-Embedding-Mistral},
  file = {/Users/br/Zotero/storage/L4UN8TIV/SFR-Embedding-Mistral.html}
}

@misc{zouhar2023,
  title = {A {{Formal Perspective}} on {{Byte-Pair Encoding}}},
  author = {Zouhar, Vil{\'e}m and Meister, Clara and Gastaldi, Juan Luis and Du, Li and Vieira, Tim and Sachan, Mrinmaya and Cotterell, Ryan},
  year = {2023},
  month = jun,
  number = {arXiv:2306.16837},
  eprint = {2306.16837},
  primaryclass = {cs, math},
  publisher = {arXiv},
  urldate = {2024-03-11},
  abstract = {Byte-Pair Encoding (BPE) is a popular algorithm used for tokenizing data in NLP, despite being devised initially as a compression method. BPE appears to be a greedy algorithm at face value, but the underlying optimization problem that BPE seeks to solve has not yet been laid down. We formalize BPE as a combinatorial optimization problem. Via submodular functions, we prove that the iterative greedy version is a \${\textbackslash}frac\{1\}\{\{{\textbackslash}sigma({\textbackslash}boldsymbol\{{\textbackslash}mu\}\^{}{\textbackslash}star)\}\}(1-e\^{}\{-\{{\textbackslash}sigma({\textbackslash}boldsymbol\{{\textbackslash}mu\}\^{}{\textbackslash}star)\}\})\$-approximation of an optimal merge sequence, where \$\{{\textbackslash}sigma({\textbackslash}boldsymbol\{{\textbackslash}mu\}\^{}{\textbackslash}star)\}\$ is the total backward curvature with respect to the optimal merge sequence \${\textbackslash}boldsymbol\{{\textbackslash}mu\}\^{}{\textbackslash}star\$. Empirically the lower bound of the approximation is \${\textbackslash}approx 0.37\$. We provide a faster implementation of BPE which improves the runtime complexity from \${\textbackslash}mathcal\{O\}{\textbackslash}left(N M{\textbackslash}right)\$ to \${\textbackslash}mathcal\{O\}{\textbackslash}left(N {\textbackslash}log M{\textbackslash}right)\$, where \$N\$ is the sequence length and \$M\$ is the merge count. Finally, we optimize the brute-force algorithm for optimal BPE using memoization.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Mathematics - Optimization and Control},
  file = {/Users/br/My Drive/Vincent' Vault/Sources/PDFs/zouhar2023.pdf;/Users/br/Zotero/storage/IKHUQGNF/2306.html}
}
