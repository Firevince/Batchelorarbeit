{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 2 Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/br/Projects/Bachelorarbeit/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from db_connect import db_get_df, db_save_df\n",
    "\n",
    "from scripts.Embedding_creation.embedding_creator_TF_IDF import calc_all_tf_idf, calculate_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manuelle Erstellung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = AutoModel.from_pretrained('mesolitica/llama2-embedding-1b-8k', trust_remote_code = True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('mesolitica/llama2-embedding-1b-8k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"transcript_segments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[\"segment_text\"][1300:1600].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\n",
    "    df[\"segment_text\"].to_list(), \n",
    "    return_tensors = 'pt',\n",
    "    padding = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = model.encode(input_ids).detach().numpy()\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"embedding_json\"] = [json.dumps(model.encode(chunk_text).detach().numpy()) for chunk_text in tqdm(input_ids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(v[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vereinigen der Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_id = list(\"bcdyefghij\")\n",
    "all_df = db_get_df(\"transcript_segments_llama_2_a\")\n",
    "for id in table_id:\n",
    "    df_temp = db_get_df(f\"transcript_segments_llama_2_{id}\")\n",
    "    # print(len(df_temp))\n",
    "    print(df_temp.head(1).iloc[0, 0])\n",
    "    all_df = pd.concat([all_df, df_temp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(all_df, \"transcript_segments_llama_2_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = db_get_df(\"transcript_segments_llama_2_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_strings = [json.dumps(row.tolist()) for index, row in all_df.iterrows()]\n",
    "df[\"embedding_json\"] = json_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"transcript_segments_llama_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF fÃ¼r alle Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arrays = [\n",
    "    [\"bar\", \"bar\", \"bar\", \"bar\", \"foo\", \"foo\", \"qux\", \"qux\"],\n",
    "    [\"one\", \"two\", \"three\", \"four\", \"one\", \"two\", \"one\", \"two\"],\n",
    "]\n",
    "tuples = list(zip(*arrays))\n",
    "index = pd.MultiIndex.from_tuples(tuples, names=[\"first\", \"second\"])\n",
    "s = pd.Series(np.random.randn(8), index=index)\n",
    "s = pd.concat([s, pd.Series(np.random.randn(8))])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer file dumped and converting to df\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from db_connect import db_get_df, db_save_df\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "df = db_get_df(table=\"sentences_lemmatized\")\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['sentence_lemmatized'])\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
    "# df['tfidf_representation_json'] =  [json.dumps(tfidf_array[df.index[df['sentence'] == x][0]].tolist()) for x in tqdm(df['sentence'])]\n",
    "print(\"vectorizer file dumped and converting to df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m db_get_df(table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranscript_sentences\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[0;32m----> 4\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Bachelorarbeit/.venv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:2139\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2134\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2135\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2136\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2137\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2138\u001b[0m )\n\u001b[0;32m-> 2139\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2142\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Bachelorarbeit/.venv/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Bachelorarbeit/.venv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1386\u001b[0m             )\n\u001b[1;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1391\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/Bachelorarbeit/.venv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1274\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1272\u001b[0m values \u001b[38;5;241m=\u001b[39m _make_int_array()\n\u001b[1;32m   1273\u001b[0m indptr\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1275\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "df = db_get_df(table=\"sentences_lemmatized\")\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200341\n"
     ]
    }
   ],
   "source": [
    "print(len(tfidf_array[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "sparse.save_npz(\"tf_idf_matrix.npz\", tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import joblib\n",
    "\n",
    "tf_idf_matrix = sparse.load_npz(\"tf_idf_matrix.npz\")\n",
    "tfidf_vectorizer = joblib.load('tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = dict(sorted(tfidf_vectorizer.get_vocab().items()))\n",
    "\n",
    "# Specify the output file name\n",
    "output_file = 'sorted_keys.txt'\n",
    "\n",
    "# Open the file for writing\n",
    "with open(output_file, 'w') as file:\n",
    "    # Write each key to a separate line in the file\n",
    "    for key in sorted_dict.keys():\n",
    "        file.write(key + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embedding = pd.DataFrame(columns=list(range(len(tfidf_array))))\n",
    "print(df_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, representation in tqdm(enumerate(tfidf_array)):\n",
    "    print(representation)\n",
    "    for tf_idf_index in representation:\n",
    "        df.loc[i, f\"(tfidf_representation, {i})\"] = \"rep\"\n",
    "print(\"finished\")\n",
    "print(df)\n",
    "db_save_df(df, tablename=\"transcript_sentences_tf_idf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tfidf_array))\n",
    "print(len(df))\n",
    "print()\n",
    "df['tfidf_representation'] = [0] * len(tfidf_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_all_tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.Embedding_creation.embedding_creator_TF_IDF import calc_all_tf_idf, calculate_distances\n",
    "\n",
    "\n",
    "df_tfidf =  calculate_distances(\"Geschichte von Deutschland\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eigene idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import joblib\n",
    "import spacy\n",
    "\n",
    "tf_idf_matrix = sparse.load_npz(\"tf_idf_matrix.npz\")\n",
    "tfidf_vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "idf_values = tfidf_vectorizer.idf_\n",
    "idf_dict = dict(zip(feature_names, idf_values))\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "\n",
    "def lemmatize_german_sentence(input_sentence, nlp):\n",
    "    doc = nlp(input_sentence)\n",
    "    lemmatized_words = []\n",
    "    for token in doc:\n",
    "        lemma = token.lemma_\n",
    "        if lemma:\n",
    "            lemmatized_words.append(lemma)\n",
    "        else: \n",
    "            lemmatized_words.append(token)\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"sentences_lemmatized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4.895880118288627, 'wer'), (2.072407238502506, 'sein'), (5.2368274695944, 'Frau'), (9.971718606195646, 'Meier')]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Wer ist frau meier\"\n",
    "sentence = lemmatize_german_sentence(sentence, nlp)\n",
    "\n",
    "encoded_words = [(idf_dict[word.lower()], word) for word in sentence if word.lower() in idf_dict]\n",
    "print(encoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"sentences_lemmatized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lek meier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 370224/370224 [00:00<00:00, 529709.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word = sorted(encoded_words, key=lambda x:x[0])[-1][1].lower()\n",
    "print(\"lek\",word)\n",
    "occurences = []\n",
    "# for i, row in tqdm(df.iterrows()):\n",
    "#     sentence = row[\"sentence\"]\n",
    "#     if word in sentence.lower():\n",
    "#         occurences.append(row)\n",
    "for sentence in tqdm(df[\"sentence\"]):\n",
    "    if word in sentence.lower():\n",
    "        occurences.append(word)\n",
    "print(len(occurences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ã¼berwachsener', 13.128719027345758),\n",
       " ('Ã¼berwachungsapparat', 13.128719027345758),\n",
       " ('Ã¼berwachungsausschuÃ', 13.128719027345758),\n",
       " ('Ã¼berwachungsausschÃ¼ssen', 13.128719027345758),\n",
       " ('Ã¼berwachungsgerat', 13.128719027345758),\n",
       " ('Ã¼berwachungsinstitut', 13.128719027345758),\n",
       " ('Ã¼berwachungskamera', 13.128719027345758),\n",
       " ('Ã¼berwachungskomponente', 13.128719027345758),\n",
       " ('Ã¼berwachungsschiff', 13.128719027345758),\n",
       " ('Ã¼berwachungstechnisch', 13.128719027345758),\n",
       " ('Ã¼berwanden', 13.128719027345758),\n",
       " ('Ã¼berwarf', 13.128719027345758),\n",
       " ('Ã¼berwasser', 13.128719027345758),\n",
       " ('Ã¼berweg', 13.128719027345758),\n",
       " ('Ã¼berweiden', 13.128719027345758),\n",
       " ('Ã¼berwese', 13.128719027345758),\n",
       " ('Ã¼berwiegelnd', 13.128719027345758),\n",
       " ('Ã¼berwind', 13.128719027345758),\n",
       " ('Ã¼berwindbar', 13.128719027345758),\n",
       " ('Ã¼berwinder', 13.128719027345758),\n",
       " ('Ã¼berwindet', 13.128719027345758),\n",
       " ('Ã¼berwindungshÃ¼rde', 13.128719027345758),\n",
       " ('Ã¼berwinter', 13.128719027345758),\n",
       " ('Ã¼berwinternde', 13.128719027345758),\n",
       " ('Ã¼berwinterungsgrÃ¼nd', 13.128719027345758),\n",
       " ('Ã¼berwinterungsmechanism', 13.128719027345758),\n",
       " ('Ã¼berwinterungsorte', 13.128719027345758),\n",
       " ('Ã¼berwinterungsplÃ¤tz', 13.128719027345758),\n",
       " ('Ã¼berwinterungsquartier', 13.128719027345758),\n",
       " ('Ã¼berwirkungsvoll', 13.128719027345758),\n",
       " ('Ã¼berwund', 13.128719027345758),\n",
       " ('Ã¼berwÃ¤ltigbar', 13.128719027345758),\n",
       " ('Ã¼berwÃ¤ltigung', 13.128719027345758),\n",
       " ('Ã¼berwÃ¤ltigungsstrategie', 13.128719027345758),\n",
       " ('Ã¼berwÃ¶lbt', 13.128719027345758),\n",
       " ('Ã¼berwÃ¶lkt', 13.128719027345758),\n",
       " ('Ã¼berzeichnend', 13.128719027345758),\n",
       " ('Ã¼berzeichnung', 13.128719027345758),\n",
       " ('Ã¼berzeugender', 13.128719027345758),\n",
       " ('Ã¼berzeugendes', 13.128719027345758),\n",
       " ('Ã¼berzeugendst', 13.128719027345758),\n",
       " ('Ã¼berzeugungsmacht', 13.128719027345758),\n",
       " ('Ã¼berzeugungssystem', 13.128719027345758),\n",
       " ('Ã¼berzeugungstat', 13.128719027345758),\n",
       " ('Ã¼berzeugungsversuch', 13.128719027345758),\n",
       " ('Ã¼berzufahren', 13.128719027345758),\n",
       " ('Ã¼berzugeben', 13.128719027345758),\n",
       " ('Ã¼berzuggeschwindigkeit', 13.128719027345758),\n",
       " ('Ã¼berzuhaben', 13.128719027345758),\n",
       " ('Ã¼berzureagieren', 13.128719027345758),\n",
       " ('Ã¼berzustrapazieren', 13.128719027345758),\n",
       " ('Ã¼berzustrappazieren', 13.128719027345758),\n",
       " ('Ã¼berzuwechseln', 13.128719027345758),\n",
       " ('Ã¼berzÃ¤hlig', 13.128719027345758),\n",
       " ('Ã¼berzÃ¼chten', 13.128719027345758),\n",
       " ('Ã¼berzÃ¼chtet', 13.128719027345758),\n",
       " ('Ã¼berzÃ¼chtung', 13.128719027345758),\n",
       " ('Ã¼berÃ¤ngstlichkeit', 13.128719027345758),\n",
       " ('Ã¼berÃ¼bermorg', 13.128719027345758),\n",
       " ('Ã¼berÃ¼bernÃ¤chst', 13.128719027345758),\n",
       " ('Ã¼big', 13.128719027345758),\n",
       " ('Ã¼bler', 13.128719027345758),\n",
       " ('Ã¼bliche', 13.128719027345758),\n",
       " ('Ã¼bliches', 13.128719027345758),\n",
       " ('Ã¼brigbleibend', 13.128719027345758),\n",
       " ('Ã¼briger', 13.128719027345758),\n",
       " ('Ã¼briggebliebenen', 13.128719027345758),\n",
       " ('Ã¼briglieÃ', 13.128719027345758),\n",
       " ('Ã¼bst', 13.128719027345758),\n",
       " ('Ã¼bunge', 13.128719027345758),\n",
       " ('Ã¼bungen', 13.128719027345758),\n",
       " ('Ã¼bungs', 13.128719027345758),\n",
       " ('Ã¼bungsberg', 13.128719027345758),\n",
       " ('Ã¼bungseinheit', 13.128719027345758),\n",
       " ('Ã¼bungsfahrt', 13.128719027345758),\n",
       " ('Ã¼bungsfelder', 13.128719027345758),\n",
       " ('Ã¼bungsgelÃ¤nde', 13.128719027345758),\n",
       " ('Ã¼bungslager', 13.128719027345758),\n",
       " ('Ã¼bungsmÃ¶glichkeit', 13.128719027345758),\n",
       " ('Ã¼bungsmÃ¶glichkeiten', 13.128719027345758),\n",
       " ('Ã¼bungsplan', 13.128719027345758),\n",
       " ('Ã¼bungsplÃ¤ne', 13.128719027345758),\n",
       " ('Ã¼bungsrÃ¤um', 13.128719027345758),\n",
       " ('Ã¼bungsschieÃplatz', 13.128719027345758),\n",
       " ('Ã¼bungstechnik', 13.128719027345758),\n",
       " ('Ã¼bungsverhalt', 13.128719027345758),\n",
       " ('Ã¼bungsweg', 13.128719027345758),\n",
       " ('Ã¼bungszweck', 13.128719027345758),\n",
       " ('Ã¼d', 13.128719027345758),\n",
       " ('Ã¼mier', 13.128719027345758),\n",
       " ('Ã¼ppiges', 13.128719027345758),\n",
       " ('Ã¼rchen', 13.128719027345758),\n",
       " ('Ã¼stad', 13.128719027345758),\n",
       " ('Ã¼sÃ¼m', 13.128719027345758),\n",
       " ('Ã¼sÃ¼mcÃ¼', 13.128719027345758),\n",
       " ('Äernik', 13.128719027345758),\n",
       " ('Åafak', 13.128719027345758),\n",
       " ('Å¡koda', 13.128719027345758),\n",
       " ('Å¼atkowski', 13.128719027345758),\n",
       " ('Ð¼ÐµÑ', 13.128719027345758)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(idf_dict.items(), key=lambda x:x[1])[-100:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
