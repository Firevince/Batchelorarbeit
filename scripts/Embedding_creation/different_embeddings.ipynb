{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vergleichen verschiedener Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Notebook werden die verschiedenen Embedding Algorithmen demonstriert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import json\n",
    "\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from db_connect import db_get_df, db_save_df\n",
    "from Embedding_creation.embedding_creator_TF_IDF import (\n",
    "    calc_all_tf_idf,\n",
    "    calculate_distances_batchwise,\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLAMA 2 Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laden der Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModel.from_pretrained('mesolitica/llama2-embedding-1b-8k', trust_remote_code = True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('mesolitica/llama2-embedding-1b-8k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"transcript_sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisierung der Sätze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\n",
    "    df[\"sentence\"].to_list(), \n",
    "    return_tensors = 'pt',\n",
    "    padding = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = model.encode(input_ids).detach().numpy()\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speichern der Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"embedding_json\"] = [json.dumps(model.encode(chunk_text).detach().numpy()) for chunk_text in tqdm(input_ids)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vereinigen der Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_id = list(\"bcdyefghij\")\n",
    "all_df = db_get_df(\"transcript_segments_llama_2_a\")\n",
    "for id in table_id:\n",
    "    df_temp = db_get_df(f\"transcript_segments_llama_2_{id}\")\n",
    "    # print(len(df_temp))\n",
    "    print(df_temp.head(1).iloc[0, 0])\n",
    "    all_df = pd.concat([all_df, df_temp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(all_df, \"transcript_segments_llama_2_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = db_get_df(\"transcript_segments_llama_2_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_strings = [json.dumps(row.tolist()) for index, row in all_df.iterrows()]\n",
    "df[\"embedding_json\"] = json_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"transcript_segments_llama_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF für alle Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "from db_connect import db_get_df, db_save_df, load_pkl, save_pkl, save_npz, load_npz\n",
    "from embedding_creator_MINI_L6 import calculate_distances\n",
    "from segment_ranking.rank_segments import get_most_similar_documents_MINI_LM, get_most_similar_documents_tf_idf\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle\n",
    "\n",
    "load_dotenv()\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatisieren der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"sentences_lemmatized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filepath = \"/Users/br/Projects/Bachelorarbeit/data/matrices/tfidf_vectorizer200k.pkl\"\n",
    "with open(filepath, \"rb\") as fIn:\n",
    "    stored_data = pickle.load(fIn)\n",
    "    # stored_embeddings = stored_data[\"embeddings\"]\n",
    "    \n",
    "# tfidf_vectorizer = load_pkl(\"tfidf_vectorizer_200k.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_similar_documents_tf_idf(\"Oktoberfest bayern\", 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf auf lemmatisierte Sätze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"transcript_sentences\")\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['sentence'])\n",
    "\n",
    "\n",
    "# save_pkl(tfidf_vectorizer, 'tfidf_vectorizer200k.pkl')\n",
    "# print(\"vectorizer file dumped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tfidf_vectorizer.get_vocab())\n",
    "save_pkl(tfidf_vectorizer,\"tfidf_vectorizer_230k.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(table=\"sentences_lemmatized\")\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abspeichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_path = os.path.join(DATA_PATH,'matricies/tf_idf_matrix200k.npz')\n",
    "sparse.save_npz(matrix_path, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laden der Modelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_matrix = load_npz('tf_idf_matrix_230k.npz')\n",
    "tfidf_vectorizer = load_pkl('tfidf_vectorizer_230k.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional speichern des Vocabulars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = dict(sorted(tfidf_vectorizer.get_vocab().items()))\n",
    "output_file = 'vocabulary.txt'\n",
    "with open(output_file, 'w') as file:\n",
    "    for key in sorted_dict.keys():\n",
    "        file.write(key + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_all_tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.Embedding_creation.embedding_creator_TF_IDF import calc_all_tf_idf, calculate_distances_batchwise\n",
    "\n",
    "\n",
    "df_tfidf =  calculate_distances_batchwise(\"Geschichte von Deutschland\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDF Analyse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import joblib\n",
    "import spacy\n",
    "\n",
    "tf_idf_matrix = sparse.load_npz(\"tf_idf_matrix.npz\")\n",
    "tfidf_vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "idf_values = tfidf_vectorizer.idf_\n",
    "idf_dict = dict(zip(feature_names, idf_values))\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "\n",
    "def lemmatize_german_sentence(input_sentence, nlp):\n",
    "    doc = nlp(input_sentence)\n",
    "    lemmatized_words = []\n",
    "    for token in doc:\n",
    "        lemma = token.lemma_\n",
    "        if lemma:\n",
    "            lemmatized_words.append(lemma)\n",
    "        else: \n",
    "            lemmatized_words.append(token)\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"sentences_lemmatized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Wer ist frau meier\"\n",
    "sentence = lemmatize_german_sentence(sentence, nlp)\n",
    "\n",
    "encoded_words = [(idf_dict[word.lower()], word) for word in sentence if word.lower() in idf_dict]\n",
    "print(encoded_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SBert\n",
    "synchron - asynchron\n",
    "\n",
    "Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from db_connect import db_get_df, db_save_df, save_pkl, load_pkl\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from embedding_creator_MINI_L6 import calculate_distances\n",
    "from segment_ranking.rank_segments import get_most_similar_documents_MINI_LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_pkl(\"MINI_L6_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0:1000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_similar_documents_MINI_LM(\"Oktoberfest München\", 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.', \n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "save_pkl(sentences, embeddings, \"test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = load_pkl(\"MINI_L6_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(table=\"transcript_sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = calculate_distances(\"Oktoberfest in München\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from db_connect import db_get_df, db_save_df, save_pkl, load_pkl\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from embedding_creator_MINI_L6 import all_document_embeddings_batchwise_MINI_LM\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.', \n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "save_pkl(sentences, embeddings, \"test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, embeddings = load_pkl(\"MINI_L6_embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(table=\"transcript_sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = all_document_embeddings_batchwise_MINI_LM(df[\"sentence\"])\n",
    "save_pkl(sentences, embeddings, \"MINI_LM.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentece Transformer + TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from db_connect import db_get_df, db_save_df, save_pkl, load_pkl, save_npz, load_npz\n",
    "import scipy.sparse as sparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_lm_matrix = load_pkl(\"MINI_L6_embeddings.pkl\")\n",
    "tf_idf_matrix = load_npz(\"tf_idf_matrix_230k.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mini_lm_matrix.shape)\n",
    "print(tf_idf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(mini_lm_matrix, np.ndarray):\n",
    "    mini_lm_embeddings = np.array(mini_lm_matrix)\n",
    "\n",
    "mini_lm_sparse_matrix = sparse.csr_matrix(mini_lm_matrix)\n",
    "\n",
    "combined_matrix = sparse.hstack([tf_idf_matrix, mini_lm_sparse_matrix], format=\"csr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz(combined_matrix, \"tf_idf_mini_lm_matrix.npz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
