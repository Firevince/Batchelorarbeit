{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama 2 Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/br/Projects/Bachelorarbeit/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from db_connect import db_get_df, db_save_df\n",
    "\n",
    "from scripts.Embedding_creation.embedding_creator_TF_IDF import calc_all_tf_idf, calculate_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manuelle Erstellung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = AutoModel.from_pretrained('mesolitica/llama2-embedding-1b-8k', trust_remote_code = True)\n",
    "tokenizer = AutoTokenizer.from_pretrained('mesolitica/llama2-embedding-1b-8k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"transcript_segments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[\"segment_text\"][1300:1600].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\n",
    "    df[\"segment_text\"].to_list(), \n",
    "    return_tensors = 'pt',\n",
    "    padding = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = model.encode(input_ids).detach().numpy()\n",
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"embedding_json\"] = [json.dumps(model.encode(chunk_text).detach().numpy()) for chunk_text in tqdm(input_ids)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids[\"input_ids\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(v[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vereinigen der Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df()\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_id = list(\"bcdyefghij\")\n",
    "all_df = db_get_df(\"transcript_segments_llama_2_a\")\n",
    "for id in table_id:\n",
    "    df_temp = db_get_df(f\"transcript_segments_llama_2_{id}\")\n",
    "    # print(len(df_temp))\n",
    "    print(df_temp.head(1).iloc[0, 0])\n",
    "    all_df = pd.concat([all_df, df_temp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(all_df, \"transcript_segments_llama_2_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = db_get_df(\"transcript_segments_llama_2_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_strings = [json.dumps(row.tolist()) for index, row in all_df.iterrows()]\n",
    "df[\"embedding_json\"] = json_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"transcript_segments_llama_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF für alle Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arrays = [\n",
    "    [\"bar\", \"bar\", \"bar\", \"bar\", \"foo\", \"foo\", \"qux\", \"qux\"],\n",
    "    [\"one\", \"two\", \"three\", \"four\", \"one\", \"two\", \"one\", \"two\"],\n",
    "]\n",
    "tuples = list(zip(*arrays))\n",
    "index = pd.MultiIndex.from_tuples(tuples, names=[\"first\", \"second\"])\n",
    "s = pd.Series(np.random.randn(8), index=index)\n",
    "s = pd.concat([s, pd.Series(np.random.randn(8))])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorizer file dumped and converting to df\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from db_connect import db_get_df, db_save_df\n",
    "import numpy as np\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "df = db_get_df(table=\"sentences_lemmatized\")\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['sentence_lemmatized'])\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
    "# df['tfidf_representation_json'] =  [json.dumps(tfidf_array[df.index[df['sentence'] == x][0]].tolist()) for x in tqdm(df['sentence'])]\n",
    "print(\"vectorizer file dumped and converting to df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m db_get_df(table\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranscript_sentences\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer()\n\u001b[0;32m----> 4\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_vectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Bachelorarbeit/.venv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:2139\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[1;32m   2133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2134\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[1;32m   2135\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[1;32m   2136\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[1;32m   2137\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[1;32m   2138\u001b[0m )\n\u001b[0;32m-> 2139\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2142\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Bachelorarbeit/.venv/lib/python3.8/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/Bachelorarbeit/.venv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1388\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1386\u001b[0m             )\n\u001b[1;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1388\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1391\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/Bachelorarbeit/.venv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1274\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1272\u001b[0m values \u001b[38;5;241m=\u001b[39m _make_int_array()\n\u001b[1;32m   1273\u001b[0m indptr\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m-> 1274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[1;32m   1275\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   1276\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m analyze(doc):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "df = db_get_df(table=\"sentences_lemmatized\")\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200341\n"
     ]
    }
   ],
   "source": [
    "print(len(tfidf_array[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "sparse.save_npz(\"tf_idf_matrix.npz\", tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import joblib\n",
    "\n",
    "tf_idf_matrix = sparse.load_npz(\"tf_idf_matrix.npz\")\n",
    "tfidf_vectorizer = joblib.load('tfidf_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_dict = dict(sorted(tfidf_vectorizer.get_vocab().items()))\n",
    "\n",
    "# Specify the output file name\n",
    "output_file = 'sorted_keys.txt'\n",
    "\n",
    "# Open the file for writing\n",
    "with open(output_file, 'w') as file:\n",
    "    # Write each key to a separate line in the file\n",
    "    for key in sorted_dict.keys():\n",
    "        file.write(key + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embedding = pd.DataFrame(columns=list(range(len(tfidf_array))))\n",
    "print(df_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, representation in tqdm(enumerate(tfidf_array)):\n",
    "    print(representation)\n",
    "    for tf_idf_index in representation:\n",
    "        df.loc[i, f\"(tfidf_representation, {i})\"] = \"rep\"\n",
    "print(\"finished\")\n",
    "print(df)\n",
    "db_save_df(df, tablename=\"transcript_sentences_tf_idf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tfidf_array))\n",
    "print(len(df))\n",
    "print()\n",
    "df['tfidf_representation'] = [0] * len(tfidf_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_all_tf_idf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.Embedding_creation.embedding_creator_TF_IDF import calc_all_tf_idf, calculate_distances\n",
    "\n",
    "\n",
    "df_tfidf =  calculate_distances(\"Geschichte von Deutschland\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eigene idf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import joblib\n",
    "import spacy\n",
    "\n",
    "tf_idf_matrix = sparse.load_npz(\"tf_idf_matrix.npz\")\n",
    "tfidf_vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "idf_values = tfidf_vectorizer.idf_\n",
    "idf_dict = dict(zip(feature_names, idf_values))\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "\n",
    "def lemmatize_german_sentence(input_sentence, nlp):\n",
    "    doc = nlp(input_sentence)\n",
    "    lemmatized_words = []\n",
    "    for token in doc:\n",
    "        lemma = token.lemma_\n",
    "        if lemma:\n",
    "            lemmatized_words.append(lemma)\n",
    "        else: \n",
    "            lemmatized_words.append(token)\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"sentences_lemmatized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(4.895880118288627, 'wer'), (2.072407238502506, 'sein'), (5.2368274695944, 'Frau'), (9.971718606195646, 'Meier')]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Wer ist frau meier\"\n",
    "sentence = lemmatize_german_sentence(sentence, nlp)\n",
    "\n",
    "encoded_words = [(idf_dict[word.lower()], word) for word in sentence if word.lower() in idf_dict]\n",
    "print(encoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"sentences_lemmatized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lek meier\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 370224/370224 [00:00<00:00, 529709.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word = sorted(encoded_words, key=lambda x:x[0])[-1][1].lower()\n",
    "print(\"lek\",word)\n",
    "occurences = []\n",
    "# for i, row in tqdm(df.iterrows()):\n",
    "#     sentence = row[\"sentence\"]\n",
    "#     if word in sentence.lower():\n",
    "#         occurences.append(row)\n",
    "for sentence in tqdm(df[\"sentence\"]):\n",
    "    if word in sentence.lower():\n",
    "        occurences.append(word)\n",
    "print(len(occurences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('überwachsener', 13.128719027345758),\n",
       " ('überwachungsapparat', 13.128719027345758),\n",
       " ('überwachungsausschuß', 13.128719027345758),\n",
       " ('überwachungsausschüssen', 13.128719027345758),\n",
       " ('überwachungsgerat', 13.128719027345758),\n",
       " ('überwachungsinstitut', 13.128719027345758),\n",
       " ('überwachungskamera', 13.128719027345758),\n",
       " ('überwachungskomponente', 13.128719027345758),\n",
       " ('überwachungsschiff', 13.128719027345758),\n",
       " ('überwachungstechnisch', 13.128719027345758),\n",
       " ('überwanden', 13.128719027345758),\n",
       " ('überwarf', 13.128719027345758),\n",
       " ('überwasser', 13.128719027345758),\n",
       " ('überweg', 13.128719027345758),\n",
       " ('überweiden', 13.128719027345758),\n",
       " ('überwese', 13.128719027345758),\n",
       " ('überwiegelnd', 13.128719027345758),\n",
       " ('überwind', 13.128719027345758),\n",
       " ('überwindbar', 13.128719027345758),\n",
       " ('überwinder', 13.128719027345758),\n",
       " ('überwindet', 13.128719027345758),\n",
       " ('überwindungshürde', 13.128719027345758),\n",
       " ('überwinter', 13.128719027345758),\n",
       " ('überwinternde', 13.128719027345758),\n",
       " ('überwinterungsgründ', 13.128719027345758),\n",
       " ('überwinterungsmechanism', 13.128719027345758),\n",
       " ('überwinterungsorte', 13.128719027345758),\n",
       " ('überwinterungsplätz', 13.128719027345758),\n",
       " ('überwinterungsquartier', 13.128719027345758),\n",
       " ('überwirkungsvoll', 13.128719027345758),\n",
       " ('überwund', 13.128719027345758),\n",
       " ('überwältigbar', 13.128719027345758),\n",
       " ('überwältigung', 13.128719027345758),\n",
       " ('überwältigungsstrategie', 13.128719027345758),\n",
       " ('überwölbt', 13.128719027345758),\n",
       " ('überwölkt', 13.128719027345758),\n",
       " ('überzeichnend', 13.128719027345758),\n",
       " ('überzeichnung', 13.128719027345758),\n",
       " ('überzeugender', 13.128719027345758),\n",
       " ('überzeugendes', 13.128719027345758),\n",
       " ('überzeugendst', 13.128719027345758),\n",
       " ('überzeugungsmacht', 13.128719027345758),\n",
       " ('überzeugungssystem', 13.128719027345758),\n",
       " ('überzeugungstat', 13.128719027345758),\n",
       " ('überzeugungsversuch', 13.128719027345758),\n",
       " ('überzufahren', 13.128719027345758),\n",
       " ('überzugeben', 13.128719027345758),\n",
       " ('überzuggeschwindigkeit', 13.128719027345758),\n",
       " ('überzuhaben', 13.128719027345758),\n",
       " ('überzureagieren', 13.128719027345758),\n",
       " ('überzustrapazieren', 13.128719027345758),\n",
       " ('überzustrappazieren', 13.128719027345758),\n",
       " ('überzuwechseln', 13.128719027345758),\n",
       " ('überzählig', 13.128719027345758),\n",
       " ('überzüchten', 13.128719027345758),\n",
       " ('überzüchtet', 13.128719027345758),\n",
       " ('überzüchtung', 13.128719027345758),\n",
       " ('überängstlichkeit', 13.128719027345758),\n",
       " ('überübermorg', 13.128719027345758),\n",
       " ('überübernächst', 13.128719027345758),\n",
       " ('übig', 13.128719027345758),\n",
       " ('übler', 13.128719027345758),\n",
       " ('übliche', 13.128719027345758),\n",
       " ('übliches', 13.128719027345758),\n",
       " ('übrigbleibend', 13.128719027345758),\n",
       " ('übriger', 13.128719027345758),\n",
       " ('übriggebliebenen', 13.128719027345758),\n",
       " ('übrigließ', 13.128719027345758),\n",
       " ('übst', 13.128719027345758),\n",
       " ('übunge', 13.128719027345758),\n",
       " ('übungen', 13.128719027345758),\n",
       " ('übungs', 13.128719027345758),\n",
       " ('übungsberg', 13.128719027345758),\n",
       " ('übungseinheit', 13.128719027345758),\n",
       " ('übungsfahrt', 13.128719027345758),\n",
       " ('übungsfelder', 13.128719027345758),\n",
       " ('übungsgelände', 13.128719027345758),\n",
       " ('übungslager', 13.128719027345758),\n",
       " ('übungsmöglichkeit', 13.128719027345758),\n",
       " ('übungsmöglichkeiten', 13.128719027345758),\n",
       " ('übungsplan', 13.128719027345758),\n",
       " ('übungspläne', 13.128719027345758),\n",
       " ('übungsräum', 13.128719027345758),\n",
       " ('übungsschießplatz', 13.128719027345758),\n",
       " ('übungstechnik', 13.128719027345758),\n",
       " ('übungsverhalt', 13.128719027345758),\n",
       " ('übungsweg', 13.128719027345758),\n",
       " ('übungszweck', 13.128719027345758),\n",
       " ('üd', 13.128719027345758),\n",
       " ('ümier', 13.128719027345758),\n",
       " ('üppiges', 13.128719027345758),\n",
       " ('ürchen', 13.128719027345758),\n",
       " ('üstad', 13.128719027345758),\n",
       " ('üsüm', 13.128719027345758),\n",
       " ('üsümcü', 13.128719027345758),\n",
       " ('černik', 13.128719027345758),\n",
       " ('şafak', 13.128719027345758),\n",
       " ('škoda', 13.128719027345758),\n",
       " ('żatkowski', 13.128719027345758),\n",
       " ('мер', 13.128719027345758)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(idf_dict.items(), key=lambda x:x[1])[-100:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
