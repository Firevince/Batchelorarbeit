{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_segmentation.concat_audio import produce_final_audio\n",
    "from audio_segmentation.split_audio import produce_audio_snippets\n",
    "from db_connect import db_get_df\n",
    "from dotenv import load_dotenv\n",
    "import matplotlib.pyplot as plt\n",
    "import mutagen.mp3\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "audiofile_path = os.getenv(\"AUDIO_SOURCE_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(table=\"transcripts_all\")\n",
    "print(len(df))\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir verwenden hier das Deutsche model, um die deutschen Transkripte optimal zu encodieren."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio file length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mp3_lengths(directory):\n",
    "    mp3_lengths = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".mp3\"):\n",
    "                try:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    audio = mutagen.mp3.MP3(file_path)\n",
    "                    mp3_lengths.append(audio.info.length)\n",
    "                except:\n",
    "                    print(f\"file {file} corrupted\")\n",
    "    return mp3_lengths\n",
    "\n",
    "def plot_mp3_lengths(mp3_lengths):\n",
    "    plt.hist(mp3_lengths, bins=100, color='cornflowerblue', edgecolor='black')\n",
    "    plt.xlabel('MP3 Länge (Sekungen)')\n",
    "    plt.ylabel('Anzahl MP3 Datein')\n",
    "    plt.title('Länge der MP3 Datein')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp3_directory = audiofile_path\n",
    "mp3_lengths = get_mp3_lengths(mp3_directory)\n",
    "plot_mp3_lengths(mp3_lengths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcript length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(table=\"transcript_word_level_2237\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df[df['filename'] == 'david-bowie-das-chamaeleon-des-pop.mp3']\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transcript_lengths(transcript_lengths):\n",
    "    ax = plt.gca()\n",
    "    ax.set_xlim([1500, 4000])\n",
    "    plt.hist(transcript_lengths, bins=100, color='cornflowerblue', edgecolor='black')\n",
    "    plt.xlabel('Transkript Länge (Wörter)')\n",
    "    plt.ylabel('Anzahl  Transkripte')\n",
    "    plt.title('Länge der Trankripte')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "filenames = df[\"filename\"].drop_duplicates(ignore_index=True)\n",
    "for filename in tqdm(filenames):\n",
    "    word_entries = df[df['filename'] == filename]\n",
    "    lengths.append(len(word_entries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_transcript_lengths(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Länge der Sätze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(table=\"transcript_sentences_spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentence_lengths(sentence_lengths):\n",
    "    plt.hist(sentence_lengths, bins=100, color='cornflowerblue', edgecolor='black')\n",
    "    plt.xlabel('Sätze Länge (Wörter)')\n",
    "    plt.ylabel('Anzahl  Sätze')\n",
    "    plt.title('Länge der Sätze')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentence_lenght\"] = df[\"sentence\"].apply(len)\n",
    "df = df.sort_values(by=\"sentence_lenght\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[-1][\"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"sentence_lenght\"] > 2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logarithmische Normalverteilung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sentence_lengths(df[df[\"sentence_lenght\"] < 500][\"sentence_lenght\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anzahl aller Episoden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "GRAPHQL_URL = \"https://api.ardaudiothek.de/graphql\"\n",
    "def get_graphql(query):\n",
    "    response = requests.post(GRAPHQL_URL, json={\"query\": query})\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        raise f\"GraphQL request failed with status code {response.status_code}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "{\n",
    "\tshows: programSets(\n",
    "    filter:{\n",
    "      numberOfElements:{\n",
    "        greaterThanOrEqualTo: 0\n",
    "      }\n",
    "    }\n",
    "  )\n",
    "  {totalCount}\n",
    "  \n",
    "items(filter:{isPublished:{equalTo:true}}){\n",
    "    totalCount\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alle Autoren (nicht Sprecher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    {\n",
    "        programSet(id: 5945518) {\n",
    "        items(\n",
    "            filter: {\n",
    "            isPublished: {\n",
    "                equalTo: true\n",
    "            }\n",
    "            }\n",
    "        ) {\n",
    "            nodes {\n",
    "              description\n",
    "            }\n",
    "        }\n",
    "        }\n",
    "    }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_graphql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(autor[\"description\"].split(\"Autorin: \")[-1].split(\"Autor: \")[-1]) for autor in response[\"data\"][\"programSet\"][\"items\"][\"nodes\"][:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "autors = set()\n",
    "\n",
    "for node in response[\"data\"][\"programSet\"][\"items\"][\"nodes\"]:\n",
    "    autor = node[\"description\"]\n",
    "    if \"Autor: \" in autor or \"Autorin: \" in autor:\n",
    "        autor = autor.split(\"Autorin: \")[-1].split(\"Autor: \")[-1]\n",
    "        autor = autor.split(\"(\")[0]\n",
    "    else:\n",
    "        autor = \"\"\n",
    "    autors.add(autor)\n",
    "\n",
    "print(len(autors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Einzelne Episdoden transkripte analysieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anzahl Nomen herausfinden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from german_compound_splitter import comp_split\n",
    "from embedding_creation.embedding_creator_TF_IDF import is_number, compound_split_sentence\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import spacy\n",
    "\n",
    "load_dotenv()\n",
    "AUDIO_SOURCE_PATH = os.getenv(\"AUDIO_SOURCE_PATH\")\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateipfad = os.path.join(DATA_PATH, \"test_data\", \"vocabulary_compound_split.txt\")\n",
    "nlp = spacy.load(\"de_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_words_by_pos(words, sorted_words):\n",
    "    doc = nlp(\" \".join(words))\n",
    "    \n",
    "    for token in doc:\n",
    "        pos = token.pos_\n",
    "        if pos in sorted_words:\n",
    "            sorted_words[pos].append(token.text)\n",
    "        else:\n",
    "            sorted_words[pos] = [token.text]\n",
    "    \n",
    "    return sorted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "with open(dateipfad, 'r', encoding='utf-8') as datei:\n",
    "    for zeile in tqdm(datei):\n",
    "        word = zeile.strip()\n",
    "        words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_size = 50_000\n",
    "sorted_words = {}\n",
    "for i in tqdm(range(0,len(words), doc_size)):\n",
    "    sorted_words = sort_words_by_pos(words[i: i+doc_size], sorted_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for key in sorted_words.keys():\n",
    "    print(key, len(sorted_words[key]))\n",
    "    count += len(sorted_words[key])\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(sorted_words[\"NOUN\"]) + len(sorted_words[\"PROPN\"])) / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_words[\"INTJ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbs = sorted(sorted_words[\"NOUN\"], key=len)\n",
    "print(verbs[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Längste Wörter herausfinden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "with open(dateipfad, 'r', encoding='utf-8') as datei:\n",
    "    for zeile in tqdm(datei):\n",
    "        word = zeile.strip()\n",
    "        words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[170000:170010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_words = sum(len(s) > 10 for s in words)\n",
    "print(long_words / len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(vocab_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUM 1736\n",
    "ADJ 29240\n",
    "NOUN 68457\n",
    "ADV 20331\n",
    "PROPN 65982\n",
    "X 2140\n",
    "PUNCT 27\n",
    "VERB 15147\n",
    "ADP 568\n",
    "AUX 229\n",
    "DET 674\n",
    "PART 10\n",
    "CCONJ 103\n",
    "PRON 174\n",
    "SCONJ 16\n",
    "INTJ 1\n",
    "204835"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
