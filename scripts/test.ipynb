{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Notebook werden alle Sachen geschrieben, die: \n",
    "- ausprobiert werden\n",
    "- gerade nicht mehr eingesetzt werden\n",
    "- Nur einmal für Datenverarbeitung benutzt werden, die nicht sein müsste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Server DB richtig formatieren\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "unable to open database file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdb_connect\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m db_get_df, db_save_df\n\u001b[0;32m----> 5\u001b[0m con \u001b[38;5;241m=\u001b[39m \u001b[43msqlite3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Volumes/Samsung_T5/Backup/transcripts_all.sqlite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql_query(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM transcript_word_level_2237\u001b[39m\u001b[38;5;124m\"\u001b[39m, con)\n\u001b[1;32m      7\u001b[0m con\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mOperationalError\u001b[0m: unable to open database file"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "from db_connect import db_get_df, db_save_df\n",
    "\n",
    "con = sqlite3.connect(\"/Volumes/Samsung_T5/Backup/transcripts_all.sqlite\")\n",
    "df = pd.read_sql_query(f\"SELECT * FROM transcript_word_level_2237\", con)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)\n",
    "print(df[\"filename\"].drop_duplicates(ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['filename'] = df['filename'].str.replace('/nfs/scratch/students/neumannvi84434/Podcast_Episoden/', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"/Volumes/Samsung_T5/Backup/transcripts.sqlite\") as con:\n",
    "    df.to_sql(\"transcript_word_level_2237\", con, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"transcript_word_level_2237\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satzbildung durch Whisper Punkte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir nun alle MP3 Datein transkribiert haben, ist für jedes einzelne vorkommende Wort ein Zeitstempel gespeichert.\n",
    "\n",
    "Für das Projekt lohnt es sich allerdings größere Abschnitte zu erstellen, auf die später die Embeddings und die Suchen angewendet werden können, um mehr Kontext miteinzubeziehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from db_connect import db_get_df, db_save_df\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"transcript_word_level_2237\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2235/2235 [24:59<00:00,  1.49it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m             sentence_dict\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfilename\u001b[39m\u001b[38;5;124m\"\u001b[39m: filename, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence\u001b[39m\u001b[38;5;124m\"\u001b[39m: sentence, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart\u001b[39m\u001b[38;5;124m\"\u001b[39m: start, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:end})\n\u001b[1;32m     19\u001b[0m             one_sentence \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 21\u001b[0m df_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(sentence_dict)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "one_sentence = []\n",
    "sentence_dict = [] \n",
    "filenames = df[\"filename\"].drop_duplicates(ignore_index=True)\n",
    "\n",
    "for filename in tqdm(filenames):\n",
    "    word_entries = df[df['filename'] == filename]\n",
    "    start = -1\n",
    "    end = -1\n",
    "    for index, word_entry in word_entries.iterrows(): \n",
    "        word = word_entry[\"word\"]\n",
    "        \n",
    "        if not one_sentence: ##start\n",
    "            start = word_entry[\"start\"]\n",
    "        one_sentence.append(word.strip())\n",
    "        if \".\" in word:\n",
    "            end = word_entry[\"end\"]\n",
    "            sentence = \" \".join(one_sentence)\n",
    "            sentence_dict.append({\"filename\": filename, \"sentence\": sentence, \"start\": start, \"end\":end})\n",
    "            one_sentence = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences = pd.DataFrame(sentence_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "425374"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df_sentences, \"transcript_sentences_whisper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setences ID hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =db_get_df(\"sentences_compound_split\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['segment_id'] = df.groupby('filename').cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"sentences_compound_split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kompositatrennung mit Pyphen (Silbentrennung)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyphen\n",
    "import os\n",
    "\n",
    "deutsche_worte = pyphen.Pyphen(lang='de_DE')\n",
    "einzelwoerter_set = set()\n",
    "\n",
    "dateipfad = os.path.join(DATA_PATH, \"vocabulary.txt\")\n",
    "\n",
    "with open(dateipfad, 'r', encoding='utf-8') as datei:\n",
    "    for zeile in datei:\n",
    "        woerter = zeile.strip().split()\n",
    "        \n",
    "        for wort in woerter:\n",
    "            getrenntes_wort = deutsche_worte.inserted(wort)\n",
    "            \n",
    "            einzelwoerter = getrenntes_wort.split('-')\n",
    "            einzelwoerter_set.update(einzelwoerter)\n",
    "\n",
    "for wort in einzelwoerter_set:\n",
    "    print(wort)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(einzelwoerter_set))\n",
    "print(einzelwoerter_set[3000:3010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list_to_file(lst, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for item in lst:\n",
    "            file.write(str(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "einzelwoerter_set = sorted(einzelwoerter_set)\n",
    "save_list_to_file(einzelwoerter_set, os.path.join(DATA_PATH, \"vocabulary_pyphen.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vokabular compound splitten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from german_compound_splitter import comp_split\n",
    "from embedding_creation.embedding_creator_TF_IDF import is_number, compound_split_sentence\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "AUDIO_SOURCE_PATH = os.getenv(\"AUDIO_SOURCE_PATH\")\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "einzelwoerter_set = set()\n",
    "dateipfad = os.path.join(DATA_PATH, \"vocabulary.txt\")\n",
    "input_file = os.path.join(DATA_PATH, \"german.dic\")\n",
    "ahocs = comp_split.read_dictionary_from_file(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dateipfad, 'r', encoding='utf-8') as datei:\n",
    "\n",
    "    for zeile in tqdm(datei):\n",
    "        word = zeile.strip()\n",
    "        if is_number(word):\n",
    "            continue\n",
    "        try:\n",
    "            dissection = comp_split.dissect(word, ahocs, make_singular=True)\n",
    "        except:\n",
    "            dissection = \"\"\n",
    "        for split_word in dissection:\n",
    "            einzelwoerter_set.add(split_word)\n",
    "\n",
    "print(len(einzelwoerter_set))\n",
    "# print(einzelwoerter_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "einzelwoerter_set = sorted(einzelwoerter_set)\n",
    "save_list_to_file(einzelwoerter_set, os.path.join(DATA_PATH, \"vocabulary_compound_split.txt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
