{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podcast Episode Erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "from episodes_downloader.episodes_downloader import (\n",
    "    get_metadata_all_episodes,\n",
    "    download_and_save_mp3_in_dir\n",
    ")\n",
    "import os\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "from playsound import playsound\n",
    "import IPython\n",
    "import io\n",
    "import json\n",
    "\n",
    "from db_connect import db_get_df, db_save_df\n",
    "from segment_ranking.rank_segments import (\n",
    "    get_most_similar_segments,\n",
    ")\n",
    "\n",
    "# from Audio_segmentation.split_audio import produce_snippets\n",
    "# from Audio_segmentation.concat_audio import produce_audio\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "AUDIO_SOURCE_PATH = os.getenv(\"AUDIO_SOURCE_PATH\")\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episoden URLs laden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst wollen wir alle Epsioden auflisten und die Download URLs erhalten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt werden wir für jeden Podcast noch andere Metadaten, wie die description, das publish date und keywörter abspeichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_metadata_all_episodes()\n",
    "print(\"Beispieltitel\",df[\"title\"].head(5))\n",
    "print(\"Anzahl Episoden:\",len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"filename\"] = [url.split(\"/\")[-1] for url in df[\"download_url\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"episodes_metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alle mp3s herunterladen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Audios zu bearbeiten müssen wir sie herunterladen. Da die über 2000 Episoden mp3 ca. 50 GB beanspruchen werden sie hier auf eine externe SSD gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = AUDIO_SOURCE_PATH\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"episodes_metadata\")\n",
    "\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    title = row[\"download_url\"].split(\"/\")[-1]\n",
    "    if title in os.listdir(path):\n",
    "        continue\n",
    "    download_and_save_mp3_in_dir(row[\"download_url\"], path, title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[179, \"download_url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transkription auf Word level Ebene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dann müssen die einzelnen Audios transkribiert werden. \n",
    "Whisper bietet sich als Transkriptionstool an. Allerdings ist der Prozess alle Audiofiles zu transkribieren sehr aufwändig und sollte auf guter Hardware mit GPU Cuda unterstützung erfolgen.\n",
    "\n",
    "Beispieltranskription für ein file: (kann ca. 20 min dauern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_transcription.faster_whisper_word_level import transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(AUDIO_SOURCE_PATH, os.listdir(AUDIO_SOURCE_PATH)[4])\n",
    "df = transcribe(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle MP3s sind auf Wort Ebene transkribiert und in der Tabelle transcript_word_level_2237 abgespeichert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satzbildung durch Whisper Punkte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir nun alle MP3 Datein transkribiert haben, ist für jedes einzelne vorkommende Wort ein Zeitstempel gespeichert.\n",
    "\n",
    "Für das Projekt lohnt es sich allerdings größere Abschnitte zu erstellen, auf die später die Embeddings und die Suchen angewendet werden können, um mehr Kontext miteinzubeziehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"transcript_word_level_2237\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_sentence = []\n",
    "sentence_dict = [] \n",
    "filenames = df[\"filename\"].drop_duplicates(ignore_index=True)\n",
    "\n",
    "for filename in tqdm(filenames):\n",
    "    word_entries = df[df['filename'] == filename]\n",
    "    start = -1\n",
    "    end = -1\n",
    "    for index, word_entry in word_entries.iterrows(): \n",
    "        word = word_entry[\"word\"]\n",
    "        \n",
    "        if not one_sentence: ##start\n",
    "            start = word_entry[\"start\"]\n",
    "        one_sentence.append(word.strip())\n",
    "        if \".\" in word:\n",
    "            end = word_entry[\"end\"]\n",
    "            sentence = \" \".join(one_sentence)\n",
    "            sentence_dict.append({\"filename\": filename, \"sentence\": sentence, \"start\": start, \"end\":end})\n",
    "            one_sentence = []\n",
    "\n",
    "df_sentences = pd.DataFrame(sentence_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df_sentences, \"transcript_sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satzbildung mit Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"transcript_word_level_2237\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "\n",
    "filenames = df[\"filename\"].drop_duplicates(ignore_index=True)\n",
    "sentence_dict = []\n",
    "\n",
    "for filename in tqdm(filenames):\n",
    "    word_entries = df[df['filename'] == filename]\n",
    "    text = \" \".join(word_entries[\"word\"].apply(lambda x: x.strip()).tolist())\n",
    "    \n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    sentences = list(doc.sents)\n",
    "    current_word_index = 0  # To keep track of the word index in word_entries\n",
    "\n",
    "    for sent in sentences:\n",
    "        words_in_sent = sent.text.split()\n",
    "        sentence_length = len(words_in_sent)\n",
    "        \n",
    "        if current_word_index + sentence_length > len(word_entries):\n",
    "            break  \n",
    "        \n",
    "        start_time = word_entries.iloc[current_word_index][\"start\"]\n",
    "        end_time = word_entries.iloc[current_word_index + sentence_length - 1][\"end\"]\n",
    "\n",
    "        sentence_dict.append({\"filename\": filename, \"sentence\": sent.text, \"start\": start_time, \"end\": end_time})\n",
    "        \n",
    "        current_word_index += sentence_length  # Move to the index for the next sentence\n",
    "\n",
    "df_sentences = pd.DataFrame(sentence_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df_sentences, \"transcript_sentences_spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "con = sqlite3.connect(\"/Volumes/Samsung_T5/Backup/transcripts.sqlite\")\n",
    "df = pd.read_sql_query(f\"SELECT * FROM transcript_word_level_2237\", con)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)\n",
    "print(df[\"filename\"].drop_duplicates(ignore_index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Namen der Filenames ändern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['filename'] = df['filename'].str.replace('/nfs/scratch/students/neumannvi84434/Podcast_Episoden/', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"/Volumes/Samsung_T5/Backup/transcripts.sqlite\") as con:\n",
    "    df.to_sql(\"transcript_word_level_2237\", con, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"transcript_word_level_2237\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setence ID hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =db_get_df(\"transcript_sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['segment_id'] = df.groupby('filename').cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"transcript_sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisierung mit spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Keywordsuche mit TF-IDF zu verbessern kann man die einzelnen Wörter vor dem Suchen Lemmatisieren.\n",
    "Das heißt man mapped mehrere Verwandte Wörter auf ein einziges Wort.\n",
    "\n",
    "Bsp. Bäume -> Baum; war -> sein; schneller -> schnell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_german_sentence(input_sentence, nlp):\n",
    "    doc = nlp(input_sentence)\n",
    "    lemmatized_words = []\n",
    "    for token in doc:\n",
    "        lemmatized_words.append(token.lemma_)\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"transcript_sentences\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "all_sentences = []\n",
    "for sentence in tqdm(df[\"sentence\"]):\n",
    "    all_sentences.append(lemmatize_german_sentence(sentence, nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences_full = [\" \".join(sentence) for sentence in all_sentences]\n",
    "df[\"sentence_lemmatized\"] = all_sentences_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"sentences_lemmatized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kompositatrennung mit pyphen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyphen\n",
    "\n",
    "deutsche_worte = pyphen.Pyphen(lang='de_DE')\n",
    "einzelwoerter_set = set()\n",
    "\n",
    "dateipfad = os.path.join(DATA_PATH, \"vocabulary.txt\")\n",
    "\n",
    "with open(dateipfad, 'r', encoding='utf-8') as datei:\n",
    "    for zeile in datei:\n",
    "        woerter = zeile.strip().split()\n",
    "        \n",
    "        for wort in woerter:\n",
    "            getrenntes_wort = deutsche_worte.inserted(wort)\n",
    "            \n",
    "            einzelwoerter = getrenntes_wort.split('-')\n",
    "            einzelwoerter_set.update(einzelwoerter)\n",
    "\n",
    "for wort in einzelwoerter_set:\n",
    "    print(wort)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(einzelwoerter_set))\n",
    "print(einzelwoerter_set[3000:3010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list_to_file(lst, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for item in lst:\n",
    "            file.write(str(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "einzelwoerter_set = sorted(einzelwoerter_set)\n",
    "save_list_to_file(einzelwoerter_set, os.path.join(DATA_PATH, \"vocabulary_pyphen.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kompositatrennung mit german_compound_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from german_compound_splitter import comp_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "einzelwoerter_set = set()\n",
    "dateipfad = os.path.join(DATA_PATH, \"vocabulary.txt\")\n",
    "input_file = os.path.join(DATA_PATH, \"german.dic\")\n",
    "ahocs = comp_split.read_dictionary_from_file(input_file)\n",
    "\n",
    "def is_number(string):\n",
    "    try:\n",
    "        float(string)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "\n",
    "with open(dateipfad, 'r', encoding='utf-8') as datei:\n",
    "\n",
    "    for zeile in tqdm(datei):\n",
    "        wort = zeile.strip()\n",
    "        if is_number(wort):\n",
    "            continue\n",
    "        try:\n",
    "            dissection = comp_split.dissect(wort, ahocs, make_singular=True)\n",
    "        except:\n",
    "            dissection = \"\"\n",
    "        for split_word in dissection:\n",
    "            einzelwoerter_set.add(split_word)\n",
    "\n",
    "print(len(einzelwoerter_set))\n",
    "# print(einzelwoerter_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "einzelwoerter_set = sorted(einzelwoerter_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_list_to_file(einzelwoerter_set, os.path.join(DATA_PATH, \"vocabulary_compound_split.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration: Suche mit Keywörtern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lemmatized = db_get_df(\"sentences_lemmatized\")\n",
    "df_unlemmatized = db_get_df(\"transcript_sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occurences(df, word):\n",
    "    occurences = []\n",
    "    for sentence in tqdm(df[\"sentence\"]):\n",
    "        if word in sentence.lower().split():\n",
    "            occurences.append(sentence)\n",
    "    return occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"?\"\n",
    "occurences_lemmatized = get_occurences(df_lemmatized, word)\n",
    "occurences_unlemmatized = get_occurences(df_unlemmatized, word)\n",
    "print(f\"Occurences Lemmatized({len(occurences_lemmatized)}):\",occurences_lemmatized)\n",
    "print(f\"Occurences UnLemmatized({len(occurences_unlemmatized)}):\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(table=\"transcript_sentences_spacy\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bowie = df[df[\"filename\"] == \"david-bowie-das-chamaeleon-des-pop.mp3\"]\n",
    "save_list_to_file(df_bowie[\"sentence\"].to_list(), os.path.join(DATA_PATH, \"david_bowie_spacy.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userInputText = \"Zugspitze wandern\"\n",
    "userInput_segment_count = 7\n",
    "best_fitting = get_most_similar_documents_tf_idf(userInputText, userInput_segment_count)\n",
    "print(best_fitting[\"sentence\"].to_markdown())\n",
    "\n",
    "produce_snippets()\n",
    "produce_audio()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"/Users/br/Projects/Bachelorarbeit/scripts/server/audio/concatenated_audio.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Umwandelen aller MP3s in WAVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_mp3_to_wav(source_dir, target_dir):\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    for filename in tqdm(os.listdir(source_dir)):\n",
    "        if filename.endswith('.mp3') and filename.replace('.mp3', '.wav') not in os.listdir(target_dir) :\n",
    "            mp3_path = os.path.join(source_dir, filename)\n",
    "            wav_path = os.path.join(target_dir, filename.replace('.mp3', '.wav'))\n",
    "            \n",
    "            \n",
    "            audio = AudioSegment.from_mp3(mp3_path)\n",
    "            \n",
    "            audio.export(wav_path, format=\"wav\")\n",
    "            print(f\"Converted {filename} to WAV and saved to {wav_path}\")\n",
    "\n",
    "# Example usage\n",
    "source_directory = '/Volumes/Samsung_T5/Podcast_Episoden'\n",
    "target_directory = '/Volumes/Samsung_T5/Podcast_Episoden_Wav'\n",
    "convert_mp3_to_wav(source_directory, target_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
