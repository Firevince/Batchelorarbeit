{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podcast Episode Erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")\n",
    "from audio_transcription.faster_whisper_word_level import transcribe\n",
    "from Episodes_Downloader.episodes_downloader import get_names_and_urls_all_episodes, download_and_save_mp3_in_dir\n",
    "import os\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "from playsound import playsound\n",
    "import IPython \n",
    "import io\n",
    "from db_connect import db_get_df,db_save_df\n",
    "from segment_ranking.rank_segments import get_most_similar_documents_Llama2, get_most_similar_documents_tf_idf\n",
    "# from Audio_segmentation.split_audio import produce_snippets\n",
    "# from Audio_segmentation.concat_audio import produce_audio\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episoden URLs laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles, audio_urls = get_names_and_urls_all_episodes()\n",
    "print(titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"title\": titles, \"download_url\": audio_urls, \"transcript\": None})\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"transcripts_all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadaten herunterladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")\n",
    "from Episodes_Downloader.episodes_downloader import get_graphql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"transcripts_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"filename\"] = [url.split(\"/\")[-1] for url in df[\"download_url\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    {\n",
    "        programSet(id: 5945518) {\n",
    "        title\n",
    "        items(\n",
    "            orderBy: PUBLISH_DATE_DESC\n",
    "            filter: {\n",
    "            isPublished: {\n",
    "                equalTo: true\n",
    "            }\n",
    "            }\n",
    "        ) {\n",
    "            nodes {\n",
    "              keywords\n",
    "              publishDate\n",
    "              description\n",
    "              audios {\n",
    "                downloadUrl\n",
    "              }\n",
    "            }\n",
    "        }\n",
    "        }\n",
    "    }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_graphql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"description\"] = [None] * len(df)\n",
    "df[\"publish_date\"] = [None] * len(df)\n",
    "df[\"keywords\"] = [None] * len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for audio_data in response[\"data\"][\"programSet\"][\"items\"][\"nodes\"]:\n",
    "    audio_filename = audio_data[\"audios\"][0][\"downloadUrl\"].split(\"/\")[-1]\n",
    "    matching_row = df[df[\"filename\"] == audio_filename]\n",
    "    \n",
    "    if not matching_row.empty:\n",
    "        matching_index = matching_row.index[0]\n",
    "        df.at[matching_index, \"description\"] = audio_data[\"description\"]\n",
    "        df.at[matching_index, \"publish_date\"] = audio_data[\"publishDate\"]\n",
    "        df.at[matching_index, \"keywords_json\"] = json.dumps(audio_data[\"keywords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df[\"description\"].isna()][\"filename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"episodes_metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alle mp3s herunterladen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Volumes/Samsung_T5/Podcast_Episoden\"\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"transcripts_all\")\n",
    "\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    title = row[\"download_url\"].split(\"/\")[-1]\n",
    "    if title in os.listdir(path):\n",
    "        continue\n",
    "    download_and_save_mp3_in_dir(row[\"download_url\"], path, title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[179, \"download_url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transkription auf Word level Ebene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = transcribe(\"/Users/br/Projects/Bachelorarbeit/data/Episode_audio_files/jonathan swift gullivers reisen 2.mp3\")\n",
    "# db_save_df(df,\"transcript_word_level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"transcript_gulliver_word_level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "con = sqlite3.connect(\"/Volumes/Samsung_T5/Backup/transcripts.sqlite\")\n",
    "df = pd.read_sql_query(f\"SELECT * FROM transcript_word_level_2237\", con)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)\n",
    "print(df[\"filename\"].drop_duplicates(ignore_index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Namen der Filenames Ã¤ndern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['filename'] = df['filename'].str.replace('/nfs/scratch/students/neumannvi84434/Podcast_Episoden/', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"/Volumes/Samsung_T5/Backup/transcripts.sqlite\") as con:\n",
    "    df.to_sql(\"transcript_word_level_2237\", con, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"transcript_word_level_2237\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### setence id add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =db_get_df(\"transcript_sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['segment_id'] = df.groupby('filename').cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"transcript_sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisierung mit spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_german_sentence(input_sentence, nlp):\n",
    "    doc = nlp(input_sentence)\n",
    "    lemmatized_words = []\n",
    "    for token in doc:\n",
    "        lemmatized_words.append(token.lemma_)\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"transcript_sentences\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "all_sentences = []\n",
    "for sentence in tqdm(df[\"sentence\"]):\n",
    "    all_sentences.append(lemmatize_german_sentence(sentence, nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences[3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences_full = [\" \".join(sentence) for sentence in all_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_sentences_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentence_lemmatized\"] = all_sentences_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"sentences_lemmatized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences_full[4000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kompositatrennung mit pyphen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyphen\n",
    "\n",
    "# Erstellen Sie ein WÃ¶rterbuch fÃ¼r die deutsche Sprache\n",
    "deutsche_worte = pyphen.Pyphen(lang='de_DE')\n",
    "\n",
    "# Set zum Speichern der einzelnen WÃ¶rter initialisieren\n",
    "einzelwoerter_set = set()\n",
    "\n",
    "# Pfad zur Textdatei\n",
    "dateipfad = '/Users/br/Projects/Bachelorarbeit/scripts/Embedding_creation/vocabulary.txt'\n",
    "\n",
    "# Textdatei Ã¶ffnen und lesen\n",
    "with open(dateipfad, 'r', encoding='utf-8') as datei:\n",
    "    for zeile in datei:\n",
    "        # WÃ¶rter in der Zeile aufteilen (z.B. Leerzeichen als Trennzeichen verwenden)\n",
    "        woerter = zeile.strip().split()\n",
    "        \n",
    "        for wort in woerter:\n",
    "            # Wort trennen\n",
    "            getrenntes_wort = deutsche_worte.inserted(wort)\n",
    "            \n",
    "            # Trennungsergebnis in einzelne WÃ¶rter aufteilen\n",
    "            einzelwoerter = getrenntes_wort.split('-')\n",
    "            \n",
    "            # Einzelne WÃ¶rter zum Set hinzufÃ¼gen\n",
    "            einzelwoerter_set.update(einzelwoerter)\n",
    "\n",
    "# Ausgabe der einzelnen WÃ¶rter im Set\n",
    "for wort in einzelwoerter_set:\n",
    "    print(wort)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(einzelwoerter_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from german_compound_splitter import comp_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "einzelwoerter_set = set()\n",
    "dateipfad = '/Users/br/Projects/Bachelorarbeit/scripts/Embedding_creation/vocabulary.txt'\n",
    "input_file = '/Users/br/Projects/Bachelorarbeit/german.dic'\n",
    "ahocs = comp_split.read_dictionary_from_file(input_file)\n",
    "\n",
    "def is_number(string):\n",
    "    try:\n",
    "        float(string)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "\n",
    "with open(dateipfad, 'r', encoding='utf-8') as datei:\n",
    "\n",
    "    for zeile in tqdm(datei):\n",
    "        wort = zeile.strip()\n",
    "        if is_number(wort):\n",
    "            continue\n",
    "        try:\n",
    "            dissection = comp_split.dissect(wort, ahocs, make_singular=True)\n",
    "        except:\n",
    "            print(wort)\n",
    "            dissection = \"\"\n",
    "        einzelwoerter_set.update(wort)\n",
    "\n",
    "print(len(einzelwoerter_set))\n",
    "print(einzelwoerter_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(einzelwoerter_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"sentences_lemmatized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"seife\"\n",
    "occurences = []\n",
    "for sentence in tqdm(df[\"sentence\"]):\n",
    "    if word in sentence.lower():\n",
    "        occurences.append(sentence)\n",
    "print(occurences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"transcript_sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"chiemsee\"\n",
    "occurences = []\n",
    "for sentence in tqdm(df[\"sentence\"]):\n",
    "    if word in sentence.lower():\n",
    "        occurences.append(sentence)\n",
    "print(occurences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satzbildung durch Whisper Punkte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZunÃ¤chst erstmal mit Whispers Punkten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"transcript_word_level_2237\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_sentence = []\n",
    "sentence_dict = [] \n",
    "filenames = df[\"filename\"].drop_duplicates(ignore_index=True)\n",
    "\n",
    "for filename in tqdm(filenames):\n",
    "    word_entries = df[df['filename'] == filename]\n",
    "    start = -1\n",
    "    end = -1\n",
    "    for index, word_entry in word_entries.iterrows(): \n",
    "        word = word_entry[\"word\"]\n",
    "        \n",
    "        if not one_sentence: ##start\n",
    "            start = word_entry[\"start\"]\n",
    "        one_sentence.append(word.strip())\n",
    "        if \".\" in word:\n",
    "            end = word_entry[\"end\"]\n",
    "            sentence = \" \".join(one_sentence)\n",
    "            sentence_dict.append({\"filename\": filename, \"sentence\": sentence, \"start\": start, \"end\":end})\n",
    "            one_sentence = []\n",
    "\n",
    "df_sentences = pd.DataFrame(sentence_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df_sentences, \"transcript_sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(table=\"transcript_sentences\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userInputText = \"Zugspitze wandern\"\n",
    "userInput_segment_count = 7\n",
    "best_fitting = get_most_similar_documents_tf_idf(userInputText, userInput_segment_count)\n",
    "print(best_fitting[\"sentence\"].to_markdown())\n",
    "\n",
    "produce_snippets()\n",
    "produce_audio()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"/Users/br/Projects/Bachelorarbeit/scripts/server/audio/concatenated_audio.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## umwandelen aller MP3s in WAVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_mp3_to_wav(source_dir, target_dir):\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    for filename in tqdm(os.listdir(source_dir)):\n",
    "        if filename.endswith('.mp3') and filename.replace('.mp3', '.wav') not in os.listdir(target_dir) :\n",
    "            mp3_path = os.path.join(source_dir, filename)\n",
    "            wav_path = os.path.join(target_dir, filename.replace('.mp3', '.wav'))\n",
    "            \n",
    "            \n",
    "            audio = AudioSegment.from_mp3(mp3_path)\n",
    "            \n",
    "            audio.export(wav_path, format=\"wav\")\n",
    "            print(f\"Converted {filename} to WAV and saved to {wav_path}\")\n",
    "\n",
    "# Example usage\n",
    "source_directory = '/Volumes/Samsung_T5/Podcast_Episoden'\n",
    "target_directory = '/Volumes/Samsung_T5/Podcast_Episoden_Wav'\n",
    "convert_mp3_to_wav(source_directory, target_directory)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
