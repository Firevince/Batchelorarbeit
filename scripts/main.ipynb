{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podcast Episode Erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "playsound is relying on a python 2 subprocess. Please use `pip3 install PyObjC` if you want playsound to run more efficiently.\n",
      "/Users/br/Projects/Bachelorarbeit/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "from episodes_downloader.episodes_downloader import (\n",
    "    get_metadata_all_episodes,\n",
    "    download_and_save_mp3_in_dir\n",
    ")\n",
    "import os\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "from playsound import playsound\n",
    "import IPython\n",
    "import io\n",
    "import json\n",
    "\n",
    "from db_connect import db_get_df, db_save_df\n",
    "from segment_ranking.rank_segments import (\n",
    "    get_most_similar_segments,\n",
    ")\n",
    "\n",
    "# from Audio_segmentation.split_audio import produce_snippets\n",
    "# from Audio_segmentation.concat_audio import produce_audio\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "AUDIO_SOURCE_PATH = os.getenv(\"AUDIO_SOURCE_PATH\")\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episoden URLs laden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst wollen wir alle Epsioden auflisten und die Download URLs erhalten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt werden wir für jeden Podcast noch andere Metadaten, wie die description, das publish date und keywörter abspeichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_metadata_all_episodes()\n",
    "print(\"Beispieltitel\",df[\"title\"].head(5))\n",
    "print(\"Anzahl Episoden:\",len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"filename\"] = [url.split(\"/\")[-1] for url in df[\"download_url\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"episodes_metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alle mp3s herunterladen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Audios zu bearbeiten müssen wir sie herunterladen. Da die über 2000 Episoden mp3 ca. 50 GB beanspruchen werden sie hier auf eine externe SSD gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = AUDIO_SOURCE_PATH\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"episodes_metadata\")\n",
    "\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    title = row[\"download_url\"].split(\"/\")[-1]\n",
    "    if title in os.listdir(path):\n",
    "        continue\n",
    "    download_and_save_mp3_in_dir(row[\"download_url\"], path, title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[179, \"download_url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transkription auf Word level Ebene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dann müssen die einzelnen Audios transkribiert werden. \n",
    "Whisper bietet sich als Transkriptionstool an. Allerdings ist der Prozess alle Audiofiles zu transkribieren sehr aufwändig und sollte auf guter Hardware mit GPU Cuda unterstützung erfolgen.\n",
    "\n",
    "Beispieltranskription für ein file: (kann ca. 20 min dauern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_transcription.faster_whisper_word_level import transcribe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(AUDIO_SOURCE_PATH, os.listdir(AUDIO_SOURCE_PATH)[4])\n",
    "df = transcribe(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satzbildung durch Whisper Punkte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir nun alle MP3 Datein transkribiert haben, ist für jedes einzelne vorkommende Wort ein Zeitstempel gespeichert.\n",
    "\n",
    "Für das Projekt lohnt es sich allerdings größere Abschnitte zu erstellen, auf die später die Embeddings und die Suchen angewendet werden können, um mehr Kontext miteinzubeziehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"transcript_word_level_2237\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_sentence = []\n",
    "sentence_dict = [] \n",
    "filenames = df[\"filename\"].drop_duplicates(ignore_index=True)\n",
    "\n",
    "for filename in tqdm(filenames):\n",
    "    word_entries = df[df['filename'] == filename]\n",
    "    start = -1\n",
    "    end = -1\n",
    "    for index, word_entry in word_entries.iterrows(): \n",
    "        word = word_entry[\"word\"]\n",
    "        \n",
    "        if not one_sentence: ##start\n",
    "            start = word_entry[\"start\"]\n",
    "        one_sentence.append(word.strip())\n",
    "        if \".\" in word:\n",
    "            end = word_entry[\"end\"]\n",
    "            sentence = \" \".join(one_sentence)\n",
    "            sentence_dict.append({\"filename\": filename, \"sentence\": sentence, \"start\": start, \"end\":end})\n",
    "            one_sentence = []\n",
    "\n",
    "df_sentences = pd.DataFrame(sentence_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df_sentences, \"transcript_sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satzbildung mit Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"transcript_word_level_2237\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1945/1945 [28:44<00:00,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "\n",
    "# Assuming df is your DataFrame and already defined\n",
    "filenames = df[\"filename\"].drop_duplicates(ignore_index=True)\n",
    "sentence_dict = []\n",
    "\n",
    "for filename in tqdm(filenames):\n",
    "    word_entries = df[df['filename'] == filename]\n",
    "    text = \" \".join(word_entries[\"word\"].apply(lambda x: x.strip()).tolist())\n",
    "    \n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    sentences = list(doc.sents)\n",
    "    current_word_index = 0  # To keep track of the word index in word_entries\n",
    "\n",
    "    for sent in sentences:\n",
    "        words_in_sent = sent.text.split()\n",
    "        sentence_length = len(words_in_sent)\n",
    "        \n",
    "        if current_word_index + sentence_length > len(word_entries):\n",
    "            break  \n",
    "        \n",
    "        start_time = word_entries.iloc[current_word_index][\"start\"]\n",
    "        end_time = word_entries.iloc[current_word_index + sentence_length - 1][\"end\"]\n",
    "\n",
    "        sentence_dict.append({\"filename\": filename, \"sentence\": sent.text, \"start\": start_time, \"end\": end_time})\n",
    "        \n",
    "        current_word_index += sentence_length  # Move to the index for the next sentence\n",
    "\n",
    "df_sentences = pd.DataFrame(sentence_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377443"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df_sentences, \"transcript_sentences_spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "con = sqlite3.connect(\"/Volumes/Samsung_T5/Backup/transcripts.sqlite\")\n",
    "df = pd.read_sql_query(f\"SELECT * FROM transcript_word_level_2237\", con)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.dtypes)\n",
    "print(df[\"filename\"].drop_duplicates(ignore_index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Namen der Filenames ändern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['filename'] = df['filename'].str.replace('/nfs/scratch/students/neumannvi84434/Podcast_Episoden/', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"/Volumes/Samsung_T5/Backup/transcripts.sqlite\") as con:\n",
    "    df.to_sql(\"transcript_word_level_2237\", con, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"transcript_word_level_2237\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setence ID hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =db_get_df(\"transcript_sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['segment_id'] = df.groupby('filename').cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"transcript_sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisierung mit spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Keywordsuche mit TF-IDF zu verbessern kann man die einzelnen Wörter vor dem Suchen Lemmatisieren.\n",
    "Das heißt man mapped mehrere Verwandte Wörter auf ein einziges Wort.\n",
    "\n",
    "Bsp. Bäume -> Baum; war -> sein; schneller -> schnell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_german_sentence(input_sentence, nlp):\n",
    "    doc = nlp(input_sentence)\n",
    "    lemmatized_words = []\n",
    "    for token in doc:\n",
    "        lemmatized_words.append(token.lemma_)\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"transcript_sentences\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "all_sentences = []\n",
    "for sentence in tqdm(df[\"sentence\"]):\n",
    "    all_sentences.append(lemmatize_german_sentence(sentence, nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences_full = [\" \".join(sentence) for sentence in all_sentences]\n",
    "df[\"sentence_lemmatized\"] = all_sentences_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"sentences_lemmatized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kompositatrennung mit pyphen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyphen\n",
    "\n",
    "deutsche_worte = pyphen.Pyphen(lang='de_DE')\n",
    "einzelwoerter_set = set()\n",
    "\n",
    "dateipfad = os.path.join(DATA_PATH, \"vocabulary.txt\")\n",
    "\n",
    "with open(dateipfad, 'r', encoding='utf-8') as datei:\n",
    "    for zeile in datei:\n",
    "        woerter = zeile.strip().split()\n",
    "        \n",
    "        for wort in woerter:\n",
    "            getrenntes_wort = deutsche_worte.inserted(wort)\n",
    "            \n",
    "            einzelwoerter = getrenntes_wort.split('-')\n",
    "            einzelwoerter_set.update(einzelwoerter)\n",
    "\n",
    "for wort in einzelwoerter_set:\n",
    "    print(wort)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24669\n",
      "['biebst', 'bied', 'bief', 'bieg', 'biegt', 'bien', 'bier', 'bierl', 'biers', 'bierst']\n"
     ]
    }
   ],
   "source": [
    "print(len(einzelwoerter_set))\n",
    "print(einzelwoerter_set[3000:3010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list_to_file(lst, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        for item in lst:\n",
    "            file.write(str(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "einzelwoerter_set = sorted(einzelwoerter_set)\n",
    "save_list_to_file(einzelwoerter_set, os.path.join(DATA_PATH, \"vocabulary_pyphen.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kompositatrennung mit german_compound_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from german_compound_splitter import comp_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "einzelwoerter_set = set()\n",
    "dateipfad = os.path.join(DATA_PATH, \"vocabulary.txt\")\n",
    "input_file = os.path.join(DATA_PATH, \"german.dic\")\n",
    "ahocs = comp_split.read_dictionary_from_file(input_file)\n",
    "\n",
    "def is_number(string):\n",
    "    try:\n",
    "        float(string)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "\n",
    "with open(dateipfad, 'r', encoding='utf-8') as datei:\n",
    "\n",
    "    for zeile in tqdm(datei):\n",
    "        wort = zeile.strip()\n",
    "        if is_number(wort):\n",
    "            continue\n",
    "        try:\n",
    "            dissection = comp_split.dissect(wort, ahocs, make_singular=True)\n",
    "        except:\n",
    "            dissection = \"\"\n",
    "        for split_word in dissection:\n",
    "            einzelwoerter_set.add(split_word)\n",
    "\n",
    "print(len(einzelwoerter_set))\n",
    "# print(einzelwoerter_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "einzelwoerter_set = sorted(einzelwoerter_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_list_to_file(einzelwoerter_set, os.path.join(DATA_PATH, \"vocabulary_compound_split.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration: Suche mit Keywörtern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lemmatized = db_get_df(\"sentences_lemmatized\")\n",
    "df_unlemmatized = db_get_df(\"transcript_sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occurences(df, word):\n",
    "    occurences = []\n",
    "    for sentence in tqdm(df[\"sentence\"]):\n",
    "        if word in sentence.lower().split():\n",
    "            occurences.append(sentence)\n",
    "    return occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/370224 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 370224/370224 [00:01<00:00, 284255.28it/s]\n",
      "100%|██████████| 370224/370224 [00:01<00:00, 315795.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occurences Lemmatized(9): ['? Gar keine Wahl, weder super noch normal.', '? Und es ist, wo man fragt, der Benzin -Blusang gesagt.', '? Ach, das Leben ist so üd und ohne Sinn.', '? Ist das nicht gemein, wir haben kein Benzin? Die junge Generation erlebt zum ersten Mal, was ein gewisser Mangel bedeuten kann.', \"7, 6, 7, 6, 7, 6, 7, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7 7, 6, 8, 9, 1, 2, 3, 4 5, 4, 5, 5, 4, 5 6, 7, 10, 11, 12 ? Major Tom's a drunk kid, strung out in heaven's mind, ? hitting that all -time low.\", '? Sitting in the jungle, ? I remember the straw sun.', '? My mother said to get things done, ? to better my mess with Major Tom.', 'Pourquoi est -il devenu un Flaksbauer ? Il y a beaucoup de raisons.', \"Et qu 'est -ce que Bléze de Saint -Just pense quand il pense à un flac ? Je pense immédiatement à une très jolie femme avec un joli drapeau et une très jolie femme avec un joli drapeau et une très jolie femme avec un joli drapeau et une très jolie femme avec un joli drapeau et une très jolie femme avec un joli drapeau et une très jolie femme avec un joli drapeau avec un joli drapeau et une très jolie femme avec un joli drapeau avec un joli drapeau et une très jolie femme avec un joli drapeau et une très jolie femme avec un joli drapeau Ist das bayerische Volk hochgewachsen und stark, auf Nächstenliebe und Sitte gegründet, wie es schon hieß? Oder sind die Bayern sau grob versoffen und, vorsichtig gesagt, sinnenfroh, was auch schon behauptet wurde? Ja, so san's.\"]\n",
      "Occurences UnLemmatized(9):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "word = \"?\"\n",
    "occurences_lemmatized = get_occurences(df_lemmatized, word)\n",
    "occurences_unlemmatized = get_occurences(df_unlemmatized, word)\n",
    "print(f\"Occurences Lemmatized({len(occurences_lemmatized)}):\",occurences_lemmatized)\n",
    "print(f\"Occurences UnLemmatized({len(occurences_unlemmatized)}):\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filename     object\n",
       "sentence     object\n",
       "start       float64\n",
       "end         float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = db_get_df(table=\"transcript_sentences_spacy\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bowie = df[df[\"filename\"] == \"david-bowie-das-chamaeleon-des-pop.mp3\"]\n",
    "save_list_to_file(df_bowie[\"sentence\"].to_list(), os.path.join(DATA_PATH, \"david_bowie_spacy.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userInputText = \"Zugspitze wandern\"\n",
    "userInput_segment_count = 7\n",
    "best_fitting = get_most_similar_documents_tf_idf(userInputText, userInput_segment_count)\n",
    "print(best_fitting[\"sentence\"].to_markdown())\n",
    "\n",
    "produce_snippets()\n",
    "produce_audio()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"/Users/br/Projects/Bachelorarbeit/scripts/server/audio/concatenated_audio.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Umwandelen aller MP3s in WAVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_mp3_to_wav(source_dir, target_dir):\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    for filename in tqdm(os.listdir(source_dir)):\n",
    "        if filename.endswith('.mp3') and filename.replace('.mp3', '.wav') not in os.listdir(target_dir) :\n",
    "            mp3_path = os.path.join(source_dir, filename)\n",
    "            wav_path = os.path.join(target_dir, filename.replace('.mp3', '.wav'))\n",
    "            \n",
    "            \n",
    "            audio = AudioSegment.from_mp3(mp3_path)\n",
    "            \n",
    "            audio.export(wav_path, format=\"wav\")\n",
    "            print(f\"Converted {filename} to WAV and saved to {wav_path}\")\n",
    "\n",
    "# Example usage\n",
    "source_directory = '/Volumes/Samsung_T5/Podcast_Episoden'\n",
    "target_directory = '/Volumes/Samsung_T5/Podcast_Episoden_Wav'\n",
    "convert_mp3_to_wav(source_directory, target_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
