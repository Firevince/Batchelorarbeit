{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podcast Episode Erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\".\")\n",
    "from audio_downloader.episodes_downloader import (\n",
    "    get_metadata_all_episodes,\n",
    "    download_and_save_mp3_in_dir\n",
    ")\n",
    "import os\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "from playsound import playsound\n",
    "import IPython\n",
    "import io\n",
    "import json\n",
    "\n",
    "from db_connect import db_get_df, db_save_df\n",
    "from segment_ranking.rank_segments import (\n",
    "    get_most_similar_segments,\n",
    ")\n",
    "\n",
    "# from Audio_segmentation.split_audio import produce_snippets\n",
    "# from Audio_segmentation.concat_audio import produce_audio\n",
    "from tqdm import tqdm\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "AUDIO_SOURCE_PATH = os.getenv(\"AUDIO_SOURCE_PATH\")\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_SOURCE_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episoden URLs laden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst wollen wir alle Epsioden auflisten und die Download URLs erhalten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jetzt werden wir für jeden Podcast noch andere Metadaten, wie die description, das publish date und keywörter abspeichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_downloader.episodes_downloader import (\n",
    "    get_metadata_all_episodes,\n",
    "    download_and_save_mp3_in_dir\n",
    ")\n",
    "from db_connect import db_get_df, db_save_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_metadata_all_episodes()\n",
    "print(\"Beispieltitel\",df[\"title\"].head(5))\n",
    "print(\"Anzahl Episoden:\",len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"filename\"] = [url.split(\"/\")[-1] for url in df[\"download_url\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"episodes_metadata\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alle MP3 Datein herunterladen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Audios zu bearbeiten müssen wir sie herunterladen. Da die über 2000 Episoden mp3 ca. 50 GB beanspruchen werden sie hier auf eine externe SSD gespeichert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from db_connect import db_get_df, db_save_df\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "AUDIO_SOURCE_PATH = os.getenv(\"AUDIO_SOURCE_PATH\")\n",
    "path = AUDIO_SOURCE_PATH\n",
    "len(os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"episodes_metadata\")\n",
    "\n",
    "for i, row in tqdm(df.iterrows()):\n",
    "    title = row[\"download_url\"].split(\"/\")[-1]\n",
    "    if title in os.listdir(path):\n",
    "        continue\n",
    "    download_and_save_mp3_in_dir(row[\"download_url\"], path, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[179, \"download_url\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transkription auf Word level Ebene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dann müssen die einzelnen Audios transkribiert werden. \n",
    "Whisper bietet sich als Transkriptionstool an. Allerdings ist der Prozess alle Audiofiles zu transkribieren sehr aufwändig und sollte auf guter Hardware mit GPU Cuda unterstützung erfolgen.\n",
    "\n",
    "Beispieltranskription für ein file: (kann ca. 20 min dauern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from audio_transcription.faster_whisper_word_level import transcribe\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "AUDIO_SOURCE_PATH = os.getenv(\"AUDIO_SOURCE_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(AUDIO_SOURCE_PATH, os.listdir(AUDIO_SOURCE_PATH)[4])\n",
    "df = transcribe(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle MP3s sind auf Wort Ebene transkribiert und in der Tabelle transcript_word_level_2237 abgespeichert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Satzbildung mit Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from db_connect import db_get_df, db_save_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_spacy(df, nlp):\n",
    "    filenames = df[\"filename\"].drop_duplicates(ignore_index=True)\n",
    "    sentence_dict = []\n",
    "\n",
    "    for filename in tqdm(filenames):\n",
    "        word_entries = df[df['filename'] == filename]\n",
    "        text = \" \".join(word_entries[\"word\"].apply(lambda x: x.strip()).tolist())\n",
    "        \n",
    "        # Process the text with spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        sentences = list(doc.sents)\n",
    "        current_word_index = 0  # To keep track of the word index in word_entries\n",
    "\n",
    "        for sent in sentences:\n",
    "            words_in_sent = sent.text.split()\n",
    "            sentence_length = len(words_in_sent)\n",
    "            \n",
    "            if current_word_index + sentence_length > len(word_entries):\n",
    "                break  \n",
    "            \n",
    "            start_time = word_entries.iloc[current_word_index][\"start\"]\n",
    "            end_time = word_entries.iloc[current_word_index + sentence_length - 1][\"end\"]\n",
    "\n",
    "            sentence_dict.append({\"filename\": filename, \"sentence\": sent.text, \"start\": start_time, \"end\": end_time})\n",
    "            \n",
    "            current_word_index += sentence_length  # Move to the index for the next sentence\n",
    "\n",
    "    df_sentences = pd.DataFrame(sentence_dict)\n",
    "    return df_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"transcript_word_level\")\n",
    "nlp = spacy.load(\"de_dep_news_trf\") # model is 0.04% better in sentence than de_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentences = get_sentences_spacy(df, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['segment_id'] = df.groupby('filename').cumcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df_sentences, \"transcript_sentences_spacy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisierung mit spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Keywordsuche mit TF-IDF zu verbessern kann man die einzelnen Wörter vor dem Suchen Lemmatisieren.\n",
    "Das heißt man mapped mehrere Verwandte Wörter auf ein einziges Wort.\n",
    "Dabei werden in der deutschen Sprache \n",
    "- Alle Nomen zu Nominativ Singular\n",
    "- Alle Verben zu Infinitiv Präsenz aktiv\n",
    "\n",
    "Bsp. Bäume -> Baum; war -> sein; schneller -> schnell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from embedding_creation.embedding_creator_TF_IDF import lemmatize_german_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"transcript_sentences_spacy\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_md\")\n",
    "all_sentences = []\n",
    "for sentence in tqdm(df[\"sentence\"]):\n",
    "    all_sentences.append(lemmatize_german_sentence(sentence, nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences_full = [\" \".join(sentence) for sentence in all_sentences]\n",
    "df[\"sentence_lemmatized\"] = all_sentences_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"sentences_lemmatized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kompositatrennung mit german_compound_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from german_compound_splitter import comp_split\n",
    "from embedding_creation.embedding_creator_TF_IDF import is_number, compound_split_sentence\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from db_connect import db_get_df, db_save_df\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "DATA_PATH = os.getenv(\"DATA_PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound_split_df(df, ahocs):\n",
    "    df_temp = df.copy()\n",
    "    compound_split_sentences = []\n",
    "    for sentence in tqdm(df_temp[\"sentence\"]):\n",
    "        compound_split_sentences.append(compound_split_sentence(sentence, ahocs))\n",
    "    df_temp[\"sentence\"] = compound_split_sentences\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(\"transcript_sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df, \"transcript_sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = os.path.join(DATA_PATH, \"german.dic\")\n",
    "ahocs = comp_split.read_dictionary_from_file(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compound_split = compound_split_df(df, ahocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sentence\"][4000:4005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compound_split[\"sentence\"][4000:4005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_save_df(df_compound_split, \"sentences_compound_split\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration: Suche mit Keywörtern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lemmatized = db_get_df(\"sentences_lemmatized\")\n",
    "df_unlemmatized = db_get_df(\"transcript_sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_occurences(df, word):\n",
    "    occurences = []\n",
    "    for sentence in tqdm(df[\"sentence\"]):\n",
    "        if word in sentence.lower().split():\n",
    "            occurences.append(sentence)\n",
    "    return occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"?\"\n",
    "occurences_lemmatized = get_occurences(df_lemmatized, word)\n",
    "occurences_unlemmatized = get_occurences(df_unlemmatized, word)\n",
    "print(f\"Occurences Lemmatized({len(occurences_lemmatized)}):\",occurences_lemmatized)\n",
    "print(f\"Occurences UnLemmatized({len(occurences_unlemmatized)}):\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db_get_df(table=\"transcript_sentences_spacy\")\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bowie = df[df[\"filename\"] == \"david-bowie-das-chamaeleon-des-pop.mp3\"]\n",
    "save_list_to_file(df_bowie[\"sentence\"].to_list(), os.path.join(DATA_PATH, \"david_bowie_spacy.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userInputText = \"Zugspitze wandern\"\n",
    "userInput_segment_count = 7\n",
    "best_fitting = get_most_similar_documents_tf_idf(userInputText, userInput_segment_count)\n",
    "print(best_fitting[\"sentence\"].to_markdown())\n",
    "\n",
    "produce_snippets()\n",
    "produce_audio()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"/Users/br/Projects/Bachelorarbeit/scripts/server/audio/concatenated_audio.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Umwandelen aller MP3s in WAVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_mp3_to_wav(source_dir, target_dir):\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    \n",
    "    for filename in tqdm(os.listdir(source_dir)):\n",
    "        if filename.endswith('.mp3') and filename.replace('.mp3', '.wav') not in os.listdir(target_dir) :\n",
    "            mp3_path = os.path.join(source_dir, filename)\n",
    "            wav_path = os.path.join(target_dir, filename.replace('.mp3', '.wav'))\n",
    "            \n",
    "            \n",
    "            audio = AudioSegment.from_mp3(mp3_path)\n",
    "            \n",
    "            audio.export(wav_path, format=\"wav\")\n",
    "            print(f\"Converted {filename} to WAV and saved to {wav_path}\")\n",
    "\n",
    "# Example usage\n",
    "source_directory = '/Volumes/Samsung_T5/Podcast_Episoden'\n",
    "target_directory = '/Volumes/Samsung_T5/Podcast_Episoden_Wav'\n",
    "convert_mp3_to_wav(source_directory, target_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
